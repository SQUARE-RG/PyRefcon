<!doctype html>
<html>
<head>
<title>../torch/csrc/autograd/python_variable.cpp</title>

<style type="text/css">
body { color:#000000; background-color:#ffffff }
body { font-family:Helvetica, sans-serif; font-size:10pt }
h1 { font-size:14pt }
.FileName { margin-top: 5px; margin-bottom: 5px; display: inline; }
.FileNav { margin-left: 5px; margin-right: 5px; display: inline; }
.FileNav a { text-decoration:none; font-size: larger; }
.divider { margin-top: 30px; margin-bottom: 30px; height: 15px; }
.divider { background-color: gray; }
.code { border-collapse:collapse; width:100%; }
.code { font-family: "Monospace", monospace; font-size:10pt }
.code { line-height: 1.2em }
.comment { color: green; font-style: oblique }
.keyword { color: blue }
.string_literal { color: red }
.directive { color: darkmagenta }

/* Macros and variables could have pop-up notes hidden by default.
  - Macro pop-up:    expansion of the macro
  - Variable pop-up: value (table) of the variable */
.macro_popup, .variable_popup { display: none; }

/* Pop-up appears on mouse-hover event. */
.macro:hover .macro_popup, .variable:hover .variable_popup {
  display: block;
  padding: 2px;
  -webkit-border-radius:5px;
  -webkit-box-shadow:1px 1px 7px #000;
  border-radius:5px;
  box-shadow:1px 1px 7px #000;
  position: absolute;
  top: -1em;
  left:10em;
  z-index: 1
}

.macro_popup {
  border: 2px solid red;
  background-color:#FFF0F0;
  font-weight: normal;
}

.variable_popup {
  border: 2px solid blue;
  background-color:#F0F0FF;
  font-weight: bold;
  font-family: Helvetica, sans-serif;
  font-size: 9pt;
}

/* Pop-up notes needs a relative position as a base where they pops up. */
.macro, .variable {
  background-color: PaleGoldenRod;
  position: relative;
}
.macro { color: DarkMagenta; }

#tooltiphint {
  position: fixed;
  width: 50em;
  margin-left: -25em;
  left: 50%;
  padding: 10px;
  border: 1px solid #b0b0b0;
  border-radius: 2px;
  box-shadow: 1px 1px 7px black;
  background-color: #c0c0c0;
  z-index: 2;
}

.num { width:2.5em; padding-right:2ex; background-color:#eeeeee }
.num { text-align:right; font-size:8pt }
.num { color:#444444 }
.line { padding-left: 1ex; border-left: 3px solid #ccc }
.line { white-space: pre }
.msg { -webkit-box-shadow:1px 1px 7px #000 }
.msg { box-shadow:1px 1px 7px #000 }
.msg { -webkit-border-radius:5px }
.msg { border-radius:5px }
.msg { font-family:Helvetica, sans-serif; font-size:8pt }
.msg { float:left }
.msg { padding:0.25em 1ex 0.25em 1ex }
.msg { margin-top:10px; margin-bottom:10px }
.msg { font-weight:bold }
.msg { max-width:60em; word-wrap: break-word; white-space: pre-wrap }
.msgT { padding:0x; spacing:0x }
.msgEvent { background-color:#fff8b4; color:#000000 }
.msgControl { background-color:#bbbbbb; color:#000000 }
.msgNote { background-color:#ddeeff; color:#000000 }
.mrange { background-color:#dfddf3 }
.mrange { border-bottom:1px solid #6F9DBE }
.PathIndex { font-weight: bold; padding:0px 5px; margin-right:5px; }
.PathIndex { -webkit-border-radius:8px }
.PathIndex { border-radius:8px }
.PathIndexEvent { background-color:#bfba87 }
.PathIndexControl { background-color:#8c8c8c }
.PathIndexPopUp { background-color: #879abc; }
.PathNav a { text-decoration:none; font-size: larger }
.CodeInsertionHint { font-weight: bold; background-color: #10dd10 }
.CodeRemovalHint { background-color:#de1010 }
.CodeRemovalHint { border-bottom:1px solid #6F9DBE }
.selected{ background-color:orange !important; }

table.simpletable {
  padding: 5px;
  font-size:12pt;
  margin:20px;
  border-collapse: collapse; border-spacing: 0px;
}
td.rowname {
  text-align: right;
  vertical-align: top;
  font-weight: bold;
  color:#444444;
  padding-right:2ex;
}

/* Hidden text. */
input.spoilerhider + label {
  cursor: pointer;
  text-decoration: underline;
  display: block;
}
input.spoilerhider {
 display: none;
}
input.spoilerhider ~ .spoiler {
  overflow: hidden;
  margin: 10px auto 0;
  height: 0;
  opacity: 0;
}
input.spoilerhider:checked + label + .spoiler{
  height: auto;
  opacity: 1;
}
</style>
</head>
<body>
<!-- BUGDESC PyObject ownership leak with reference count of 1 -->

<!-- BUGTYPE Non-Zero Dead Object -->

<!-- BUGCATEGORY Python Memory Error -->

<!-- BUGFILE /home/alan/workspace/canalyze/test/python/pytorch/build/../torch/csrc/autograd/python_variable.cpp -->

<!-- FILENAME python_variable.cpp -->

<!-- FUNCTIONNAME THPVariable_get_names -->

<!-- ISSUEHASHCONTENTOFLINEINCONTEXT 45e3df92237e676232c12dc9ff19af51 -->

<!-- BUGLINE 604 -->

<!-- BUGCOLUMN 22 -->

<!-- BUGPATHLENGTH 12 -->

<!-- BUGMETAEND -->
<!-- REPORTHEADER -->
<h3>Bug Summary</h3>
<table class="simpletable">
<tr><td class="rowname">File:</td><td>build/../torch/csrc/autograd/python_variable.cpp</td></tr>
<tr><td class="rowname">Warning:</td><td><a href="#EndPath">line 604, column 22</a><br />PyObject ownership leak with reference count of 1</td></tr>

</table>
<!-- REPORTSUMMARYEXTRA -->
<h3>Annotated Source Code</h3>
<p>Press <a href="#" onclick="toggleHelp(); return false;">'?'</a>
   to see keyboard shortcuts</p>
<input type="checkbox" class="spoilerhider" id="showinvocation" />
<label for="showinvocation" >Show analyzer invocation</label>
<div class="spoiler">clang -cc1 -cc1 -triple x86_64-unknown-linux-gnu -analyze -disable-free -disable-llvm-verifier -discard-value-names -main-file-name python_variable.cpp -analyzer-store=region -analyzer-opt-analyze-nested-blocks -analyzer-checker=core -analyzer-checker=apiModeling -analyzer-checker=unix -analyzer-checker=deadcode -analyzer-checker=cplusplus -analyzer-checker=security.insecureAPI.UncheckedReturn -analyzer-checker=security.insecureAPI.getpw -analyzer-checker=security.insecureAPI.gets -analyzer-checker=security.insecureAPI.mktemp -analyzer-checker=security.insecureAPI.mkstemp -analyzer-checker=security.insecureAPI.vfork -analyzer-checker=nullability.NullPassedToNonnull -analyzer-checker=nullability.NullReturnedFromNonnull -analyzer-output plist -w -analyzer-output=html -analyzer-checker=python -analyzer-disable-checker=deadcode -analyzer-config prune-paths=true,suppress-c++-stdlib=true,suppress-inlined-defensive-checks=false,suppress-null-return-paths=false,crosscheck-with-z3=true,model-path=/home/alan/temp/cpython/Python-3.8.5/Doc/build/models -analyzer-config experimental-enable-naive-ctu-analysis=true,ctu-dir=/home/alan/workspace/canalyze/test/python/pytorch/csa-scan,ctu-index-name=/home/alan/workspace/canalyze/test/python/pytorch/csa-scan/externalDefMap.txt,ctu-invocation-list=/home/alan/workspace/canalyze/test/python/pytorch/csa-scan/invocations.yaml,display-ctu-progress=false -setup-static-analyzer -analyzer-config-compatibility-mode=true -mrelocation-model pic -pic-level 2 -fhalf-no-semantic-interposition -mframe-pointer=none -relaxed-aliasing -fno-rounding-math -ffp-exception-behavior=ignore -mconstructor-aliases -munwind-tables -target-cpu x86-64 -tune-cpu generic -debugger-tuning=gdb -fcoverage-compilation-dir=/home/alan/workspace/canalyze/test/python/pytorch/build -resource-dir /home/alan/workspace/llvm-project/build/NATIVE/lib/clang/13.0.0 -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem ../third_party/protobuf/src -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party/XNNPACK/include -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /home/alan/temp/cpython/Python-3.8.5/Doc/build/python3.8 -isystem /usr/lib/python3/dist-packages/numpy/core/include -isystem ../cmake/../third_party/pybind11/include -isystem /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -isystem /usr/lib/x86_64-linux-gnu/openmpi/include -isystem ../third_party/ideep/mkl-dnn/include -isystem ../third_party/ideep/include -D BUILDING_TESTS -D FMT_HEADER_ONLY=1 -D HAVE_MALLOC_USABLE_SIZE=1 -D HAVE_MMAP=1 -D HAVE_SHM_OPEN=1 -D HAVE_SHM_UNLINK=1 -D MINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -D ONNXIFI_ENABLE_EXT=1 -D ONNX_ML=1 -D ONNX_NAMESPACE=onnx_torch -D THP_BUILD_MAIN_LIB -D USE_C10D -D USE_C10D_GLOO -D USE_C10D_MPI -D USE_DISTRIBUTED -D USE_EXTERNAL_MZCRC -D USE_NUMPY -D USE_RPC -D USE_TENSORPIPE -D USE_VALGRIND -D _FILE_OFFSET_BITS=64 -D torch_python_EXPORTS -I aten/src -I ../aten/src -I . -I ../ -I ../cmake/../third_party/benchmark/include -I caffe2/contrib/aten -I ../third_party/onnx -I third_party/onnx -I ../third_party/foxi -I third_party/foxi -I ../torch/.. -I ../torch/../aten/src -I ../torch/../aten/src/TH -I caffe2/aten/src -I third_party -I ../torch/../third_party/valgrind-headers -I ../torch/../third_party/gloo -I ../torch/../third_party/onnx -I ../torch/csrc -I ../torch/csrc/api/include -I ../torch/lib -I ../torch/lib/libshm -I ../torch/csrc/distributed -I ../torch/csrc/api -I ../c10/.. -I third_party/ideep/mkl-dnn/include -I ../third_party/ideep/mkl-dnn/src/../include -I ../torch/lib/libshm/../../../torch/lib -I ../third_party/fmt/include -D USE_PTHREADPOOL -D NDEBUG -D USE_KINETO -D LIBKINETO_NOCUPTI -D USE_FBGEMM -D USE_QNNPACK -D USE_PYTORCH_QNNPACK -D USE_XNNPACK -D SYMBOLICATE_MOBILE_DEBUG_HANDLE -D HAVE_AVX_CPU_DEFINITION -D HAVE_AVX2_CPU_DEFINITION -D NDEBUG -D NDEBUG -D CAFFE2_USE_GLOO -D HAVE_GCC_GET_CPUID -D USE_AVX -D USE_AVX2 -D TH_HAVE_THREAD -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10 -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/x86_64-linux-gnu/c++/10 -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/backward -internal-isystem /home/alan/workspace/llvm-project/build/NATIVE/lib/clang/13.0.0/include -internal-isystem /usr/local/include -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../x86_64-linux-gnu/include -internal-externc-isystem /usr/include/x86_64-linux-gnu -internal-externc-isystem /include -internal-externc-isystem /usr/include -O3 -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -Wno-unused-but-set-variable -Wno-maybe-uninitialized -Werror=format -Werror=cast-function-type -Wno-stringop-overflow -Wno-write-strings -Wno-strict-aliasing -Wno-cast-function-type -w -std=gnu++14 -fdeprecated-macro -fdebug-compilation-dir=/home/alan/workspace/canalyze/test/python/pytorch/build -ferror-limit 19 -fvisibility-inlines-hidden -fopenmp -fopenmp-cuda-parallel-target-regions -pthread -fgnuc-version=4.2.1 -fcxx-exceptions -fexceptions -faligned-allocation -fcolor-diagnostics -vectorize-loops -vectorize-slp -faddrsig -D__GCC_HAVE_DWARF2_CFI_ASM=1 -o /home/alan/workspace/canalyze/test/python/pytorch/csa-scan/reports -x c++ ../torch/csrc/autograd/python_variable.cpp
</div>
<div id='tooltiphint' hidden="true">
  <p>Keyboard shortcuts: </p>
  <ul>
    <li>Use 'j/k' keys for keyboard navigation</li>
    <li>Use 'Shift+S' to show/hide relevant lines</li>
    <li>Use '?' to toggle this window</li>
  </ul>
  <a href="#" onclick="toggleHelp(); return false;">Close</a>
</div>
<script type='text/javascript'>
var relevant_lines = {"1": {"594": 1, "595": 1, "596": 1, "597": 1, "602": 1, "603": 1, "604": 1, "605": 1, "607": 1, "608": 1, "610": 1, "611": 1, "622": 1, "623": 1}, "3": {"49": 1, "50": 1, "53": 1, "54": 1}, "33720": {"252": 1, "253": 1, "254": 1, "255": 1, "259": 1, "261": 1}, "39691": {"125": 1, "126": 1, "187": 1, "188": 1, "285": 1, "286": 1}, "68865": {"380": 1, "381": 1, "388": 1, "390": 1}, "68867": {"189": 1, "190": 1, "194": 1, "196": 1}, "72605": {"17": 1, "18": 1}, "91200": {"98": 1, "102": 1}, "135410": {"9": 1, "16": 1}, "144747": {"63": 1, "64": 1}, "192259": {"5": 1}, "193750": {"5": 1, "6": 1}};

var filterCounterexample = function (hide) {
  var tables = document.getElementsByClassName("code");
  for (var t=0; t<tables.length; t++) {
    var table = tables[t];
    var file_id = table.getAttribute("data-fileid");
    var lines_in_fid = relevant_lines[file_id];
    if (!lines_in_fid) {
      lines_in_fid = {};
    }
    var lines = table.getElementsByClassName("codeline");
    for (var i=0; i<lines.length; i++) {
        var el = lines[i];
        var lineNo = el.getAttribute("data-linenumber");
        if (!lines_in_fid[lineNo]) {
          if (hide) {
            el.setAttribute("hidden", "");
          } else {
            el.removeAttribute("hidden");
          }
        }
    }
  }
}

window.addEventListener("keydown", function (event) {
  if (event.defaultPrevented) {
    return;
  }
  if (event.key == "S") {
    var checked = document.getElementsByName("showCounterexample")[0].checked;
    filterCounterexample(!checked);
    document.getElementsByName("showCounterexample")[0].checked = !checked;
  } else {
    return;
  }
  event.preventDefault();
}, true);

document.addEventListener("DOMContentLoaded", function() {
    document.querySelector('input[name="showCounterexample"]').onchange=
        function (event) {
      filterCounterexample(this.checked);
    };
});
</script>

<form>
    <input type="checkbox" name="showCounterexample" id="showCounterexample" />
    <label for="showCounterexample">
       Show only relevant lines
    </label>
</form>

<script type='text/javascript'>
var digitMatcher = new RegExp("[0-9]+");

var querySelectorAllArray = function(selector) {
  return Array.prototype.slice.call(
    document.querySelectorAll(selector));
}

document.addEventListener("DOMContentLoaded", function() {
    querySelectorAllArray(".PathNav > a").forEach(
        function(currentValue, currentIndex) {
            var hrefValue = currentValue.getAttribute("href");
            currentValue.onclick = function() {
                scrollTo(document.querySelector(hrefValue));
                return false;
            };
        });
});

var findNum = function() {
    var s = document.querySelector(".selected");
    if (!s || s.id == "EndPath") {
        return 0;
    }
    var out = parseInt(digitMatcher.exec(s.id)[0]);
    return out;
};

var scrollTo = function(el) {
    querySelectorAllArray(".selected").forEach(function(s) {
        s.classList.remove("selected");
    });
    el.classList.add("selected");
    window.scrollBy(0, el.getBoundingClientRect().top -
        (window.innerHeight / 2));
}

var move = function(num, up, numItems) {
  if (num == 1 && up || num == numItems - 1 && !up) {
    return 0;
  } else if (num == 0 && up) {
    return numItems - 1;
  } else if (num == 0 && !up) {
    return 1 % numItems;
  }
  return up ? num - 1 : num + 1;
}

var numToId = function(num) {
  if (num == 0) {
    return document.getElementById("EndPath")
  }
  return document.getElementById("Path" + num);
};

var navigateTo = function(up) {
  var numItems = document.querySelectorAll(
      ".line > .msgEvent, .line > .msgControl").length;
  var currentSelected = findNum();
  var newSelected = move(currentSelected, up, numItems);
  var newEl = numToId(newSelected, numItems);

  // Scroll element into center.
  scrollTo(newEl);
};

window.addEventListener("keydown", function (event) {
  if (event.defaultPrevented) {
    return;
  }
  if (event.key == "j") {
    navigateTo(/*up=*/false);
  } else if (event.key == "k") {
    navigateTo(/*up=*/true);
  } else {
    return;
  }
  event.preventDefault();
}, true);
</script>
  
<script type='text/javascript'>

var toggleHelp = function() {
    var hint = document.querySelector("#tooltiphint");
    var attributeName = "hidden";
    if (hint.hasAttribute(attributeName)) {
      hint.removeAttribute(attributeName);
    } else {
      hint.setAttribute("hidden", "true");
    }
};
window.addEventListener("keydown", function (event) {
  if (event.defaultPrevented) {
    return;
  }
  if (event.key == "?") {
    toggleHelp();
  } else {
    return;
  }
  event.preventDefault();
});
</script>
<div id=File1>
<h4 class=FileName>../torch/csrc/autograd/python_variable.cpp</h4>
<div class=FileNav><a href="#File193750">&#x2192;</a></div></div>
<table class="code" data-fileid="1">
<tr class="codeline" data-linenumber="1"><td class="num" id="LN1">1</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/python_variable.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="2"><td class="num" id="LN2">2</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="3"><td class="num" id="LN3">3</td><td class="line"><span class='directive'>#include &lt;torch/csrc/THP.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="4"><td class="num" id="LN4">4</td><td class="line"><span class='directive'>#include &lt;torch/csrc/DynamicTypes.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="5"><td class="num" id="LN5">5</td><td class="line"><span class='directive'>#include &lt;torch/csrc/Exceptions.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="6"><td class="num" id="LN6">6</td><td class="line"><span class='directive'>#include &lt;torch/csrc/Device.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="7"><td class="num" id="LN7">7</td><td class="line"><span class='directive'>#include &lt;torch/csrc/Size.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="8"><td class="num" id="LN8">8</td><td class="line"><span class='directive'>#include &lt;torch/csrc/Types.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="9"><td class="num" id="LN9">9</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/autograd.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="10"><td class="num" id="LN10">10</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/edge.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="11"><td class="num" id="LN11">11</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/python_cpp_function.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="12"><td class="num" id="LN12">12</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/python_hook.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="13"><td class="num" id="LN13">13</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/python_variable_indexing.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="14"><td class="num" id="LN14">14</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/variable.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="15"><td class="num" id="LN15">15</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/functions/accumulate_grad.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="16"><td class="num" id="LN16">16</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/function.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="17"><td class="num" id="LN17">17</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/generated/VariableType.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="18"><td class="num" id="LN18">18</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/utils/error_messages.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="19"><td class="num" id="LN19">19</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/utils/wrap_outputs.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="20"><td class="num" id="LN20">20</td><td class="line"><span class='directive'>#include &lt;torch/csrc/tensor/python_tensor.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="21"><td class="num" id="LN21">21</td><td class="line"><span class='directive'>#include &lt;pybind11/pybind11.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="22"><td class="num" id="LN22">22</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/cuda_lazy_init.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="23"><td class="num" id="LN23">23</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/pybind.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="24"><td class="num" id="LN24">24</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/pycfunction_helpers.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="25"><td class="num" id="LN25">25</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/python_strings.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="26"><td class="num" id="LN26">26</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/python_arg_parser.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="27"><td class="num" id="LN27">27</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/tensor_new.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="28"><td class="num" id="LN28">28</td><td class="line"><span class='directive'>#include &lt;torch/csrc/jit/frontend/tracer.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="29"><td class="num" id="LN29">29</td><td class="line"><span class='directive'>#include &lt;ATen/NamedTensorUtils.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="30"><td class="num" id="LN30">30</td><td class="line"><span class='directive'>#include &lt;c10/util/DeadlockDetection.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="31"><td class="num" id="LN31">31</td><td class="line"><span class='directive'>#include &lt;c10/util/irange.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="32"><td class="num" id="LN32">32</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="33"><td class="num" id="LN33">33</td><td class="line"><span class='directive'>#include &lt;torch/library.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="34"><td class="num" id="LN34">34</td><td class="line"><span class='directive'>#include &lt;torch/csrc/jit/python/pybind_utils.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="35"><td class="num" id="LN35">35</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="36"><td class="num" id="LN36">36</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="37"><td class="num" id="LN37">37</td><td class="line"><span class='directive'>#include &lt;ATen/ATen.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="38"><td class="num" id="LN38">38</td><td class="line"><span class='directive'>#include &lt;pybind11/pybind11.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="39"><td class="num" id="LN39">39</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="40"><td class="num" id="LN40">40</td><td class="line"><span class='directive'>#include &lt;structmember.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="41"><td class="num" id="LN41">41</td><td class="line"><span class='directive'>#include &lt;cstdint&gt;</span></td></tr>
<tr class="codeline" data-linenumber="42"><td class="num" id="LN42">42</td><td class="line"><span class='directive'>#include &lt;iostream&gt;</span></td></tr>
<tr class="codeline" data-linenumber="43"><td class="num" id="LN43">43</td><td class="line"><span class='directive'>#include &lt;memory&gt;</span></td></tr>
<tr class="codeline" data-linenumber="44"><td class="num" id="LN44">44</td><td class="line"><span class='directive'>#include &lt;utility&gt;</span></td></tr>
<tr class="codeline" data-linenumber="45"><td class="num" id="LN45">45</td><td class="line"><span class='directive'>#include &lt;vector&gt;</span></td></tr>
<tr class="codeline" data-linenumber="46"><td class="num" id="LN46">46</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="47"><td class="num" id="LN47">47</td><td class="line"><span class='keyword'>using</span> <span class='keyword'>namespace</span> at;</td></tr>
<tr class="codeline" data-linenumber="48"><td class="num" id="LN48">48</td><td class="line"><span class='keyword'>using</span> <span class='keyword'>namespace</span> torch;</td></tr>
<tr class="codeline" data-linenumber="49"><td class="num" id="LN49">49</td><td class="line"><span class='keyword'>using</span> <span class='keyword'>namespace</span> torch::autograd;</td></tr>
<tr class="codeline" data-linenumber="50"><td class="num" id="LN50">50</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="51"><td class="num" id="LN51">51</td><td class="line"><span class='keyword'>namespace</span> {</td></tr>
<tr class="codeline" data-linenumber="52"><td class="num" id="LN52">52</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="53"><td class="num" id="LN53">53</td><td class="line">std::string concrete_name_fn(<span class='keyword'>const</span> c10::impl::PyInterpreter* self) {</td></tr>
<tr class="codeline" data-linenumber="54"><td class="num" id="LN54">54</td><td class="line">  std::stringstream ss;</td></tr>
<tr class="codeline" data-linenumber="55"><td class="num" id="LN55">55</td><td class="line">  ss &lt;&lt; self;</td></tr>
<tr class="codeline" data-linenumber="56"><td class="num" id="LN56">56</td><td class="line">  <span class='keyword'>return</span> ss.str();</td></tr>
<tr class="codeline" data-linenumber="57"><td class="num" id="LN57">57</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="58"><td class="num" id="LN58">58</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="59"><td class="num" id="LN59">59</td><td class="line"><span class='keyword'>void</span> concrete_decref_fn(<span class='keyword'>const</span> c10::impl::PyInterpreter* self, PyObject* pyobj) {</td></tr>
<tr class="codeline" data-linenumber="60"><td class="num" id="LN60">60</td><td class="line">  <span class='comment'>// Leak the pyobj if not initialized.  This can happen if we are running</span></td></tr>
<tr class="codeline" data-linenumber="61"><td class="num" id="LN61">61</td><td class="line">  <span class='comment'>// exit handlers that are destructing tensors with residual (owned)</span></td></tr>
<tr class="codeline" data-linenumber="62"><td class="num" id="LN62">62</td><td class="line">  <span class='comment'>// PyObjects stored in them.</span></td></tr>
<tr class="codeline" data-linenumber="63"><td class="num" id="LN63">63</td><td class="line">  <span class='keyword'>if</span> (!Py_IsInitialized())</td></tr>
<tr class="codeline" data-linenumber="64"><td class="num" id="LN64">64</td><td class="line">    <span class='keyword'>return</span>;</td></tr>
<tr class="codeline" data-linenumber="65"><td class="num" id="LN65">65</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="66"><td class="num" id="LN66">66</td><td class="line">  pybind11::gil_scoped_acquire gil;</td></tr>
<tr class="codeline" data-linenumber="67"><td class="num" id="LN67">67</td><td class="line">  <span class='keyword'>if</span> (<span class='macro'>Py_REFCNT(pyobj)<span class='macro_popup'>(((PyObject*)(pyobj))-&gt;ob_refcnt)</span></span> &gt; 1) {</td></tr>
<tr class="codeline" data-linenumber="68"><td class="num" id="LN68">68</td><td class="line">    <span class='comment'>// It's still alive!  This can happen if a weak ref resurrected</span></td></tr>
<tr class="codeline" data-linenumber="69"><td class="num" id="LN69">69</td><td class="line">    <span class='comment'>// the PyObject without flipping ownership.  At this point it is</span></td></tr>
<tr class="codeline" data-linenumber="70"><td class="num" id="LN70">70</td><td class="line">    <span class='comment'>// too late to rescue the object, so just stub out the PyObject</span></td></tr>
<tr class="codeline" data-linenumber="71"><td class="num" id="LN71">71</td><td class="line">    <span class='comment'>// so that it fails on subsequent uses.  Don't raise an error here;</span></td></tr>
<tr class="codeline" data-linenumber="72"><td class="num" id="LN72">72</td><td class="line">    <span class='comment'>// you're probably in a destructor.</span></td></tr>
<tr class="codeline" data-linenumber="73"><td class="num" id="LN73">73</td><td class="line">    <span class='macro'>TORCH_WARN(<span class='macro_popup'>::c10::Warning::warn( {__func__, "../torch/csrc/autograd/python_variable.cpp"<br>, static_cast&lt;uint32_t&gt;(78)}, ::c10::str("Deallocating Tensor that still has live PyObject references.  "<br> "This probably happened because you took out a weak reference to "<br> "Tensor and didn't call _fix_weakref() after dereferencing it.  "<br> "Subsequent accesses to this tensor via the PyObject will now fail."<br>), false)</span></span></td></tr>
<tr class="codeline" data-linenumber="74"><td class="num" id="LN74">74</td><td class="line">      <span class='string_literal'><span class='macro'>"Deallocating Tensor that still has live PyObject references.  "<span class='macro_popup'>::c10::Warning::warn( {__func__, "../torch/csrc/autograd/python_variable.cpp"<br>, static_cast&lt;uint32_t&gt;(78)}, ::c10::str("Deallocating Tensor that still has live PyObject references.  "<br> "This probably happened because you took out a weak reference to "<br> "Tensor and didn't call _fix_weakref() after dereferencing it.  "<br> "Subsequent accesses to this tensor via the PyObject will now fail."<br>), false)</span></span></span></td></tr>
<tr class="codeline" data-linenumber="75"><td class="num" id="LN75">75</td><td class="line">      <span class='string_literal'><span class='macro'>"This probably happened because you took out a weak reference to "<span class='macro_popup'>::c10::Warning::warn( {__func__, "../torch/csrc/autograd/python_variable.cpp"<br>, static_cast&lt;uint32_t&gt;(78)}, ::c10::str("Deallocating Tensor that still has live PyObject references.  "<br> "This probably happened because you took out a weak reference to "<br> "Tensor and didn't call _fix_weakref() after dereferencing it.  "<br> "Subsequent accesses to this tensor via the PyObject will now fail."<br>), false)</span></span></span></td></tr>
<tr class="codeline" data-linenumber="76"><td class="num" id="LN76">76</td><td class="line">      <span class='string_literal'><span class='macro'>"Tensor and didn't call _fix_weakref() after dereferencing it.  "<span class='macro_popup'>::c10::Warning::warn( {__func__, "../torch/csrc/autograd/python_variable.cpp"<br>, static_cast&lt;uint32_t&gt;(78)}, ::c10::str("Deallocating Tensor that still has live PyObject references.  "<br> "This probably happened because you took out a weak reference to "<br> "Tensor and didn't call _fix_weakref() after dereferencing it.  "<br> "Subsequent accesses to this tensor via the PyObject will now fail."<br>), false)</span></span></span></td></tr>
<tr class="codeline" data-linenumber="77"><td class="num" id="LN77">77</td><td class="line">      <span class='string_literal'><span class='macro'>"Subsequent accesses to this tensor via the PyObject will now fail."<span class='macro_popup'>::c10::Warning::warn( {__func__, "../torch/csrc/autograd/python_variable.cpp"<br>, static_cast&lt;uint32_t&gt;(78)}, ::c10::str("Deallocating Tensor that still has live PyObject references.  "<br> "This probably happened because you took out a weak reference to "<br> "Tensor and didn't call _fix_weakref() after dereferencing it.  "<br> "Subsequent accesses to this tensor via the PyObject will now fail."<br>), false)</span></span></span></td></tr>
<tr class="codeline" data-linenumber="78"><td class="num" id="LN78">78</td><td class="line">    <span class='macro'>)<span class='macro_popup'>::c10::Warning::warn( {__func__, "../torch/csrc/autograd/python_variable.cpp"<br>, static_cast&lt;uint32_t&gt;(78)}, ::c10::str("Deallocating Tensor that still has live PyObject references.  "<br> "This probably happened because you took out a weak reference to "<br> "Tensor and didn't call _fix_weakref() after dereferencing it.  "<br> "Subsequent accesses to this tensor via the PyObject will now fail."<br>), false)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="79"><td class="num" id="LN79">79</td><td class="line">    ((THPVariable*)pyobj)-&gt;cdata = MaybeOwned&lt;Variable&gt;();</td></tr>
<tr class="codeline" data-linenumber="80"><td class="num" id="LN80">80</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="81"><td class="num" id="LN81">81</td><td class="line">  <span class='macro'>Py_DECREF(pyobj)<span class='macro_popup'>_Py_DECREF(((PyObject*)(pyobj)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="82"><td class="num" id="LN82">82</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="83"><td class="num" id="LN83">83</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="84"><td class="num" id="LN84">84</td><td class="line">c10::intrusive_ptr&lt;TensorImpl&gt; concrete_detach_fn(<span class='keyword'>const</span> c10::impl::PyInterpreter*, <span class='keyword'>const</span> c10::TensorImpl* self);</td></tr>
<tr class="codeline" data-linenumber="85"><td class="num" id="LN85">85</td><td class="line"><span class='keyword'>void</span> concrete_dispatch_fn(<span class='keyword'>const</span> c10::impl::PyInterpreter*, <span class='keyword'>const</span> c10::OperatorHandle&amp; op, torch::jit::Stack* stack);</td></tr>
<tr class="codeline" data-linenumber="86"><td class="num" id="LN86">86</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="87"><td class="num" id="LN87">87</td><td class="line"><span class='keyword'>class</span> PyInterpreterHolder {</td></tr>
<tr class="codeline" data-linenumber="88"><td class="num" id="LN88">88</td><td class="line"> <span class='keyword'>public</span>:</td></tr>
<tr class="codeline" data-linenumber="89"><td class="num" id="LN89">89</td><td class="line">  PyInterpreterHolder()</td></tr>
<tr class="codeline" data-linenumber="90"><td class="num" id="LN90">90</td><td class="line">      : impl_(<span class='keyword'>new</span> c10::impl::PyInterpreter(</td></tr>
<tr class="codeline" data-linenumber="91"><td class="num" id="LN91">91</td><td class="line">            &amp;concrete_name_fn,</td></tr>
<tr class="codeline" data-linenumber="92"><td class="num" id="LN92">92</td><td class="line">            &amp;concrete_decref_fn,</td></tr>
<tr class="codeline" data-linenumber="93"><td class="num" id="LN93">93</td><td class="line">            &amp;concrete_detach_fn,</td></tr>
<tr class="codeline" data-linenumber="94"><td class="num" id="LN94">94</td><td class="line">            &amp;concrete_dispatch_fn)) {}</td></tr>
<tr class="codeline" data-linenumber="95"><td class="num" id="LN95">95</td><td class="line">  <span class='comment'>// NB: intentionally leaks the memory</span></td></tr>
<tr class="codeline" data-linenumber="96"><td class="num" id="LN96">96</td><td class="line">  ~PyInterpreterHolder() {</td></tr>
<tr class="codeline" data-linenumber="97"><td class="num" id="LN97">97</td><td class="line">    impl_-&gt;disarm();</td></tr>
<tr class="codeline" data-linenumber="98"><td class="num" id="LN98">98</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="99"><td class="num" id="LN99">99</td><td class="line">  c10::impl::PyInterpreter* get() <span class='keyword'>const</span> <span class='keyword'>noexcept</span> {</td></tr>
<tr class="codeline" data-linenumber="100"><td class="num" id="LN100">100</td><td class="line">    <span class='keyword'>return</span> impl_;</td></tr>
<tr class="codeline" data-linenumber="101"><td class="num" id="LN101">101</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="102"><td class="num" id="LN102">102</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="103"><td class="num" id="LN103">103</td><td class="line"> <span class='keyword'>private</span>:</td></tr>
<tr class="codeline" data-linenumber="104"><td class="num" id="LN104">104</td><td class="line">  c10::impl::PyInterpreter* impl_;</td></tr>
<tr class="codeline" data-linenumber="105"><td class="num" id="LN105">105</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="106"><td class="num" id="LN106">106</td><td class="line">PyInterpreterHolder self_interpreter;</td></tr>
<tr class="codeline" data-linenumber="107"><td class="num" id="LN107">107</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="108"><td class="num" id="LN108">108</td><td class="line">} <span class='comment'>// anonymous namespace</span></td></tr>
<tr class="codeline" data-linenumber="109"><td class="num" id="LN109">109</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="110"><td class="num" id="LN110">110</td><td class="line"><span class='keyword'>namespace</span> py = pybind11;</td></tr>
<tr class="codeline" data-linenumber="111"><td class="num" id="LN111">111</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="112"><td class="num" id="LN112">112</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="113"><td class="num" id="LN113">113</td><td class="line">PyObject *THPVariableClass = <span class='keyword'>nullptr</span>;</td></tr>
<tr class="codeline" data-linenumber="114"><td class="num" id="LN114">114</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="115"><td class="num" id="LN115">115</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="116"><td class="num" id="LN116">116</td><td class="line">PyObject *ParameterClass = <span class='keyword'>nullptr</span>;</td></tr>
<tr class="codeline" data-linenumber="117"><td class="num" id="LN117">117</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="118"><td class="num" id="LN118">118</td><td class="line"><span class='comment'>// clang-tidy gets confused by static const</span></td></tr>
<tr class="codeline" data-linenumber="119"><td class="num" id="LN119">119</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="120"><td class="num" id="LN120">120</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>const</span> <span class='keyword'>char</span>* VOLATILE_WARNING =</td></tr>
<tr class="codeline" data-linenumber="121"><td class="num" id="LN121">121</td><td class="line">    <span class='string_literal'>"volatile was removed and now has no effect. Use "</span></td></tr>
<tr class="codeline" data-linenumber="122"><td class="num" id="LN122">122</td><td class="line">    <span class='string_literal'>"`with torch.no_grad():` instead."</span>;</td></tr>
<tr class="codeline" data-linenumber="123"><td class="num" id="LN123">123</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="124"><td class="num" id="LN124">124</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>bool</span> check_has_torch_dispatch(PyObject *obj) {</td></tr>
<tr class="codeline" data-linenumber="125"><td class="num" id="LN125">125</td><td class="line">  PyTypeObject *tp = <span class='macro'>Py_TYPE(obj)<span class='macro_popup'>(((PyObject*)(obj))-&gt;ob_type)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="126"><td class="num" id="LN126">126</td><td class="line">  <span class='keyword'>return</span> (</td></tr>
<tr class="codeline" data-linenumber="127"><td class="num" id="LN127">127</td><td class="line">    !THPVariable_CheckTypeExact(tp) &amp;&amp;</td></tr>
<tr class="codeline" data-linenumber="128"><td class="num" id="LN128">128</td><td class="line">    <span class='comment'>// TODO: test if Python key is disabled</span></td></tr>
<tr class="codeline" data-linenumber="129"><td class="num" id="LN129">129</td><td class="line">    PyObject_FastGetAttrString(obj, <span class='string_literal'>"__torch_dispatch__"</span>).ptr() != <span class='keyword'>nullptr</span></td></tr>
<tr class="codeline" data-linenumber="130"><td class="num" id="LN130">130</td><td class="line">  );</td></tr>
<tr class="codeline" data-linenumber="131"><td class="num" id="LN131">131</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="132"><td class="num" id="LN132">132</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="133"><td class="num" id="LN133">133</td><td class="line"><span class='comment'>// Creates a new Python object for a Variable.  The status parameter</span></td></tr>
<tr class="codeline" data-linenumber="134"><td class="num" id="LN134">134</td><td class="line"><span class='comment'>// specifies what the interpreter tag status on the object is; for</span></td></tr>
<tr class="codeline" data-linenumber="135"><td class="num" id="LN135">135</td><td class="line"><span class='comment'>// example, if you ran check_pyobj, the return optional of this object</span></td></tr>
<tr class="codeline" data-linenumber="136"><td class="num" id="LN136">136</td><td class="line"><span class='comment'>// tells you if the tensor was already tagged or not so you can pass</span></td></tr>
<tr class="codeline" data-linenumber="137"><td class="num" id="LN137">137</td><td class="line"><span class='comment'>// TAGGED_BY_US or MAYBE_UNINITIALIZED; in other cases, you know where</span></td></tr>
<tr class="codeline" data-linenumber="138"><td class="num" id="LN138">138</td><td class="line"><span class='comment'>// var came from and can directly assert that it's DEFINITELY_UNINITIALIZED.</span></td></tr>
<tr class="codeline" data-linenumber="139"><td class="num" id="LN139">139</td><td class="line"><span class='comment'>// It's ALWAYS safe (albeit slower) to call this with MAYBE_UNINITIALIZED.</span></td></tr>
<tr class="codeline" data-linenumber="140"><td class="num" id="LN140">140</td><td class="line"><span class='keyword'>static</span> PyObject* THPVariable_NewWithVar(</td></tr>
<tr class="codeline" data-linenumber="141"><td class="num" id="LN141">141</td><td class="line">    PyTypeObject* type,</td></tr>
<tr class="codeline" data-linenumber="142"><td class="num" id="LN142">142</td><td class="line">    Variable _var,</td></tr>
<tr class="codeline" data-linenumber="143"><td class="num" id="LN143">143</td><td class="line">    c10::impl::PyInterpreterStatus status) {</td></tr>
<tr class="codeline" data-linenumber="144"><td class="num" id="LN144">144</td><td class="line">  PyObject* obj = type-&gt;tp_alloc(type, 0);</td></tr>
<tr class="codeline" data-linenumber="145"><td class="num" id="LN145">145</td><td class="line">  <span class='keyword'>if</span> (obj) {</td></tr>
<tr class="codeline" data-linenumber="146"><td class="num" id="LN146">146</td><td class="line">    <span class='keyword'>auto</span> v = (THPVariable*) obj;</td></tr>
<tr class="codeline" data-linenumber="147"><td class="num" id="LN147">147</td><td class="line">    <span class='comment'>// TODO: named constructor to avoid default initialization</span></td></tr>
<tr class="codeline" data-linenumber="148"><td class="num" id="LN148">148</td><td class="line">    <span class='keyword'>new</span> (&amp;v-&gt;cdata) MaybeOwned&lt;Variable&gt;();</td></tr>
<tr class="codeline" data-linenumber="149"><td class="num" id="LN149">149</td><td class="line">    v-&gt;cdata = MaybeOwned&lt;Variable&gt;::owned(std::move(_var));</td></tr>
<tr class="codeline" data-linenumber="150"><td class="num" id="LN150">150</td><td class="line">    <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(v);</td></tr>
<tr class="codeline" data-linenumber="151"><td class="num" id="LN151">151</td><td class="line">    var.unsafeGetTensorImpl()-&gt;init_pyobj(self_interpreter.get(), obj, status);</td></tr>
<tr class="codeline" data-linenumber="152"><td class="num" id="LN152">152</td><td class="line">    <span class='keyword'>if</span> (check_has_torch_dispatch(obj)) {</td></tr>
<tr class="codeline" data-linenumber="153"><td class="num" id="LN153">153</td><td class="line">      var.unsafeGetTensorImpl()-&gt;set_python_dispatch(<span class='keyword'>true</span>);</td></tr>
<tr class="codeline" data-linenumber="154"><td class="num" id="LN154">154</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="155"><td class="num" id="LN155">155</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="156"><td class="num" id="LN156">156</td><td class="line">  <span class='keyword'>return</span> obj;</td></tr>
<tr class="codeline" data-linenumber="157"><td class="num" id="LN157">157</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="158"><td class="num" id="LN158">158</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="159"><td class="num" id="LN159">159</td><td class="line"><span class='comment'>// TODO: Make this take Variable by const reference</span></td></tr>
<tr class="codeline" data-linenumber="160"><td class="num" id="LN160">160</td><td class="line">PyObject * THPVariable_Wrap(Variable var)</td></tr>
<tr class="codeline" data-linenumber="161"><td class="num" id="LN161">161</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="162"><td class="num" id="LN162">162</td><td class="line">  <span class='keyword'>if</span> (!var.defined()) {</td></tr>
<tr class="codeline" data-linenumber="163"><td class="num" id="LN163">163</td><td class="line">    <span class='macro'>Py_RETURN_NONE<span class='macro_popup'>return _Py_INCREF(((PyObject*)((&amp;_Py_NoneStruct)))), (&amp;<br>_Py_NoneStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="164"><td class="num" id="LN164">164</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="165"><td class="num" id="LN165">165</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="166"><td class="num" id="LN166">166</td><td class="line">  c10::optional&lt;PyObject*&gt; mb_obj =</td></tr>
<tr class="codeline" data-linenumber="167"><td class="num" id="LN167">167</td><td class="line">      var.unsafeGetTensorImpl()-&gt;check_pyobj(self_interpreter.get());</td></tr>
<tr class="codeline" data-linenumber="168"><td class="num" id="LN168">168</td><td class="line">  c10::impl::PyInterpreterStatus status;</td></tr>
<tr class="codeline" data-linenumber="169"><td class="num" id="LN169">169</td><td class="line">  <span class='keyword'>if</span> (mb_obj.has_value()) {</td></tr>
<tr class="codeline" data-linenumber="170"><td class="num" id="LN170">170</td><td class="line">    <span class='keyword'>auto</span> obj = *mb_obj;</td></tr>
<tr class="codeline" data-linenumber="171"><td class="num" id="LN171">171</td><td class="line">    <span class='keyword'>if</span> (obj) {</td></tr>
<tr class="codeline" data-linenumber="172"><td class="num" id="LN172">172</td><td class="line">      <span class='keyword'>if</span> (var.unsafeGetTensorImpl()-&gt;owns_pyobj()) {</td></tr>
<tr class="codeline" data-linenumber="173"><td class="num" id="LN173">173</td><td class="line">        <span class='comment'>// C++ owns the Python object; this implies there weren't any other</span></td></tr>
<tr class="codeline" data-linenumber="174"><td class="num" id="LN174">174</td><td class="line">        <span class='comment'>// owning references to the Python object.  Since we're making the</span></td></tr>
<tr class="codeline" data-linenumber="175"><td class="num" id="LN175">175</td><td class="line">        <span class='comment'>// object "live" again on Python side, let's flip back the ownership</span></td></tr>
<tr class="codeline" data-linenumber="176"><td class="num" id="LN176">176</td><td class="line">        <span class='comment'>// (Python owns C++) as it would now be unsound to deallocate the C++</span></td></tr>
<tr class="codeline" data-linenumber="177"><td class="num" id="LN177">177</td><td class="line">        <span class='comment'>// object if all C++ references go to zero</span></td></tr>
<tr class="codeline" data-linenumber="178"><td class="num" id="LN178">178</td><td class="line">        var.unsafeGetTensorImpl()-&gt;set_owns_pyobj(<span class='keyword'>false</span>);</td></tr>
<tr class="codeline" data-linenumber="179"><td class="num" id="LN179">179</td><td class="line">        <span class='keyword'>reinterpret_cast</span>&lt;THPVariable*&gt;(obj)-&gt;cdata =</td></tr>
<tr class="codeline" data-linenumber="180"><td class="num" id="LN180">180</td><td class="line">            MaybeOwned&lt;Variable&gt;::owned(std::move(var));</td></tr>
<tr class="codeline" data-linenumber="181"><td class="num" id="LN181">181</td><td class="line">        <span class='comment'>// NB: incref is not necessary, because we are "stealing" the previous</span></td></tr>
<tr class="codeline" data-linenumber="182"><td class="num" id="LN182">182</td><td class="line">        <span class='comment'>// ownership from the Variable to return it here for the wrap</span></td></tr>
<tr class="codeline" data-linenumber="183"><td class="num" id="LN183">183</td><td class="line">        <span class='keyword'>return</span> obj;</td></tr>
<tr class="codeline" data-linenumber="184"><td class="num" id="LN184">184</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="185"><td class="num" id="LN185">185</td><td class="line">      <span class='macro'>Py_INCREF(obj)<span class='macro_popup'>_Py_INCREF(((PyObject*)(obj)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="186"><td class="num" id="LN186">186</td><td class="line">      <span class='keyword'>return</span> obj;</td></tr>
<tr class="codeline" data-linenumber="187"><td class="num" id="LN187">187</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="188"><td class="num" id="LN188">188</td><td class="line">    <span class='comment'>// TODO: a better invariant is that if we tagged, we MUST have a valid</span></td></tr>
<tr class="codeline" data-linenumber="189"><td class="num" id="LN189">189</td><td class="line">    <span class='comment'>// PyObject.  That's PyObject preservation</span></td></tr>
<tr class="codeline" data-linenumber="190"><td class="num" id="LN190">190</td><td class="line">    <span class='comment'>// (https://github.com/pytorch/pytorch/pull/56017).  Prior to this PR</span></td></tr>
<tr class="codeline" data-linenumber="191"><td class="num" id="LN191">191</td><td class="line">    <span class='comment'>// being a thing, the PyObject field will get cleared when all references</span></td></tr>
<tr class="codeline" data-linenumber="192"><td class="num" id="LN192">192</td><td class="line">    <span class='comment'>// to the Python object are removed.</span></td></tr>
<tr class="codeline" data-linenumber="193"><td class="num" id="LN193">193</td><td class="line">    status = c10::impl::PyInterpreterStatus::TAGGED_BY_US;</td></tr>
<tr class="codeline" data-linenumber="194"><td class="num" id="LN194">194</td><td class="line">  } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="195"><td class="num" id="LN195">195</td><td class="line">    <span class='comment'>// Assumption: if a Tensor has been shared across threads, this induces</span></td></tr>
<tr class="codeline" data-linenumber="196"><td class="num" id="LN196">196</td><td class="line">    <span class='comment'>// a refcount bump.  Therefore, if the use count 1, we are the sole thread</span></td></tr>
<tr class="codeline" data-linenumber="197"><td class="num" id="LN197">197</td><td class="line">    <span class='comment'>// with access to this tensor and no race is possible.</span></td></tr>
<tr class="codeline" data-linenumber="198"><td class="num" id="LN198">198</td><td class="line">    <span class='keyword'>if</span> (var.use_count() &lt;= 1) {</td></tr>
<tr class="codeline" data-linenumber="199"><td class="num" id="LN199">199</td><td class="line">      status = c10::impl::PyInterpreterStatus::DEFINITELY_UNINITIALIZED;</td></tr>
<tr class="codeline" data-linenumber="200"><td class="num" id="LN200">200</td><td class="line">    } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="201"><td class="num" id="LN201">201</td><td class="line">      status = c10::impl::PyInterpreterStatus::MAYBE_UNINITIALIZED;</td></tr>
<tr class="codeline" data-linenumber="202"><td class="num" id="LN202">202</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="203"><td class="num" id="LN203">203</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="204"><td class="num" id="LN204">204</td><td class="line">  <span class='keyword'>return</span> THPVariable_NewWithVar(</td></tr>
<tr class="codeline" data-linenumber="205"><td class="num" id="LN205">205</td><td class="line">      (PyTypeObject*)THPVariableClass, std::move(var), status);</td></tr>
<tr class="codeline" data-linenumber="206"><td class="num" id="LN206">206</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="207"><td class="num" id="LN207">207</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="208"><td class="num" id="LN208">208</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>int</span> THPVariable_clear(THPVariable* self) {</td></tr>
<tr class="codeline" data-linenumber="209"><td class="num" id="LN209">209</td><td class="line">  <span class='macro'>Py_CLEAR(self-&gt;backward_hooks)<span class='macro_popup'>do { PyObject *_py_tmp = ((PyObject*)(self-&gt;backward_hooks<br>)); if (_py_tmp != __null) { (self-&gt;backward_hooks) = __null<br>; _Py_DECREF(((PyObject*)(_py_tmp))); } } while (0)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="210"><td class="num" id="LN210">210</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; tensor = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="211"><td class="num" id="LN211">211</td><td class="line">  <span class='keyword'>if</span> (tensor.defined()) {</td></tr>
<tr class="codeline" data-linenumber="212"><td class="num" id="LN212">212</td><td class="line">    <span class='comment'>// Two situations to consider:</span></td></tr>
<tr class="codeline" data-linenumber="213"><td class="num" id="LN213">213</td><td class="line">    <span class='comment'>//    PyObject -owns-&gt; Tensor</span></td></tr>
<tr class="codeline" data-linenumber="214"><td class="num" id="LN214">214</td><td class="line">    <span class='comment'>//        unsafeIsBorrowed() is FALSE.  We're obligated to look through</span></td></tr>
<tr class="codeline" data-linenumber="215"><td class="num" id="LN215">215</td><td class="line">    <span class='comment'>//        Tensor to break references.  Clearing cdata must induce the</span></td></tr>
<tr class="codeline" data-linenumber="216"><td class="num" id="LN216">216</td><td class="line">    <span class='comment'>//        destruction of the C++ Tensor.  If there were other references</span></td></tr>
<tr class="codeline" data-linenumber="217"><td class="num" id="LN217">217</td><td class="line">    <span class='comment'>//        to C++ tensor, the Python object would have been resurrected</span></td></tr>
<tr class="codeline" data-linenumber="218"><td class="num" id="LN218">218</td><td class="line">    <span class='comment'>//        by flipping the ownership.</span></td></tr>
<tr class="codeline" data-linenumber="219"><td class="num" id="LN219">219</td><td class="line">    <span class='comment'>//    Tensor -owns-&gt; PyObject</span></td></tr>
<tr class="codeline" data-linenumber="220"><td class="num" id="LN220">220</td><td class="line">    <span class='comment'>//        unsafeIsBorrowed() is TRUE.  We're deallocating the PyObject</span></td></tr>
<tr class="codeline" data-linenumber="221"><td class="num" id="LN221">221</td><td class="line">    <span class='comment'>//        because Tensor asked us to (it's already destructing).</span></td></tr>
<tr class="codeline" data-linenumber="222"><td class="num" id="LN222">222</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="223"><td class="num" id="LN223">223</td><td class="line">    <span class='keyword'>if</span> (!self-&gt;cdata.unsafeIsBorrowed()) {</td></tr>
<tr class="codeline" data-linenumber="224"><td class="num" id="LN224">224</td><td class="line">      <span class='comment'>// TODO: empirically, on OS X this assert appears to be untrue</span></td></tr>
<tr class="codeline" data-linenumber="225"><td class="num" id="LN225">225</td><td class="line">      <span class='comment'>// In test_py_tensors_multi_async_call - ProcessGroupRpcTestWithSpawn</span></td></tr>
<tr class="codeline" data-linenumber="226"><td class="num" id="LN226">226</td><td class="line">      <span class='comment'>// distributed/rpc/test_process_group_agent.py</span></td></tr>
<tr class="codeline" data-linenumber="227"><td class="num" id="LN227">227</td><td class="line">      <span class='comment'>//</span></td></tr>
<tr class="codeline" data-linenumber="228"><td class="num" id="LN228">228</td><td class="line">      <span class='comment'>//  libc++abi.dylib: terminating with uncaught exception of type</span></td></tr>
<tr class="codeline" data-linenumber="229"><td class="num" id="LN229">229</td><td class="line">      <span class='comment'>//  c10::Error: !tensor.unsafeGetTensorImpl()-&gt;owns_pyobj()INTERNAL ASSERT</span></td></tr>
<tr class="codeline" data-linenumber="230"><td class="num" id="LN230">230</td><td class="line">      <span class='comment'>//  FAILED at "../torch/csrc/autograd/python_variable.cpp":171, please</span></td></tr>
<tr class="codeline" data-linenumber="231"><td class="num" id="LN231">231</td><td class="line">      <span class='comment'>//  report a bug to PyTorch. Exception raised from THPVariable_clear at</span></td></tr>
<tr class="codeline" data-linenumber="232"><td class="num" id="LN232">232</td><td class="line">      <span class='comment'>//  ../torch/csrc/autograd/python_variable.cpp:171 (most recent call</span></td></tr>
<tr class="codeline" data-linenumber="233"><td class="num" id="LN233">233</td><td class="line">      <span class='comment'>//  first): frame #0: c10::Error::Error(c10::SourceLocation,</span></td></tr>
<tr class="codeline" data-linenumber="234"><td class="num" id="LN234">234</td><td class="line">      <span class='comment'>//  std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;,</span></td></tr>
<tr class="codeline" data-linenumber="235"><td class="num" id="LN235">235</td><td class="line">      <span class='comment'>//  std::__1::allocator&lt;char&gt; &gt;) + 98 (0x1158a0442 in libc10.dylib) frame</span></td></tr>
<tr class="codeline" data-linenumber="236"><td class="num" id="LN236">236</td><td class="line">      <span class='comment'>//  #1: c10::detail::torchCheckFail(char const*, char const*, unsigned</span></td></tr>
<tr class="codeline" data-linenumber="237"><td class="num" id="LN237">237</td><td class="line">      <span class='comment'>//  int, char const*) + 205 (0x11589ed3d in libc10.dylib) frame #2:</span></td></tr>
<tr class="codeline" data-linenumber="238"><td class="num" id="LN238">238</td><td class="line">      <span class='comment'>//  c10::detail::torchInternalAssertFail(char const*, char const*,</span></td></tr>
<tr class="codeline" data-linenumber="239"><td class="num" id="LN239">239</td><td class="line">      <span class='comment'>//  unsigned int, char const*, c10::detail::CompileTimeEmptyString) + 9</span></td></tr>
<tr class="codeline" data-linenumber="240"><td class="num" id="LN240">240</td><td class="line">      <span class='comment'>//  (0x1141e3f89 in libtorch_python.dylib) frame #3:</span></td></tr>
<tr class="codeline" data-linenumber="241"><td class="num" id="LN241">241</td><td class="line">      <span class='comment'>//  THPVariable_clear(THPVariable*) + 412 (0x1148a547c in</span></td></tr>
<tr class="codeline" data-linenumber="242"><td class="num" id="LN242">242</td><td class="line">      <span class='comment'>//  libtorch_python.dylib) frame #4:</span></td></tr>
<tr class="codeline" data-linenumber="243"><td class="num" id="LN243">243</td><td class="line">      <span class='comment'>//  THPVariable_subclass_dealloc(_object*) + 453 (0x1148a5035 in</span></td></tr>
<tr class="codeline" data-linenumber="244"><td class="num" id="LN244">244</td><td class="line">      <span class='comment'>//  libtorch_python.dylib) frame #5: (anonymous</span></td></tr>
<tr class="codeline" data-linenumber="245"><td class="num" id="LN245">245</td><td class="line">      <span class='comment'>//  namespace)::concrete_decref_fn(c10::impl::PyInterpreter const*,</span></td></tr>
<tr class="codeline" data-linenumber="246"><td class="num" id="LN246">246</td><td class="line">      <span class='comment'>//  _object*) + 53 (0x1148a5ea5 in libtorch_python.dylib) frame #6:</span></td></tr>
<tr class="codeline" data-linenumber="247"><td class="num" id="LN247">247</td><td class="line">      <span class='comment'>//  c10::TensorImpl::release_resources() + 182 (0x11588c4a6 in</span></td></tr>
<tr class="codeline" data-linenumber="248"><td class="num" id="LN248">248</td><td class="line">      <span class='comment'>//  libc10.dylib) frame #7:</span></td></tr>
<tr class="codeline" data-linenumber="249"><td class="num" id="LN249">249</td><td class="line">      <span class='comment'>//  c10::MaybeOwned&lt;at::Tensor&gt;::operator=(c10::MaybeOwned&lt;at::Tensor&gt;&amp;&amp;)</span></td></tr>
<tr class="codeline" data-linenumber="250"><td class="num" id="LN250">250</td><td class="line">      <span class='comment'>//  + 91 (0x11488c11b in libtorch_python.dylib) frame #8:</span></td></tr>
<tr class="codeline" data-linenumber="251"><td class="num" id="LN251">251</td><td class="line">      <span class='comment'>//  THPVariable_subclass_dealloc(_object*) + 607 (0x1148a50cf in</span></td></tr>
<tr class="codeline" data-linenumber="252"><td class="num" id="LN252">252</td><td class="line">      <span class='comment'>//  libtorch_python.dylib) &lt;omitting python frames&gt; frame #47: start + 1</span></td></tr>
<tr class="codeline" data-linenumber="253"><td class="num" id="LN253">253</td><td class="line">      <span class='comment'>//  (0x7fff6ffc7cc9 in libdyld.dylib) frame #48: 0x0 + 4 (0x4 in ???)</span></td></tr>
<tr class="codeline" data-linenumber="254"><td class="num" id="LN254">254</td><td class="line">      <span class='comment'>// TORCH_INTERNAL_ASSERT(!tensor.unsafeGetTensorImpl()-&gt;owns_pyobj());</span></td></tr>
<tr class="codeline" data-linenumber="255"><td class="num" id="LN255">255</td><td class="line">      <span class='keyword'>if</span> (<span class='keyword'>auto</span> grad_acc =</td></tr>
<tr class="codeline" data-linenumber="256"><td class="num" id="LN256">256</td><td class="line">              torch::autograd::impl::try_get_grad_accumulator(tensor)) {</td></tr>
<tr class="codeline" data-linenumber="257"><td class="num" id="LN257">257</td><td class="line">        grad_acc-&gt;pre_hooks().clear();</td></tr>
<tr class="codeline" data-linenumber="258"><td class="num" id="LN258">258</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="259"><td class="num" id="LN259">259</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="260"><td class="num" id="LN260">260</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="261"><td class="num" id="LN261">261</td><td class="line">  self-&gt;cdata = MaybeOwned&lt;Variable&gt;();</td></tr>
<tr class="codeline" data-linenumber="262"><td class="num" id="LN262">262</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="263"><td class="num" id="LN263">263</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="264"><td class="num" id="LN264">264</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="265"><td class="num" id="LN265">265</td><td class="line"><span class='comment'>// returns true if successfully rezzed; if so, cancel the</span></td></tr>
<tr class="codeline" data-linenumber="266"><td class="num" id="LN266">266</td><td class="line"><span class='comment'>// rest of deallocation</span></td></tr>
<tr class="codeline" data-linenumber="267"><td class="num" id="LN267">267</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>bool</span> THPVariable_tryResurrect(THPVariable* self) {</td></tr>
<tr class="codeline" data-linenumber="268"><td class="num" id="LN268">268</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; tensor = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="269"><td class="num" id="LN269">269</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="270"><td class="num" id="LN270">270</td><td class="line">  <span class='comment'>// Is this true or not???  Triggered by TestAutograd.test_variable_traverse</span></td></tr>
<tr class="codeline" data-linenumber="271"><td class="num" id="LN271">271</td><td class="line">  <span class='comment'>// TORCH_INTERNAL_ASSERT(tensor.defined());</span></td></tr>
<tr class="codeline" data-linenumber="272"><td class="num" id="LN272">272</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="273"><td class="num" id="LN273">273</td><td class="line">  <span class='comment'>// Check if there are other C++ owners</span></td></tr>
<tr class="codeline" data-linenumber="274"><td class="num" id="LN274">274</td><td class="line">  <span class='keyword'>if</span> (tensor.use_count() &lt;= 1) {</td></tr>
<tr class="codeline" data-linenumber="275"><td class="num" id="LN275">275</td><td class="line">    <span class='keyword'>return</span> <span class='keyword'>false</span>;</td></tr>
<tr class="codeline" data-linenumber="276"><td class="num" id="LN276">276</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="277"><td class="num" id="LN277">277</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="278"><td class="num" id="LN278">278</td><td class="line">  <span class='comment'>// There are other C++ owners of the tensor.  Flip ownership</span></td></tr>
<tr class="codeline" data-linenumber="279"><td class="num" id="LN279">279</td><td class="line">  <span class='comment'>// so that C++ owns this Python object, and cancel deallocation.</span></td></tr>
<tr class="codeline" data-linenumber="280"><td class="num" id="LN280">280</td><td class="line">  <span class='macro'>TORCH_INTERNAL_ASSERT(!tensor.unsafeGetTensorImpl()-&gt;owns_pyobj())<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(!tensor.unsafeGetTensorImpl<br>()-&gt;owns_pyobj())), 0))) { ::c10::detail::torchInternalAssertFail<br>( __func__, "../torch/csrc/autograd/python_variable.cpp", static_cast<br>&lt;uint32_t&gt;(280), "!tensor.unsafeGetTensorImpl()-&gt;owns_pyobj()"<br> "INTERNAL ASSERT FAILED at " "\"../torch/csrc/autograd/python_variable.cpp\""<br> ":" "280" ", please report a bug to PyTorch. ", c10::str());<br> }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="281"><td class="num" id="LN281">281</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="282"><td class="num" id="LN282">282</td><td class="line">  tensor.unsafeGetTensorImpl()-&gt;set_owns_pyobj(<span class='keyword'>true</span>);</td></tr>
<tr class="codeline" data-linenumber="283"><td class="num" id="LN283">283</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="284"><td class="num" id="LN284">284</td><td class="line"><span class='comment'>// Resurrect the Python object.  This is something CPython does</span></td></tr>
<tr class="codeline" data-linenumber="285"><td class="num" id="LN285">285</td><td class="line"><span class='comment'>// internally occasionally, see</span></td></tr>
<tr class="codeline" data-linenumber="286"><td class="num" id="LN286">286</td><td class="line"><span class='comment'>// https://github.com/python/cpython/blob/b98eba5bc2ffbe7a0ed49d540ebc4f756ae61985/Objects/object.c#L248-L259</span></td></tr>
<tr class="codeline" data-linenumber="287"><td class="num" id="LN287">287</td><td class="line"><span class='comment'>// so we just copy the pattern here.  Note that we don't have to worry</span></td></tr>
<tr class="codeline" data-linenumber="288"><td class="num" id="LN288">288</td><td class="line"><span class='comment'>// about saving and restoring the refcount (as the quoted code does)</span></td></tr>
<tr class="codeline" data-linenumber="289"><td class="num" id="LN289">289</td><td class="line"><span class='comment'>// because we actually DO need to reset the refcount to one here, we</span></td></tr>
<tr class="codeline" data-linenumber="290"><td class="num" id="LN290">290</td><td class="line"><span class='comment'>// can't assume that some other code has taken care of it.</span></td></tr>
<tr class="codeline" data-linenumber="291"><td class="num" id="LN291">291</td><td class="line"><span class='comment'>// NB: this will overreport _Py_RefTotal but based on inspection of object.c</span></td></tr>
<tr class="codeline" data-linenumber="292"><td class="num" id="LN292">292</td><td class="line"><span class='comment'>// there is no way to avoid this</span></td></tr>
<tr class="codeline" data-linenumber="293"><td class="num" id="LN293">293</td><td class="line"><span class='directive'>#ifdef Py_TRACE_REFS</span></td></tr>
<tr class="codeline" data-linenumber="294"><td class="num" id="LN294">294</td><td class="line">  _Py_AddToAllObjects(<span class='keyword'>reinterpret_cast</span>&lt;PyObject *&gt;(self), 1);</td></tr>
<tr class="codeline" data-linenumber="295"><td class="num" id="LN295">295</td><td class="line"><span class='directive'>#endif</span></td></tr>
<tr class="codeline" data-linenumber="296"><td class="num" id="LN296">296</td><td class="line">  <span class='macro'>Py_INCREF(self)<span class='macro_popup'>_Py_INCREF(((PyObject*)(self)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="297"><td class="num" id="LN297">297</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="298"><td class="num" id="LN298">298</td><td class="line">  <span class='comment'>// Flip THPVariable to be non-owning</span></td></tr>
<tr class="codeline" data-linenumber="299"><td class="num" id="LN299">299</td><td class="line">  <span class='comment'>// (near use-after-free miss here: fresh MaybeOwned is created breaking</span></td></tr>
<tr class="codeline" data-linenumber="300"><td class="num" id="LN300">300</td><td class="line">  <span class='comment'>// reference on Tensor in struct BEFORE we overwrite the old one)</span></td></tr>
<tr class="codeline" data-linenumber="301"><td class="num" id="LN301">301</td><td class="line">  self-&gt;cdata = MaybeOwned&lt;Variable&gt;::borrowed(tensor);</td></tr>
<tr class="codeline" data-linenumber="302"><td class="num" id="LN302">302</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="303"><td class="num" id="LN303">303</td><td class="line">  <span class='comment'>// NB: At this point, tensor *could* be dead (e.g., some other C++ thread</span></td></tr>
<tr class="codeline" data-linenumber="304"><td class="num" id="LN304">304</td><td class="line">  <span class='comment'>// decrefed it.)  At this point, it is probably waiting on the GIL to</span></td></tr>
<tr class="codeline" data-linenumber="305"><td class="num" id="LN305">305</td><td class="line">  <span class='comment'>// deallocate the Python object and will kill self, BUT NOT YET.</span></td></tr>
<tr class="codeline" data-linenumber="306"><td class="num" id="LN306">306</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="307"><td class="num" id="LN307">307</td><td class="line">  <span class='keyword'>return</span> <span class='keyword'>true</span>;</td></tr>
<tr class="codeline" data-linenumber="308"><td class="num" id="LN308">308</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="309"><td class="num" id="LN309">309</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="310"><td class="num" id="LN310">310</td><td class="line">PyObject *THPVariable_pynew(PyTypeObject *type, PyObject *args, PyObject *kwargs);</td></tr>
<tr class="codeline" data-linenumber="311"><td class="num" id="LN311">311</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="312"><td class="num" id="LN312">312</td><td class="line"><span class='keyword'>static</span> PyObject* THPVariable_fix_weakref(PyObject* self, PyObject* noargs) {</td></tr>
<tr class="codeline" data-linenumber="313"><td class="num" id="LN313">313</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="314"><td class="num" id="LN314">314</td><td class="line">  THPVariable_Wrap(var);</td></tr>
<tr class="codeline" data-linenumber="315"><td class="num" id="LN315">315</td><td class="line">  <span class='macro'>Py_RETURN_NONE<span class='macro_popup'>return _Py_INCREF(((PyObject*)((&amp;_Py_NoneStruct)))), (&amp;<br>_Py_NoneStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="316"><td class="num" id="LN316">316</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="317"><td class="num" id="LN317">317</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="318"><td class="num" id="LN318">318</td><td class="line"><span class='comment'>// Instantiates a subclass of self with the same data.</span></td></tr>
<tr class="codeline" data-linenumber="319"><td class="num" id="LN319">319</td><td class="line"><span class='keyword'>static</span> PyObject* THPVariable_as_subclass(PyObject* _self, PyObject* args, PyObject* kwargs) {</td></tr>
<tr class="codeline" data-linenumber="320"><td class="num" id="LN320">320</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="321"><td class="num" id="LN321">321</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; self = THPVariable_Unpack(_self);</td></tr>
<tr class="codeline" data-linenumber="322"><td class="num" id="LN322">322</td><td class="line">  <span class='keyword'>static</span> PythonArgParser parser({</td></tr>
<tr class="codeline" data-linenumber="323"><td class="num" id="LN323">323</td><td class="line">    <span class='string_literal'>"as_subclass(PyObject* cls)"</span>,</td></tr>
<tr class="codeline" data-linenumber="324"><td class="num" id="LN324">324</td><td class="line">  });</td></tr>
<tr class="codeline" data-linenumber="325"><td class="num" id="LN325">325</td><td class="line">  ParsedArgs&lt;1&gt; parsed_args{};</td></tr>
<tr class="codeline" data-linenumber="326"><td class="num" id="LN326">326</td><td class="line">  <span class='keyword'>auto</span> r = parser.parse(_self, args, kwargs, parsed_args);</td></tr>
<tr class="codeline" data-linenumber="327"><td class="num" id="LN327">327</td><td class="line">  PyObject* cls = r.pyobject(0);</td></tr>
<tr class="codeline" data-linenumber="328"><td class="num" id="LN328">328</td><td class="line">  <span class='keyword'>if</span> (!<span class='macro'>PyType_Check(cls)<span class='macro_popup'>((((((PyObject*)(cls))-&gt;ob_type))-&gt;tp_flags &amp; ((1UL<br> &lt;&lt; 31))) != 0)</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="329"><td class="num" id="LN329">329</td><td class="line">    <span class='keyword'>throw</span> torch::TypeError(<span class='string_literal'>"cls must be a type (got %s)"</span>, <span class='macro'>Py_TYPE(cls)<span class='macro_popup'>(((PyObject*)(cls))-&gt;ob_type)</span></span>-&gt;tp_name);</td></tr>
<tr class="codeline" data-linenumber="330"><td class="num" id="LN330">330</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="331"><td class="num" id="LN331">331</td><td class="line">  <span class='keyword'>return</span> THPVariable_NewWithVar(</td></tr>
<tr class="codeline" data-linenumber="332"><td class="num" id="LN332">332</td><td class="line">      (PyTypeObject*)cls,</td></tr>
<tr class="codeline" data-linenumber="333"><td class="num" id="LN333">333</td><td class="line">      self.alias(),</td></tr>
<tr class="codeline" data-linenumber="334"><td class="num" id="LN334">334</td><td class="line">      c10::impl::PyInterpreterStatus::DEFINITELY_UNINITIALIZED);</td></tr>
<tr class="codeline" data-linenumber="335"><td class="num" id="LN335">335</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="336"><td class="num" id="LN336">336</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="337"><td class="num" id="LN337">337</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="338"><td class="num" id="LN338">338</td><td class="line"><span class='keyword'>static</span> PyObject* THPVariable_make_subclass(PyObject* _ignored, PyObject* args, PyObject* kwargs) {</td></tr>
<tr class="codeline" data-linenumber="339"><td class="num" id="LN339">339</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="340"><td class="num" id="LN340">340</td><td class="line">  <span class='keyword'>static</span> PythonArgParser parser({</td></tr>
<tr class="codeline" data-linenumber="341"><td class="num" id="LN341">341</td><td class="line">    <span class='string_literal'>"_make_subclass(PyObject* cls, Tensor data, bool require_grad=False)"</span>,</td></tr>
<tr class="codeline" data-linenumber="342"><td class="num" id="LN342">342</td><td class="line">  });</td></tr>
<tr class="codeline" data-linenumber="343"><td class="num" id="LN343">343</td><td class="line">  ParsedArgs&lt;3&gt; parsed_args{};</td></tr>
<tr class="codeline" data-linenumber="344"><td class="num" id="LN344">344</td><td class="line">  <span class='keyword'>auto</span> r = parser.parse(args, kwargs, parsed_args);</td></tr>
<tr class="codeline" data-linenumber="345"><td class="num" id="LN345">345</td><td class="line">  PyObject* cls = r.pyobject(0);</td></tr>
<tr class="codeline" data-linenumber="346"><td class="num" id="LN346">346</td><td class="line">  <span class='keyword'>if</span> (!<span class='macro'>PyType_Check(cls)<span class='macro_popup'>((((((PyObject*)(cls))-&gt;ob_type))-&gt;tp_flags &amp; ((1UL<br> &lt;&lt; 31))) != 0)</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="347"><td class="num" id="LN347">347</td><td class="line">    <span class='keyword'>throw</span> torch::TypeError(<span class='string_literal'>"cls must be a type (got %s)"</span>, <span class='macro'>Py_TYPE(cls)<span class='macro_popup'>(((PyObject*)(cls))-&gt;ob_type)</span></span>-&gt;tp_name);</td></tr>
<tr class="codeline" data-linenumber="348"><td class="num" id="LN348">348</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="349"><td class="num" id="LN349">349</td><td class="line">  <span class='keyword'>auto</span> data =</td></tr>
<tr class="codeline" data-linenumber="350"><td class="num" id="LN350">350</td><td class="line">      r.tensor(1).detach(); <span class='comment'>// creates a fresh Tensor (DEFINITELY_UNINITIALIZED)</span></td></tr>
<tr class="codeline" data-linenumber="351"><td class="num" id="LN351">351</td><td class="line">  <span class='comment'>// We set `data`'s `allow_tensor_metadata_change` to true here, because we want to</span></td></tr>
<tr class="codeline" data-linenumber="352"><td class="num" id="LN352">352</td><td class="line">  <span class='comment'>// allow the following use case for backward compatibility:</span></td></tr>
<tr class="codeline" data-linenumber="353"><td class="num" id="LN353">353</td><td class="line">  <span class='comment'>//</span></td></tr>
<tr class="codeline" data-linenumber="354"><td class="num" id="LN354">354</td><td class="line">  <span class='comment'>// ```python</span></td></tr>
<tr class="codeline" data-linenumber="355"><td class="num" id="LN355">355</td><td class="line">  <span class='comment'>// rnn = torch.nn.RNN(100, 100, 2)</span></td></tr>
<tr class="codeline" data-linenumber="356"><td class="num" id="LN356">356</td><td class="line">  <span class='comment'>// # The following calls `torch._cudnn_rnn_flatten_weight(rnn._flat_weights, ...)`,</span></td></tr>
<tr class="codeline" data-linenumber="357"><td class="num" id="LN357">357</td><td class="line">  <span class='comment'>// # which changes storage of `rnn`'s weights in-place</span></td></tr>
<tr class="codeline" data-linenumber="358"><td class="num" id="LN358">358</td><td class="line">  <span class='comment'>// rnn.flatten_parameters()</span></td></tr>
<tr class="codeline" data-linenumber="359"><td class="num" id="LN359">359</td><td class="line">  <span class='comment'>// ```</span></td></tr>
<tr class="codeline" data-linenumber="360"><td class="num" id="LN360">360</td><td class="line">  data.unsafeGetTensorImpl()-&gt;set_allow_tensor_metadata_change(<span class='keyword'>true</span>);</td></tr>
<tr class="codeline" data-linenumber="361"><td class="num" id="LN361">361</td><td class="line">  data.set_requires_grad(r.toBool(2));</td></tr>
<tr class="codeline" data-linenumber="362"><td class="num" id="LN362">362</td><td class="line">  <span class='keyword'>return</span> THPVariable_NewWithVar(</td></tr>
<tr class="codeline" data-linenumber="363"><td class="num" id="LN363">363</td><td class="line">      (PyTypeObject*)cls,</td></tr>
<tr class="codeline" data-linenumber="364"><td class="num" id="LN364">364</td><td class="line">      std::move(data),</td></tr>
<tr class="codeline" data-linenumber="365"><td class="num" id="LN365">365</td><td class="line">      c10::impl::PyInterpreterStatus::DEFINITELY_UNINITIALIZED);</td></tr>
<tr class="codeline" data-linenumber="366"><td class="num" id="LN366">366</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="367"><td class="num" id="LN367">367</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="368"><td class="num" id="LN368">368</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="369"><td class="num" id="LN369">369</td><td class="line"><span class='keyword'>typedef</span> PyObject *(*getter)(PyObject *, <span class='keyword'>void</span> *);</td></tr>
<tr class="codeline" data-linenumber="370"><td class="num" id="LN370">370</td><td class="line"><span class='keyword'>typedef</span> <span class='keyword'>int</span> (*setter)(PyObject *, PyObject *, <span class='keyword'>void</span> *);</td></tr>
<tr class="codeline" data-linenumber="371"><td class="num" id="LN371">371</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="372"><td class="num" id="LN372">372</td><td class="line">PyObject *THPVariable_get_python_dispatch(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="373"><td class="num" id="LN373">373</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="374"><td class="num" id="LN374">374</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="375"><td class="num" id="LN375">375</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="376"><td class="num" id="LN376">376</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(var.unsafeGetTensorImpl()-&gt;is_python_dispatch());</td></tr>
<tr class="codeline" data-linenumber="377"><td class="num" id="LN377">377</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="378"><td class="num" id="LN378">378</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="379"><td class="num" id="LN379">379</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="380"><td class="num" id="LN380">380</td><td class="line">PyObject *THPVariable_get_T(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="381"><td class="num" id="LN381">381</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="382"><td class="num" id="LN382">382</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="383"><td class="num" id="LN383">383</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="384"><td class="num" id="LN384">384</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"T"</span>);</td></tr>
<tr class="codeline" data-linenumber="385"><td class="num" id="LN385">385</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="386"><td class="num" id="LN386">386</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="387"><td class="num" id="LN387">387</td><td class="line">  <span class='keyword'>return</span> THPVariable_Wrap(var.numpy_T());</td></tr>
<tr class="codeline" data-linenumber="388"><td class="num" id="LN388">388</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="389"><td class="num" id="LN389">389</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="390"><td class="num" id="LN390">390</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="391"><td class="num" id="LN391">391</td><td class="line">PyObject *THPVariable_get_cdata(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="392"><td class="num" id="LN392">392</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="393"><td class="num" id="LN393">393</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="394"><td class="num" id="LN394">394</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="395"><td class="num" id="LN395">395</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"_cdata"</span>);</td></tr>
<tr class="codeline" data-linenumber="396"><td class="num" id="LN396">396</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="397"><td class="num" id="LN397">397</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="398"><td class="num" id="LN398">398</td><td class="line">  <span class='keyword'>return</span> PyLong_FromVoidPtr(var.unsafeGetTensorImpl());</td></tr>
<tr class="codeline" data-linenumber="399"><td class="num" id="LN399">399</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="400"><td class="num" id="LN400">400</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="401"><td class="num" id="LN401">401</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="402"><td class="num" id="LN402">402</td><td class="line">PyObject *THPVariable_get_version(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="403"><td class="num" id="LN403">403</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="404"><td class="num" id="LN404">404</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="405"><td class="num" id="LN405">405</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="406"><td class="num" id="LN406">406</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"_version"</span>);</td></tr>
<tr class="codeline" data-linenumber="407"><td class="num" id="LN407">407</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="408"><td class="num" id="LN408">408</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="409"><td class="num" id="LN409">409</td><td class="line">  <span class='keyword'>return</span> <span class='macro'>PyInt_FromLong<span class='macro_popup'>PyLong_FromLong</span></span>(var._version());</td></tr>
<tr class="codeline" data-linenumber="410"><td class="num" id="LN410">410</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="411"><td class="num" id="LN411">411</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="412"><td class="num" id="LN412">412</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="413"><td class="num" id="LN413">413</td><td class="line">PyObject *THPVariable_get_grad_fn(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="414"><td class="num" id="LN414">414</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="415"><td class="num" id="LN415">415</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="416"><td class="num" id="LN416">416</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="417"><td class="num" id="LN417">417</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"grad_fn"</span>);</td></tr>
<tr class="codeline" data-linenumber="418"><td class="num" id="LN418">418</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="419"><td class="num" id="LN419">419</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="420"><td class="num" id="LN420">420</td><td class="line">  <span class='keyword'>if</span> (!var.grad_fn()) {</td></tr>
<tr class="codeline" data-linenumber="421"><td class="num" id="LN421">421</td><td class="line">    <span class='macro'>Py_RETURN_NONE<span class='macro_popup'>return _Py_INCREF(((PyObject*)((&amp;_Py_NoneStruct)))), (&amp;<br>_Py_NoneStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="422"><td class="num" id="LN422">422</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="423"><td class="num" id="LN423">423</td><td class="line">  <span class='keyword'>return</span> functionToPyObject(var.grad_fn());</td></tr>
<tr class="codeline" data-linenumber="424"><td class="num" id="LN424">424</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="425"><td class="num" id="LN425">425</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="426"><td class="num" id="LN426">426</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="427"><td class="num" id="LN427">427</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>int</span> THPVariable_set_grad_fn(THPVariable *self, PyObject *obj, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="428"><td class="num" id="LN428">428</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="429"><td class="num" id="LN429">429</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="430"><td class="num" id="LN430">430</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="431"><td class="num" id="LN431">431</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_setter(self, <span class='string_literal'>"_grad_fn"</span>, obj);</td></tr>
<tr class="codeline" data-linenumber="432"><td class="num" id="LN432">432</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="433"><td class="num" id="LN433">433</td><td class="line">  <span class='macro'>THPUtils_assertRet(-1, obj, <span class='string_literal'>"Deletion of _grad_fn not allowed. Detach tensor instead!"</span>)<span class='macro_popup'>if ((__builtin_expect((!(obj)), (0)))) { THPUtils_setError("Deletion of _grad_fn not allowed. Detach tensor instead!"<br>); return -1; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="434"><td class="num" id="LN434">434</td><td class="line">  <span class='macro'>THPUtils_assertRet(-1, obj == Py_None, <span class='string_literal'>"_grad_fn can be only set to None"</span>)<span class='macro_popup'>if ((__builtin_expect((!(obj == (&amp;_Py_NoneStruct))), (0))<br>)) { THPUtils_setError("_grad_fn can be only set to None"); return<br> -1; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="435"><td class="num" id="LN435">435</td><td class="line">  THPVariable_Unpack(self).detach_();</td></tr>
<tr class="codeline" data-linenumber="436"><td class="num" id="LN436">436</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="437"><td class="num" id="LN437">437</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS_RET(-1)<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return -1; }<br> catch (const c10::IndexError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_IndexError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::ValueError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return -1; } catch (torch::PyTorchError<br> &amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(e.python_type(), msg); return -1; } catch (const std::exception<br>&amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(PyExc_RuntimeError, msg); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="438"><td class="num" id="LN438">438</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="439"><td class="num" id="LN439">439</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="440"><td class="num" id="LN440">440</td><td class="line"><span class='keyword'>static</span> PyObject *THPVariable_is_leaf(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="441"><td class="num" id="LN441">441</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="442"><td class="num" id="LN442">442</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="443"><td class="num" id="LN443">443</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="444"><td class="num" id="LN444">444</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_leaf"</span>);</td></tr>
<tr class="codeline" data-linenumber="445"><td class="num" id="LN445">445</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="446"><td class="num" id="LN446">446</td><td class="line">  <span class='keyword'>return</span> PyBool_FromLong(!THPVariable_Unpack(self).grad_fn());</td></tr>
<tr class="codeline" data-linenumber="447"><td class="num" id="LN447">447</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="448"><td class="num" id="LN448">448</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="449"><td class="num" id="LN449">449</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="450"><td class="num" id="LN450">450</td><td class="line"><span class='keyword'>static</span> PyObject * THPVariable_get_data(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="451"><td class="num" id="LN451">451</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="452"><td class="num" id="LN452">452</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="453"><td class="num" id="LN453">453</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="454"><td class="num" id="LN454">454</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"data"</span>);</td></tr>
<tr class="codeline" data-linenumber="455"><td class="num" id="LN455">455</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="456"><td class="num" id="LN456">456</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(self).variable_data();</td></tr>
<tr class="codeline" data-linenumber="457"><td class="num" id="LN457">457</td><td class="line">  <span class='keyword'>return</span> THPVariable_Wrap(var);</td></tr>
<tr class="codeline" data-linenumber="458"><td class="num" id="LN458">458</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="459"><td class="num" id="LN459">459</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="460"><td class="num" id="LN460">460</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="461"><td class="num" id="LN461">461</td><td class="line"><span class='keyword'>int</span> THPVariable_set_data(THPVariable *self, PyObject *data, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="462"><td class="num" id="LN462">462</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="463"><td class="num" id="LN463">463</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="464"><td class="num" id="LN464">464</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="465"><td class="num" id="LN465">465</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_setter(self, <span class='string_literal'>"data"</span>, data);</td></tr>
<tr class="codeline" data-linenumber="466"><td class="num" id="LN466">466</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="467"><td class="num" id="LN467">467</td><td class="line">  <span class='macro'>THPUtils_assertRet(-1, data, <span class='string_literal'>"Deleting tensor data is not allowed. Delete tensor instead!"</span>)<span class='macro_popup'>if ((__builtin_expect((!(data)), (0)))) { THPUtils_setError("Deleting tensor data is not allowed. Delete tensor instead!"<br>); return -1; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="468"><td class="num" id="LN468">468</td><td class="line">  <span class='keyword'>if</span> (!THPVariable_Check(data)) {</td></tr>
<tr class="codeline" data-linenumber="469"><td class="num" id="LN469">469</td><td class="line">    <span class='keyword'>throw</span> torch::TypeError(<span class='string_literal'>"Variable data has to be a tensor, but got %s"</span>, <span class='macro'>Py_TYPE(data)<span class='macro_popup'>(((PyObject*)(data))-&gt;ob_type)</span></span>-&gt;tp_name);</td></tr>
<tr class="codeline" data-linenumber="470"><td class="num" id="LN470">470</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="471"><td class="num" id="LN471">471</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="472"><td class="num" id="LN472">472</td><td class="line">  THPVariable_Unpack(self).set_data(THPVariable_Unpack(data));</td></tr>
<tr class="codeline" data-linenumber="473"><td class="num" id="LN473">473</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="474"><td class="num" id="LN474">474</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS_RET(-1)<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return -1; }<br> catch (const c10::IndexError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_IndexError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::ValueError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return -1; } catch (torch::PyTorchError<br> &amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(e.python_type(), msg); return -1; } catch (const std::exception<br>&amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(PyExc_RuntimeError, msg); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="475"><td class="num" id="LN475">475</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="476"><td class="num" id="LN476">476</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="477"><td class="num" id="LN477">477</td><td class="line">PyObject *THPVariable_get_grad(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="478"><td class="num" id="LN478">478</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="479"><td class="num" id="LN479">479</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="480"><td class="num" id="LN480">480</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="481"><td class="num" id="LN481">481</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"grad"</span>);</td></tr>
<tr class="codeline" data-linenumber="482"><td class="num" id="LN482">482</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="483"><td class="num" id="LN483">483</td><td class="line">  <span class='keyword'>return</span> THPVariable_Wrap(THPVariable_Unpack(self).grad());</td></tr>
<tr class="codeline" data-linenumber="484"><td class="num" id="LN484">484</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="485"><td class="num" id="LN485">485</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="486"><td class="num" id="LN486">486</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="487"><td class="num" id="LN487">487</td><td class="line"><span class='keyword'>int</span> THPVariable_set_grad(THPVariable *self, PyObject *py_grad, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="488"><td class="num" id="LN488">488</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="489"><td class="num" id="LN489">489</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="490"><td class="num" id="LN490">490</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="491"><td class="num" id="LN491">491</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_setter(self, <span class='string_literal'>"grad"</span>, py_grad);</td></tr>
<tr class="codeline" data-linenumber="492"><td class="num" id="LN492">492</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="493"><td class="num" id="LN493">493</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="494"><td class="num" id="LN494">494</td><td class="line">  <span class='keyword'>if</span> (!py_grad || py_grad == <span class='macro'>Py_None<span class='macro_popup'>(&amp;_Py_NoneStruct)</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="495"><td class="num" id="LN495">495</td><td class="line">    var.mutable_grad().reset();</td></tr>
<tr class="codeline" data-linenumber="496"><td class="num" id="LN496">496</td><td class="line">    <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="497"><td class="num" id="LN497">497</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="498"><td class="num" id="LN498">498</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="499"><td class="num" id="LN499">499</td><td class="line">  <span class='macro'>THPUtils_assertRet(-1, self != (THPVariable*)py_grad,<span class='macro_popup'>if ((__builtin_expect((!(self != (THPVariable*)py_grad)), (0)<br>))) { THPUtils_setError("can't assign Variable as its own grad"<br>); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="500"><td class="num" id="LN500">500</td><td class="line">      <span class='string_literal'><span class='macro'>"can't assign Variable as its own grad"</span>)<span class='macro_popup'>if ((__builtin_expect((!(self != (THPVariable*)py_grad)), (0)<br>))) { THPUtils_setError("can't assign Variable as its own grad"<br>); return -1; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="501"><td class="num" id="LN501">501</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="502"><td class="num" id="LN502">502</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; grad = THPVariable_Unpack(py_grad);</td></tr>
<tr class="codeline" data-linenumber="503"><td class="num" id="LN503">503</td><td class="line">  <span class='keyword'>bool</span> gradIsSparse = (var.dtype() == grad.dtype() &amp;&amp;</td></tr>
<tr class="codeline" data-linenumber="504"><td class="num" id="LN504">504</td><td class="line">                       var.device().type() == grad.device().type() &amp;&amp;</td></tr>
<tr class="codeline" data-linenumber="505"><td class="num" id="LN505">505</td><td class="line">                       grad.layout() == kSparse);</td></tr>
<tr class="codeline" data-linenumber="506"><td class="num" id="LN506">506</td><td class="line">  <span class='macro'>THPUtils_assertRet(-1, grad.options().type_equal(var.options()) || gradIsSparse,<span class='macro_popup'>if ((__builtin_expect((!(grad.options().type_equal(var.options<br>()) || gradIsSparse)), (0)))) { THPUtils_setError("assigned grad has data of a different type"<br>); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="507"><td class="num" id="LN507">507</td><td class="line">      <span class='string_literal'><span class='macro'>"assigned grad has data of a different type"</span>)<span class='macro_popup'>if ((__builtin_expect((!(grad.options().type_equal(var.options<br>()) || gradIsSparse)), (0)))) { THPUtils_setError("assigned grad has data of a different type"<br>); return -1; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="508"><td class="num" id="LN508">508</td><td class="line">  <span class='keyword'>if</span> (var.is_cuda()) {</td></tr>
<tr class="codeline" data-linenumber="509"><td class="num" id="LN509">509</td><td class="line">    <span class='macro'>THPUtils_assertRet(-1, grad.get_device() == var.get_device(),<span class='macro_popup'>if ((__builtin_expect((!(grad.get_device() == var.get_device(<br>))), (0)))) { THPUtils_setError("assigned grad has data located on a different device"<br>); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="510"><td class="num" id="LN510">510</td><td class="line">        <span class='string_literal'><span class='macro'>"assigned grad has data located on a different device"</span>)<span class='macro_popup'>if ((__builtin_expect((!(grad.get_device() == var.get_device(<br>))), (0)))) { THPUtils_setError("assigned grad has data located on a different device"<br>); return -1; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="511"><td class="num" id="LN511">511</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="512"><td class="num" id="LN512">512</td><td class="line">  <span class='macro'>THPUtils_assertRet(-1, grad.sizes().equals(var.sizes()),<span class='macro_popup'>if ((__builtin_expect((!(grad.sizes().equals(var.sizes()))), (<br>0)))) { THPUtils_setError("assigned grad has data of a different size"<br>); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="513"><td class="num" id="LN513">513</td><td class="line">      <span class='string_literal'><span class='macro'>"assigned grad has data of a different size"</span>)<span class='macro_popup'>if ((__builtin_expect((!(grad.sizes().equals(var.sizes()))), (<br>0)))) { THPUtils_setError("assigned grad has data of a different size"<br>); return -1; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="514"><td class="num" id="LN514">514</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="515"><td class="num" id="LN515">515</td><td class="line">  var.mutable_grad() = grad;</td></tr>
<tr class="codeline" data-linenumber="516"><td class="num" id="LN516">516</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="517"><td class="num" id="LN517">517</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS_RET(-1)<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return -1; }<br> catch (const c10::IndexError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_IndexError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::ValueError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return -1; } catch (torch::PyTorchError<br> &amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(e.python_type(), msg); return -1; } catch (const std::exception<br>&amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(PyExc_RuntimeError, msg); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="518"><td class="num" id="LN518">518</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="519"><td class="num" id="LN519">519</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="520"><td class="num" id="LN520">520</td><td class="line">PyObject *THPVariable_get_volatile(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="521"><td class="num" id="LN521">521</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="522"><td class="num" id="LN522">522</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="523"><td class="num" id="LN523">523</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="524"><td class="num" id="LN524">524</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"volatile"</span>);</td></tr>
<tr class="codeline" data-linenumber="525"><td class="num" id="LN525">525</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="526"><td class="num" id="LN526">526</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>char</span>* msg = <span class='string_literal'>"volatile was removed (Variable.volatile is always False)"</span>;</td></tr>
<tr class="codeline" data-linenumber="527"><td class="num" id="LN527">527</td><td class="line">  <span class='keyword'>auto</span> r = PyErr_WarnEx(PyExc_UserWarning, msg, 1);</td></tr>
<tr class="codeline" data-linenumber="528"><td class="num" id="LN528">528</td><td class="line">  <span class='keyword'>if</span> (r != 0) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="529"><td class="num" id="LN529">529</td><td class="line">  <span class='macro'>Py_RETURN_FALSE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_FalseStruct<br>)))), ((PyObject *) &amp;_Py_FalseStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="530"><td class="num" id="LN530">530</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="531"><td class="num" id="LN531">531</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="532"><td class="num" id="LN532">532</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="533"><td class="num" id="LN533">533</td><td class="line"><span class='keyword'>int</span> THPVariable_set_volatile(THPVariable *self, PyObject *obj, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="534"><td class="num" id="LN534">534</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="535"><td class="num" id="LN535">535</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="536"><td class="num" id="LN536">536</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="537"><td class="num" id="LN537">537</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_setter(self, <span class='string_literal'>"volatile"</span>, obj);</td></tr>
<tr class="codeline" data-linenumber="538"><td class="num" id="LN538">538</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="539"><td class="num" id="LN539">539</td><td class="line">  <span class='keyword'>auto</span> r = PyErr_WarnEx(PyExc_UserWarning, VOLATILE_WARNING, 1);</td></tr>
<tr class="codeline" data-linenumber="540"><td class="num" id="LN540">540</td><td class="line">  <span class='keyword'>if</span> (r != 0) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="541"><td class="num" id="LN541">541</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="542"><td class="num" id="LN542">542</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS_RET(-1)<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return -1; }<br> catch (const c10::IndexError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_IndexError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::ValueError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return -1; } catch (torch::PyTorchError<br> &amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(e.python_type(), msg); return -1; } catch (const std::exception<br>&amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(PyExc_RuntimeError, msg); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="543"><td class="num" id="LN543">543</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="544"><td class="num" id="LN544">544</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="545"><td class="num" id="LN545">545</td><td class="line">PyObject *THPVariable_get_output_nr(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="546"><td class="num" id="LN546">546</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="547"><td class="num" id="LN547">547</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="548"><td class="num" id="LN548">548</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="549"><td class="num" id="LN549">549</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"output_nr"</span>);</td></tr>
<tr class="codeline" data-linenumber="550"><td class="num" id="LN550">550</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="551"><td class="num" id="LN551">551</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span> output_nr = <span class='keyword'>static_cast</span>&lt;<span class='keyword'>long</span>&gt;(THPVariable_Unpack(self).output_nr());</td></tr>
<tr class="codeline" data-linenumber="552"><td class="num" id="LN552">552</td><td class="line">  <span class='keyword'>return</span> <span class='macro'>PyInt_FromLong<span class='macro_popup'>PyLong_FromLong</span></span>(output_nr);</td></tr>
<tr class="codeline" data-linenumber="553"><td class="num" id="LN553">553</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="554"><td class="num" id="LN554">554</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="555"><td class="num" id="LN555">555</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="556"><td class="num" id="LN556">556</td><td class="line">PyObject *THPVariable_get_requires_grad(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="557"><td class="num" id="LN557">557</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="558"><td class="num" id="LN558">558</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="559"><td class="num" id="LN559">559</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="560"><td class="num" id="LN560">560</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"requires_grad"</span>);</td></tr>
<tr class="codeline" data-linenumber="561"><td class="num" id="LN561">561</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="562"><td class="num" id="LN562">562</td><td class="line">  <span class='keyword'>if</span>(THPVariable_Unpack(self).requires_grad()) {</td></tr>
<tr class="codeline" data-linenumber="563"><td class="num" id="LN563">563</td><td class="line">    <span class='macro'>Py_RETURN_TRUE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_TrueStruct<br>)))), ((PyObject *) &amp;_Py_TrueStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="564"><td class="num" id="LN564">564</td><td class="line">  } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="565"><td class="num" id="LN565">565</td><td class="line">    <span class='macro'>Py_RETURN_FALSE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_FalseStruct<br>)))), ((PyObject *) &amp;_Py_FalseStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="566"><td class="num" id="LN566">566</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="567"><td class="num" id="LN567">567</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="568"><td class="num" id="LN568">568</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="569"><td class="num" id="LN569">569</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="570"><td class="num" id="LN570">570</td><td class="line">PyObject *THPVariable_retains_grad(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="571"><td class="num" id="LN571">571</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="572"><td class="num" id="LN572">572</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="573"><td class="num" id="LN573">573</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="574"><td class="num" id="LN574">574</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"retains_grad"</span>);</td></tr>
<tr class="codeline" data-linenumber="575"><td class="num" id="LN575">575</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="576"><td class="num" id="LN576">576</td><td class="line">  <span class='keyword'>if</span>(THPVariable_Unpack(self).retains_grad()) {</td></tr>
<tr class="codeline" data-linenumber="577"><td class="num" id="LN577">577</td><td class="line">    <span class='macro'>Py_RETURN_TRUE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_TrueStruct<br>)))), ((PyObject *) &amp;_Py_TrueStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="578"><td class="num" id="LN578">578</td><td class="line">  } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="579"><td class="num" id="LN579">579</td><td class="line">    <span class='macro'>Py_RETURN_FALSE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_FalseStruct<br>)))), ((PyObject *) &amp;_Py_FalseStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="580"><td class="num" id="LN580">580</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="581"><td class="num" id="LN581">581</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="582"><td class="num" id="LN582">582</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="583"><td class="num" id="LN583">583</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="584"><td class="num" id="LN584">584</td><td class="line">PyObject *THPVariable_get_ndim(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="585"><td class="num" id="LN585">585</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="586"><td class="num" id="LN586">586</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="587"><td class="num" id="LN587">587</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="588"><td class="num" id="LN588">588</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"ndim"</span>);</td></tr>
<tr class="codeline" data-linenumber="589"><td class="num" id="LN589">589</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="590"><td class="num" id="LN590">590</td><td class="line">  <span class='keyword'>return</span> <span class='macro'>PyInt_FromLong<span class='macro_popup'>PyLong_FromLong</span></span>(THPVariable_Unpack(self).dim());</td></tr>
<tr class="codeline" data-linenumber="591"><td class="num" id="LN591">591</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="592"><td class="num" id="LN592">592</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="593"><td class="num" id="LN593">593</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="594"><td class="num" id="LN594">594</td><td class="line">PyObject *THPVariable_get_names(PyObject *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="595"><td class="num" id="LN595">595</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="596"><td class="num" id="LN596">596</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="597"><td class="num" id="LN597">597</td><td class="line">  <span class='keyword'>if</span> (<span class="mrange">check_has_torch_function(self)</span>) {</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path1" class="msg msgEvent" style="margin-left:7ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">1</div></td><td>Assuming the condition is false</td><td><div class="PathNav"><a href="#Path2" title="Next event (2)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path2" class="msg msgControl" style="margin-left:3ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexControl">2</div></td><td><div class="PathNav"><a href="#Path1" title="Previous event (1)">&#x2190;</a></div></td><td>Taking false branch</td><td><div class="PathNav"><a href="#Path3" title="Next event (3)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="598"><td class="num" id="LN598">598</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter((THPVariable*)self, <span class='string_literal'>"names"</span>);</td></tr>
<tr class="codeline" data-linenumber="599"><td class="num" id="LN599">599</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="600"><td class="num" id="LN600">600</td><td class="line">  <span class='comment'>// The long-term plan is to return a list of (python) torch.Dimname.</span></td></tr>
<tr class="codeline" data-linenumber="601"><td class="num" id="LN601">601</td><td class="line">  <span class='comment'>// However, for now, return a list of string.</span></td></tr>
<tr class="codeline" data-linenumber="602"><td class="num" id="LN602">602</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; tensor = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="603"><td class="num" id="LN603">603</td><td class="line">  size_t size = tensor.dim();</td></tr>
<tr class="codeline" data-linenumber="604"><td class="num" id="LN604">604</td><td class="line">  THPObjectPtr tuple(<span class="mrange"><span class="mrange">PyTuple_New(size)</span></span>);</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path3" class="msg msgEvent" style="margin-left:22ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">3</div></td><td><div class="PathNav"><a href="#Path2" title="Previous event (2)">&#x2190;</a></div></td><td>Calling 'PyTuple_New'</td><td><div class="PathNav"><a href="#Path4" title="Next event (4)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path5" class="msg msgEvent" style="margin-left:22ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">5</div></td><td><div class="PathNav"><a href="#Path4" title="Previous event (4)">&#x2190;</a></div></td><td>Returning from 'PyTuple_New'</td><td><div class="PathNav"><a href="#Path6" title="Next event (6)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="EndPath" class="msg msgEvent" style="margin-left:22ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">12</div></td><td><div class="PathNav"><a href="#Path11" title="Previous event (11)">&#x2190;</a></div></td><td>PyObject ownership leak with reference count of 1</td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="605"><td class="num" id="LN605">605</td><td class="line">  <span class='keyword'>if</span> (<span class="mrange">!tuple</span>) <span class='keyword'>throw</span> python_error();</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path6" class="msg msgEvent" style="margin-left:7ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">6</div></td><td><div class="PathNav"><a href="#Path5" title="Previous event (5)">&#x2190;</a></div></td><td>Assuming the condition is false</td><td><div class="PathNav"><a href="#Path7" title="Next event (7)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path7" class="msg msgControl" style="margin-left:3ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexControl">7</div></td><td><div class="PathNav"><a href="#Path6" title="Previous event (6)">&#x2190;</a></div></td><td>Taking false branch</td><td><div class="PathNav"><a href="#Path8" title="Next event (8)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="606"><td class="num" id="LN606">606</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="607"><td class="num" id="LN607">607</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span> dimnames = tensor.names();</td></tr>
<tr class="codeline" data-linenumber="608"><td class="num" id="LN608">608</td><td class="line">  <span class='keyword'>for</span> (<span class='keyword'>const</span> <span class='keyword'>auto</span> i : c10::irange(size)) {</td></tr>
<tr class="codeline" data-linenumber="609"><td class="num" id="LN609">609</td><td class="line">    <span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-init-variables)</span></td></tr>
<tr class="codeline" data-linenumber="610"><td class="num" id="LN610">610</td><td class="line">    PyObject* str;</td></tr>
<tr class="codeline" data-linenumber="611"><td class="num" id="LN611">611</td><td class="line">    <span class='keyword'>if</span> (<span class="mrange">dimnames[i].type() == at::NameType::WILDCARD</span>) {</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path8" class="msg msgEvent" style="margin-left:9ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">8</div></td><td><div class="PathNav"><a href="#Path7" title="Previous event (7)">&#x2190;</a></div></td><td>Assuming the condition is false</td><td><div class="PathNav"><a href="#Path9" title="Next event (9)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path9" class="msg msgControl" style="margin-left:5ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexControl">9</div></td><td><div class="PathNav"><a href="#Path8" title="Previous event (8)">&#x2190;</a></div></td><td>Taking false branch</td><td><div class="PathNav"><a href="#Path10" title="Next event (10)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="612"><td class="num" id="LN612">612</td><td class="line">      <span class='comment'>// PyTuple_SET_ITEM steals a reference to the object. When the tuple is</span></td></tr>
<tr class="codeline" data-linenumber="613"><td class="num" id="LN613">613</td><td class="line">      <span class='comment'>// deallocated, it'll decrement the refcount on Py_None, which is bad.</span></td></tr>
<tr class="codeline" data-linenumber="614"><td class="num" id="LN614">614</td><td class="line">      <span class='comment'>// To avoid this, we "create" a new reference to Py_None by increasing</span></td></tr>
<tr class="codeline" data-linenumber="615"><td class="num" id="LN615">615</td><td class="line">      <span class='comment'>// the refcount.</span></td></tr>
<tr class="codeline" data-linenumber="616"><td class="num" id="LN616">616</td><td class="line">      <span class='comment'>// Sources:</span></td></tr>
<tr class="codeline" data-linenumber="617"><td class="num" id="LN617">617</td><td class="line">      <span class='comment'>// - https://docs.python.org/3/c-api/tuple.html#c.PyTuple_SetItem</span></td></tr>
<tr class="codeline" data-linenumber="618"><td class="num" id="LN618">618</td><td class="line">      <span class='comment'>// - https://stackoverflow.com/questions/16400600/how-to-return-a-tuple-containing-a-none-value-from-the-c-api</span></td></tr>
<tr class="codeline" data-linenumber="619"><td class="num" id="LN619">619</td><td class="line">      <span class='macro'>Py_INCREF(Py_None)<span class='macro_popup'>_Py_INCREF(((PyObject*)((&amp;_Py_NoneStruct))))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="620"><td class="num" id="LN620">620</td><td class="line">      str = <span class='macro'>Py_None<span class='macro_popup'>(&amp;_Py_NoneStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="621"><td class="num" id="LN621">621</td><td class="line">    } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="622"><td class="num" id="LN622">622</td><td class="line">      str = THPUtils_packString(dimnames[i].symbol().toUnqualString());</td></tr>
<tr class="codeline" data-linenumber="623"><td class="num" id="LN623">623</td><td class="line">      <span class='keyword'>if</span> (<span class="mrange">!str</span>) <span class='keyword'>throw</span> python_error();</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path10" class="msg msgEvent" style="margin-left:11ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">10</div></td><td><div class="PathNav"><a href="#Path9" title="Previous event (9)">&#x2190;</a></div></td><td>Assuming 'str' is null</td><td><div class="PathNav"><a href="#Path11" title="Next event (11)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path11" class="msg msgControl" style="margin-left:7ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexControl">11</div></td><td><div class="PathNav"><a href="#Path10" title="Previous event (10)">&#x2190;</a></div></td><td>Taking true branch</td><td><div class="PathNav"><a href="#EndPath" title="Next event (12)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="624"><td class="num" id="LN624">624</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="625"><td class="num" id="LN625">625</td><td class="line">    <span class='macro'>PyTuple_SET_ITEM(tuple.get(), i, str)<span class='macro_popup'>PyTuple_SetItem(tuple.get(), i, str)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="626"><td class="num" id="LN626">626</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="627"><td class="num" id="LN627">627</td><td class="line">  <span class='keyword'>return</span> tuple.release();</td></tr>
<tr class="codeline" data-linenumber="628"><td class="num" id="LN628">628</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="629"><td class="num" id="LN629">629</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="630"><td class="num" id="LN630">630</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="631"><td class="num" id="LN631">631</td><td class="line"><span class='keyword'>int</span> THPVariable_set_names(PyObject *self, PyObject *names, <span class='keyword'>void</span> *unused) {</td></tr>
<tr class="codeline" data-linenumber="632"><td class="num" id="LN632">632</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="633"><td class="num" id="LN633">633</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function(self)) {</td></tr>
<tr class="codeline" data-linenumber="634"><td class="num" id="LN634">634</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_setter((THPVariable*)self, <span class='string_literal'>"names"</span>, names);</td></tr>
<tr class="codeline" data-linenumber="635"><td class="num" id="LN635">635</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="636"><td class="num" id="LN636">636</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="637"><td class="num" id="LN637">637</td><td class="line">  <span class='keyword'>if</span> (names == <span class='macro'>Py_None<span class='macro_popup'>(&amp;_Py_NoneStruct)</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="638"><td class="num" id="LN638">638</td><td class="line">    at::internal_set_names_inplace(var, at::nullopt);</td></tr>
<tr class="codeline" data-linenumber="639"><td class="num" id="LN639">639</td><td class="line">  } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="640"><td class="num" id="LN640">640</td><td class="line">    <span class='macro'>THPUtils_assertRet(-1,<span class='macro_popup'>if ((__builtin_expect((!(THPUtils_checkDimnameList(names))), (<br>0)))) { THPUtils_setError("names must either be None or a tuple of dim names"<br>); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="641"><td class="num" id="LN641">641</td><td class="line">        <span class='macro'>THPUtils_checkDimnameList(names),<span class='macro_popup'>if ((__builtin_expect((!(THPUtils_checkDimnameList(names))), (<br>0)))) { THPUtils_setError("names must either be None or a tuple of dim names"<br>); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="642"><td class="num" id="LN642">642</td><td class="line">        <span class='string_literal'><span class='macro'>"names must either be None or a tuple of dim names"</span>)<span class='macro_popup'>if ((__builtin_expect((!(THPUtils_checkDimnameList(names))), (<br>0)))) { THPUtils_setError("names must either be None or a tuple of dim names"<br>); return -1; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="643"><td class="num" id="LN643">643</td><td class="line">    at::internal_set_names_inplace(var, torch::parseDimnameList(names));</td></tr>
<tr class="codeline" data-linenumber="644"><td class="num" id="LN644">644</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="645"><td class="num" id="LN645">645</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="646"><td class="num" id="LN646">646</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS_RET(-1)<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return -1; }<br> catch (const c10::IndexError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_IndexError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::ValueError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return -1; } catch (torch::PyTorchError<br> &amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(e.python_type(), msg); return -1; } catch (const std::exception<br>&amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(PyExc_RuntimeError, msg); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="647"><td class="num" id="LN647">647</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="648"><td class="num" id="LN648">648</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="649"><td class="num" id="LN649">649</td><td class="line"><span class='keyword'>int</span> THPVariable_set_requires_grad(THPVariable *self, PyObject *obj, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="650"><td class="num" id="LN650">650</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="651"><td class="num" id="LN651">651</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="652"><td class="num" id="LN652">652</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="653"><td class="num" id="LN653">653</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_setter(self, <span class='string_literal'>"requires_grad"</span>, obj);</td></tr>
<tr class="codeline" data-linenumber="654"><td class="num" id="LN654">654</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="655"><td class="num" id="LN655">655</td><td class="line">  <span class='macro'>THPUtils_assertRet(-1, obj &amp;&amp; PyBool_Check(obj), <span class='string_literal'>"requires_grad must be a bool"</span>)<span class='macro_popup'>if ((__builtin_expect((!(obj &amp;&amp; ((((PyObject*)(obj))-&gt;<br>ob_type) == &amp;PyBool_Type))), (0)))) { THPUtils_setError("requires_grad must be a bool"<br>); return -1; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="656"><td class="num" id="LN656">656</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="657"><td class="num" id="LN657">657</td><td class="line">  <span class='keyword'>auto</span> requires_grad = (obj == <span class='macro'>Py_True<span class='macro_popup'>((PyObject *) &amp;_Py_TrueStruct)</span></span>);</td></tr>
<tr class="codeline" data-linenumber="658"><td class="num" id="LN658">658</td><td class="line">  <span class='keyword'>if</span> (!var.is_leaf()) {</td></tr>
<tr class="codeline" data-linenumber="659"><td class="num" id="LN659">659</td><td class="line">    THPUtils_setError(autograd::utils::requires_grad_leaf_error(obj == <span class='macro'>Py_True<span class='macro_popup'>((PyObject *) &amp;_Py_TrueStruct)</span></span>).c_str());</td></tr>
<tr class="codeline" data-linenumber="660"><td class="num" id="LN660">660</td><td class="line">    <span class='keyword'>return</span> -1;</td></tr>
<tr class="codeline" data-linenumber="661"><td class="num" id="LN661">661</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="662"><td class="num" id="LN662">662</td><td class="line">  <span class='keyword'>if</span> (requires_grad &amp;&amp; !isDifferentiableType(at::typeMetaToScalarType((var.dtype())))) {</td></tr>
<tr class="codeline" data-linenumber="663"><td class="num" id="LN663">663</td><td class="line">    THPUtils_setError(<span class='string_literal'>"only Tensors of floating point and complex dtype can require gradients"</span>);</td></tr>
<tr class="codeline" data-linenumber="664"><td class="num" id="LN664">664</td><td class="line">    <span class='keyword'>return</span> -1;</td></tr>
<tr class="codeline" data-linenumber="665"><td class="num" id="LN665">665</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="666"><td class="num" id="LN666">666</td><td class="line">  var.set_requires_grad(requires_grad);</td></tr>
<tr class="codeline" data-linenumber="667"><td class="num" id="LN667">667</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="668"><td class="num" id="LN668">668</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS_RET(-1)<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return -1; }<br> catch (const c10::IndexError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_IndexError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::ValueError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return -1; } catch (torch::PyTorchError<br> &amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(e.python_type(), msg); return -1; } catch (const std::exception<br>&amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(PyExc_RuntimeError, msg); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="669"><td class="num" id="LN669">669</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="670"><td class="num" id="LN670">670</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="671"><td class="num" id="LN671">671</td><td class="line">PyObject *THPVariable_get_name(THPVariable* self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="672"><td class="num" id="LN672">672</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="673"><td class="num" id="LN673">673</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="674"><td class="num" id="LN674">674</td><td class="line">    <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="675"><td class="num" id="LN675">675</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"name"</span>);</td></tr>
<tr class="codeline" data-linenumber="676"><td class="num" id="LN676">676</td><td class="line">    <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="677"><td class="num" id="LN677">677</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="678"><td class="num" id="LN678">678</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; tensor = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="679"><td class="num" id="LN679">679</td><td class="line">  <span class='keyword'>if</span> (tensor.name() == <span class='string_literal'>""</span>)</td></tr>
<tr class="codeline" data-linenumber="680"><td class="num" id="LN680">680</td><td class="line">    <span class='macro'>Py_RETURN_NONE<span class='macro_popup'>return _Py_INCREF(((PyObject*)((&amp;_Py_NoneStruct)))), (&amp;<br>_Py_NoneStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="681"><td class="num" id="LN681">681</td><td class="line">  <span class='keyword'>return</span> THPUtils_packString(tensor.name().c_str());</td></tr>
<tr class="codeline" data-linenumber="682"><td class="num" id="LN682">682</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="683"><td class="num" id="LN683">683</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="684"><td class="num" id="LN684">684</td><td class="line">PyObject *THPVariable_get_backwards_hooks(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="685"><td class="num" id="LN685">685</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="686"><td class="num" id="LN686">686</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="687"><td class="num" id="LN687">687</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="688"><td class="num" id="LN688">688</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"_backward_hooks"</span>);</td></tr>
<tr class="codeline" data-linenumber="689"><td class="num" id="LN689">689</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="690"><td class="num" id="LN690">690</td><td class="line">  <span class='keyword'>if</span> (self-&gt;backward_hooks) {</td></tr>
<tr class="codeline" data-linenumber="691"><td class="num" id="LN691">691</td><td class="line">    <span class='macro'>Py_INCREF(self-&gt;backward_hooks)<span class='macro_popup'>_Py_INCREF(((PyObject*)(self-&gt;backward_hooks)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="692"><td class="num" id="LN692">692</td><td class="line">    <span class='keyword'>return</span> self-&gt;backward_hooks;</td></tr>
<tr class="codeline" data-linenumber="693"><td class="num" id="LN693">693</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="694"><td class="num" id="LN694">694</td><td class="line">  <span class='macro'>Py_RETURN_NONE<span class='macro_popup'>return _Py_INCREF(((PyObject*)((&amp;_Py_NoneStruct)))), (&amp;<br>_Py_NoneStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="695"><td class="num" id="LN695">695</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="696"><td class="num" id="LN696">696</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="697"><td class="num" id="LN697">697</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="698"><td class="num" id="LN698">698</td><td class="line"><span class='keyword'>int</span> THPVariable_set_backwards_hooks(THPVariable *self, PyObject *obj, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="699"><td class="num" id="LN699">699</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="700"><td class="num" id="LN700">700</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="701"><td class="num" id="LN701">701</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="702"><td class="num" id="LN702">702</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_setter(self, <span class='string_literal'>"_backward_hooks"</span>, obj);</td></tr>
<tr class="codeline" data-linenumber="703"><td class="num" id="LN703">703</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="704"><td class="num" id="LN704">704</td><td class="line">  <span class='macro'>THPUtils_assertRet(-1, obj, <span class='string_literal'>"Deletion of _backwards_hooks not allowed!"</span>)<span class='macro_popup'>if ((__builtin_expect((!(obj)), (0)))) { THPUtils_setError("Deletion of _backwards_hooks not allowed!"<br>); return -1; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="705"><td class="num" id="LN705">705</td><td class="line">  <span class='keyword'>if</span> (obj == <span class='macro'>Py_None<span class='macro_popup'>(&amp;_Py_NoneStruct)</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="706"><td class="num" id="LN706">706</td><td class="line">    obj = <span class='keyword'>nullptr</span>;</td></tr>
<tr class="codeline" data-linenumber="707"><td class="num" id="LN707">707</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="708"><td class="num" id="LN708">708</td><td class="line">  <span class='macro'>Py_XINCREF(obj)<span class='macro_popup'>_Py_XINCREF(((PyObject*)(obj)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="709"><td class="num" id="LN709">709</td><td class="line">  <span class='macro'>Py_XDECREF(self-&gt;backward_hooks)<span class='macro_popup'>_Py_XDECREF(((PyObject*)(self-&gt;backward_hooks)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="710"><td class="num" id="LN710">710</td><td class="line">  self-&gt;backward_hooks = obj;</td></tr>
<tr class="codeline" data-linenumber="711"><td class="num" id="LN711">711</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; tensor = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="712"><td class="num" id="LN712">712</td><td class="line">  torch::autograd::impl::clear_hooks(tensor);</td></tr>
<tr class="codeline" data-linenumber="713"><td class="num" id="LN713">713</td><td class="line">  <span class='keyword'>if</span> (obj) {</td></tr>
<tr class="codeline" data-linenumber="714"><td class="num" id="LN714">714</td><td class="line">    torch::autograd::impl::add_hook(tensor, std::make_shared&lt;PyFunctionPreHook&gt;(obj, 0));</td></tr>
<tr class="codeline" data-linenumber="715"><td class="num" id="LN715">715</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="716"><td class="num" id="LN716">716</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="717"><td class="num" id="LN717">717</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS_RET(-1)<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return -1; }<br> catch (const c10::IndexError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_IndexError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::ValueError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return -1; } catch (torch::PyTorchError<br> &amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(e.python_type(), msg); return -1; } catch (const std::exception<br>&amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(PyExc_RuntimeError, msg); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="718"><td class="num" id="LN718">718</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="719"><td class="num" id="LN719">719</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="720"><td class="num" id="LN720">720</td><td class="line">PyObject *THPVariable_get_base(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="721"><td class="num" id="LN721">721</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="722"><td class="num" id="LN722">722</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="723"><td class="num" id="LN723">723</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="724"><td class="num" id="LN724">724</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"_base"</span>);</td></tr>
<tr class="codeline" data-linenumber="725"><td class="num" id="LN725">725</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="726"><td class="num" id="LN726">726</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; tensor = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="727"><td class="num" id="LN727">727</td><td class="line">  <span class='keyword'>if</span> (tensor.is_view()) {</td></tr>
<tr class="codeline" data-linenumber="728"><td class="num" id="LN728">728</td><td class="line">    <span class='keyword'>return</span> THPVariable_Wrap(tensor._base());</td></tr>
<tr class="codeline" data-linenumber="729"><td class="num" id="LN729">729</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="730"><td class="num" id="LN730">730</td><td class="line">  <span class='macro'>Py_RETURN_NONE<span class='macro_popup'>return _Py_INCREF(((PyObject*)((&amp;_Py_NoneStruct)))), (&amp;<br>_Py_NoneStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="731"><td class="num" id="LN731">731</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="732"><td class="num" id="LN732">732</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="733"><td class="num" id="LN733">733</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="734"><td class="num" id="LN734">734</td><td class="line"><span class='directive'>#ifndef USE_DEPLOY</span></td></tr>
<tr class="codeline" data-linenumber="735"><td class="num" id="LN735">735</td><td class="line"><span class='comment'>// This code is only used for asserts, so it is OK to skip it entirely from</span></td></tr>
<tr class="codeline" data-linenumber="736"><td class="num" id="LN736">736</td><td class="line"><span class='comment'>// deploy interpreters (in which case we will just skip the safety check).  For</span></td></tr>
<tr class="codeline" data-linenumber="737"><td class="num" id="LN737">737</td><td class="line"><span class='comment'>// a more precise check, it would be necessary to test that we are not holding</span></td></tr>
<tr class="codeline" data-linenumber="738"><td class="num" id="LN738">738</td><td class="line"><span class='comment'>// the GIL for *all* active torch deploy interpreters.  There is not really any</span></td></tr>
<tr class="codeline" data-linenumber="739"><td class="num" id="LN739">739</td><td class="line"><span class='comment'>// reason to do this.</span></td></tr>
<tr class="codeline" data-linenumber="740"><td class="num" id="LN740">740</td><td class="line"><span class='keyword'>struct</span> ConcretePythonGILHooks : <span class='keyword'>public</span> c10::impl::PythonGILHooks {</td></tr>
<tr class="codeline" data-linenumber="741"><td class="num" id="LN741">741</td><td class="line">  <span class='keyword'>bool</span> check_python_gil() <span class='keyword'>const</span> override {</td></tr>
<tr class="codeline" data-linenumber="742"><td class="num" id="LN742">742</td><td class="line">    <span class='keyword'>return</span> Py_IsInitialized() &amp;&amp; PyGILState_Check();</td></tr>
<tr class="codeline" data-linenumber="743"><td class="num" id="LN743">743</td><td class="line">  };</td></tr>
<tr class="codeline" data-linenumber="744"><td class="num" id="LN744">744</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="745"><td class="num" id="LN745">745</td><td class="line"><span class='comment'>// During process destruction, python_gil_hooks will get destructed, making</span></td></tr>
<tr class="codeline" data-linenumber="746"><td class="num" id="LN746">746</td><td class="line"><span class='comment'>// further virtual calls on the object invalid.  By the ordering of declarations</span></td></tr>
<tr class="codeline" data-linenumber="747"><td class="num" id="LN747">747</td><td class="line"><span class='comment'>// in this file, the registerer will get destructed first, removing the</span></td></tr>
<tr class="codeline" data-linenumber="748"><td class="num" id="LN748">748</td><td class="line"><span class='comment'>// externally visible reference to the object.  Assuming at this point in time,</span></td></tr>
<tr class="codeline" data-linenumber="749"><td class="num" id="LN749">749</td><td class="line"><span class='comment'>// there aren't other threads racing to read out the hooks, subsequent calls</span></td></tr>
<tr class="codeline" data-linenumber="750"><td class="num" id="LN750">750</td><td class="line"><span class='comment'>// into GIL hooks will hit a nullptr and gracefully no-op the asserts (as</span></td></tr>
<tr class="codeline" data-linenumber="751"><td class="num" id="LN751">751</td><td class="line"><span class='comment'>// desired, since at process shutdown time the Python interpreter is definitely</span></td></tr>
<tr class="codeline" data-linenumber="752"><td class="num" id="LN752">752</td><td class="line"><span class='comment'>// dead).</span></td></tr>
<tr class="codeline" data-linenumber="753"><td class="num" id="LN753">753</td><td class="line"><span class='comment'>//</span></td></tr>
<tr class="codeline" data-linenumber="754"><td class="num" id="LN754">754</td><td class="line"><span class='comment'>// An alternative way to reduce the risk of python_gil_hooks going prematurely</span></td></tr>
<tr class="codeline" data-linenumber="755"><td class="num" id="LN755">755</td><td class="line"><span class='comment'>// dead would be to leak it at destruction time.  I didn't do that because</span></td></tr>
<tr class="codeline" data-linenumber="756"><td class="num" id="LN756">756</td><td class="line"><span class='comment'>// it's annoying to write the Registerer class for this case.</span></td></tr>
<tr class="codeline" data-linenumber="757"><td class="num" id="LN757">757</td><td class="line">ConcretePythonGILHooks python_gil_hooks;</td></tr>
<tr class="codeline" data-linenumber="758"><td class="num" id="LN758">758</td><td class="line"><span class='keyword'>static</span> c10::impl::PythonGILHooksRegisterer python_gil_hooks_registerer(&amp;python_gil_hooks);</td></tr>
<tr class="codeline" data-linenumber="759"><td class="num" id="LN759">759</td><td class="line"><span class='directive'>#endif</span></td></tr>
<tr class="codeline" data-linenumber="760"><td class="num" id="LN760">760</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="761"><td class="num" id="LN761">761</td><td class="line">PyObject *THPVariable_get_shape(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="762"><td class="num" id="LN762">762</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="763"><td class="num" id="LN763">763</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="764"><td class="num" id="LN764">764</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="765"><td class="num" id="LN765">765</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"shape"</span>);</td></tr>
<tr class="codeline" data-linenumber="766"><td class="num" id="LN766">766</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="767"><td class="num" id="LN767">767</td><td class="line">  <span class='keyword'>return</span> THPSize_New(THPVariable_Unpack(self));</td></tr>
<tr class="codeline" data-linenumber="768"><td class="num" id="LN768">768</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="769"><td class="num" id="LN769">769</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="770"><td class="num" id="LN770">770</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="771"><td class="num" id="LN771">771</td><td class="line">PyObject *THPVariable_is_cuda(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="772"><td class="num" id="LN772">772</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="773"><td class="num" id="LN773">773</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="774"><td class="num" id="LN774">774</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="775"><td class="num" id="LN775">775</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_cuda"</span>);</td></tr>
<tr class="codeline" data-linenumber="776"><td class="num" id="LN776">776</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="777"><td class="num" id="LN777">777</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="778"><td class="num" id="LN778">778</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self_.is_cuda());</td></tr>
<tr class="codeline" data-linenumber="779"><td class="num" id="LN779">779</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="780"><td class="num" id="LN780">780</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="781"><td class="num" id="LN781">781</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="782"><td class="num" id="LN782">782</td><td class="line">PyObject* THPVariable_is_xpu(THPVariable* self, <span class='keyword'>void</span>* unused) {</td></tr>
<tr class="codeline" data-linenumber="783"><td class="num" id="LN783">783</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="784"><td class="num" id="LN784">784</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject*)self)) {</td></tr>
<tr class="codeline" data-linenumber="785"><td class="num" id="LN785">785</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_xpu"</span>);</td></tr>
<tr class="codeline" data-linenumber="786"><td class="num" id="LN786">786</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="787"><td class="num" id="LN787">787</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="788"><td class="num" id="LN788">788</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self_.is_xpu());</td></tr>
<tr class="codeline" data-linenumber="789"><td class="num" id="LN789">789</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="790"><td class="num" id="LN790">790</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="791"><td class="num" id="LN791">791</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="792"><td class="num" id="LN792">792</td><td class="line">PyObject *THPVariable_is_sparse(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="793"><td class="num" id="LN793">793</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="794"><td class="num" id="LN794">794</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="795"><td class="num" id="LN795">795</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="796"><td class="num" id="LN796">796</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_sparse"</span>);</td></tr>
<tr class="codeline" data-linenumber="797"><td class="num" id="LN797">797</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="798"><td class="num" id="LN798">798</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="799"><td class="num" id="LN799">799</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self_.is_sparse());</td></tr>
<tr class="codeline" data-linenumber="800"><td class="num" id="LN800">800</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="801"><td class="num" id="LN801">801</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="802"><td class="num" id="LN802">802</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="803"><td class="num" id="LN803">803</td><td class="line">PyObject *THPVariable_is_sparse_csr(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="804"><td class="num" id="LN804">804</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="805"><td class="num" id="LN805">805</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="806"><td class="num" id="LN806">806</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="807"><td class="num" id="LN807">807</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_sparse_csr"</span>);</td></tr>
<tr class="codeline" data-linenumber="808"><td class="num" id="LN808">808</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="809"><td class="num" id="LN809">809</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="810"><td class="num" id="LN810">810</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self_.is_sparse_csr());</td></tr>
<tr class="codeline" data-linenumber="811"><td class="num" id="LN811">811</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="812"><td class="num" id="LN812">812</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="813"><td class="num" id="LN813">813</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="814"><td class="num" id="LN814">814</td><td class="line">PyObject *THPVariable_is_mkldnn(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="815"><td class="num" id="LN815">815</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="816"><td class="num" id="LN816">816</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="817"><td class="num" id="LN817">817</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="818"><td class="num" id="LN818">818</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_mkldnn"</span>);</td></tr>
<tr class="codeline" data-linenumber="819"><td class="num" id="LN819">819</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="820"><td class="num" id="LN820">820</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="821"><td class="num" id="LN821">821</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self_.is_mkldnn());</td></tr>
<tr class="codeline" data-linenumber="822"><td class="num" id="LN822">822</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="823"><td class="num" id="LN823">823</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="824"><td class="num" id="LN824">824</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="825"><td class="num" id="LN825">825</td><td class="line">PyObject *THPVariable_is_mlc(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="826"><td class="num" id="LN826">826</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="827"><td class="num" id="LN827">827</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="828"><td class="num" id="LN828">828</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="829"><td class="num" id="LN829">829</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_mlc"</span>);</td></tr>
<tr class="codeline" data-linenumber="830"><td class="num" id="LN830">830</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="831"><td class="num" id="LN831">831</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="832"><td class="num" id="LN832">832</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self_.is_mlc());</td></tr>
<tr class="codeline" data-linenumber="833"><td class="num" id="LN833">833</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="834"><td class="num" id="LN834">834</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="835"><td class="num" id="LN835">835</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="836"><td class="num" id="LN836">836</td><td class="line">PyObject *THPVariable_is_vulkan(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="837"><td class="num" id="LN837">837</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="838"><td class="num" id="LN838">838</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="839"><td class="num" id="LN839">839</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="840"><td class="num" id="LN840">840</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_vulkan"</span>);</td></tr>
<tr class="codeline" data-linenumber="841"><td class="num" id="LN841">841</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="842"><td class="num" id="LN842">842</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="843"><td class="num" id="LN843">843</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self_.is_vulkan());</td></tr>
<tr class="codeline" data-linenumber="844"><td class="num" id="LN844">844</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="845"><td class="num" id="LN845">845</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="846"><td class="num" id="LN846">846</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="847"><td class="num" id="LN847">847</td><td class="line">PyObject *THPVariable_is_quantized(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="848"><td class="num" id="LN848">848</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="849"><td class="num" id="LN849">849</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="850"><td class="num" id="LN850">850</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="851"><td class="num" id="LN851">851</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_quantized"</span>);</td></tr>
<tr class="codeline" data-linenumber="852"><td class="num" id="LN852">852</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="853"><td class="num" id="LN853">853</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="854"><td class="num" id="LN854">854</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self_.is_quantized());</td></tr>
<tr class="codeline" data-linenumber="855"><td class="num" id="LN855">855</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="856"><td class="num" id="LN856">856</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="857"><td class="num" id="LN857">857</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="858"><td class="num" id="LN858">858</td><td class="line">PyObject *THPVariable_is_meta(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="859"><td class="num" id="LN859">859</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="860"><td class="num" id="LN860">860</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="861"><td class="num" id="LN861">861</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="862"><td class="num" id="LN862">862</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_meta"</span>);</td></tr>
<tr class="codeline" data-linenumber="863"><td class="num" id="LN863">863</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="864"><td class="num" id="LN864">864</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="865"><td class="num" id="LN865">865</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self_.is_meta());</td></tr>
<tr class="codeline" data-linenumber="866"><td class="num" id="LN866">866</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="867"><td class="num" id="LN867">867</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="868"><td class="num" id="LN868">868</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="869"><td class="num" id="LN869">869</td><td class="line">PyObject *THPVariable_is_complex(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="870"><td class="num" id="LN870">870</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="871"><td class="num" id="LN871">871</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="872"><td class="num" id="LN872">872</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="873"><td class="num" id="LN873">873</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"is_complex"</span>);</td></tr>
<tr class="codeline" data-linenumber="874"><td class="num" id="LN874">874</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="875"><td class="num" id="LN875">875</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="876"><td class="num" id="LN876">876</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self_.is_complex());</td></tr>
<tr class="codeline" data-linenumber="877"><td class="num" id="LN877">877</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="878"><td class="num" id="LN878">878</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="879"><td class="num" id="LN879">879</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="880"><td class="num" id="LN880">880</td><td class="line"><span class='keyword'>static</span> PyObject *THPVariable_dtype(THPVariable *self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="881"><td class="num" id="LN881">881</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="882"><td class="num" id="LN882">882</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="883"><td class="num" id="LN883">883</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="884"><td class="num" id="LN884">884</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"dtype"</span>);</td></tr>
<tr class="codeline" data-linenumber="885"><td class="num" id="LN885">885</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="886"><td class="num" id="LN886">886</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="887"><td class="num" id="LN887">887</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(torch::getTHPDtype(self_.scalar_type()));</td></tr>
<tr class="codeline" data-linenumber="888"><td class="num" id="LN888">888</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="889"><td class="num" id="LN889">889</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="890"><td class="num" id="LN890">890</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="891"><td class="num" id="LN891">891</td><td class="line"><span class='keyword'>static</span> PyObject * THPVariable_layout(THPVariable* self, <span class='keyword'>void</span> *unused) {</td></tr>
<tr class="codeline" data-linenumber="892"><td class="num" id="LN892">892</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="893"><td class="num" id="LN893">893</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="894"><td class="num" id="LN894">894</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"layout"</span>);</td></tr>
<tr class="codeline" data-linenumber="895"><td class="num" id="LN895">895</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="896"><td class="num" id="LN896">896</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="897"><td class="num" id="LN897">897</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(torch::getTHPLayout(self_.layout()));</td></tr>
<tr class="codeline" data-linenumber="898"><td class="num" id="LN898">898</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="899"><td class="num" id="LN899">899</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="900"><td class="num" id="LN900">900</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="901"><td class="num" id="LN901">901</td><td class="line"><span class='keyword'>static</span> PyObject * THPVariable_device(THPVariable* self, <span class='keyword'>void</span> *unused) {</td></tr>
<tr class="codeline" data-linenumber="902"><td class="num" id="LN902">902</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="903"><td class="num" id="LN903">903</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="904"><td class="num" id="LN904">904</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"device"</span>);</td></tr>
<tr class="codeline" data-linenumber="905"><td class="num" id="LN905">905</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="906"><td class="num" id="LN906">906</td><td class="line">  <span class='keyword'>return</span> THPDevice_New(THPVariable_Unpack(self).device());</td></tr>
<tr class="codeline" data-linenumber="907"><td class="num" id="LN907">907</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="908"><td class="num" id="LN908">908</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="909"><td class="num" id="LN909">909</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="910"><td class="num" id="LN910">910</td><td class="line">PyObject *THPVariable_get_real(THPVariable* self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="911"><td class="num" id="LN911">911</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="912"><td class="num" id="LN912">912</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="913"><td class="num" id="LN913">913</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="914"><td class="num" id="LN914">914</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"real"</span>);</td></tr>
<tr class="codeline" data-linenumber="915"><td class="num" id="LN915">915</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="916"><td class="num" id="LN916">916</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="917"><td class="num" id="LN917">917</td><td class="line">  <span class='keyword'>auto</span> real = at::real(self_);</td></tr>
<tr class="codeline" data-linenumber="918"><td class="num" id="LN918">918</td><td class="line">  <span class='keyword'>return</span> THPVariable_Wrap(real);</td></tr>
<tr class="codeline" data-linenumber="919"><td class="num" id="LN919">919</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="920"><td class="num" id="LN920">920</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="921"><td class="num" id="LN921">921</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="922"><td class="num" id="LN922">922</td><td class="line">PyObject *THPVariable_get_imag(THPVariable* self, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="923"><td class="num" id="LN923">923</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="924"><td class="num" id="LN924">924</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="925"><td class="num" id="LN925">925</td><td class="line">  <span class='keyword'>if</span> (check_has_torch_function((PyObject *)self)) {</td></tr>
<tr class="codeline" data-linenumber="926"><td class="num" id="LN926">926</td><td class="line">    <span class='keyword'>return</span> handle_torch_function_getter(self, <span class='string_literal'>"imag"</span>);</td></tr>
<tr class="codeline" data-linenumber="927"><td class="num" id="LN927">927</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="928"><td class="num" id="LN928">928</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="929"><td class="num" id="LN929">929</td><td class="line">  <span class='keyword'>auto</span> imag = at::imag(self_);</td></tr>
<tr class="codeline" data-linenumber="930"><td class="num" id="LN930">930</td><td class="line">  <span class='keyword'>return</span> THPVariable_Wrap(imag);</td></tr>
<tr class="codeline" data-linenumber="931"><td class="num" id="LN931">931</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="932"><td class="num" id="LN932">932</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="933"><td class="num" id="LN933">933</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="934"><td class="num" id="LN934">934</td><td class="line"><span class='keyword'>int</span> THPVariable_set_real(THPVariable *self, THPVariable *real, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="935"><td class="num" id="LN935">935</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="936"><td class="num" id="LN936">936</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="937"><td class="num" id="LN937">937</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="938"><td class="num" id="LN938">938</td><td class="line">  <span class='keyword'>auto</span> self_real = at::real(self_);</td></tr>
<tr class="codeline" data-linenumber="939"><td class="num" id="LN939">939</td><td class="line">  self_real.copy_(THPVariable_Unpack(real));</td></tr>
<tr class="codeline" data-linenumber="940"><td class="num" id="LN940">940</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="941"><td class="num" id="LN941">941</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS_RET(-1)<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return -1; }<br> catch (const c10::IndexError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_IndexError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::ValueError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return -1; } catch (torch::PyTorchError<br> &amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(e.python_type(), msg); return -1; } catch (const std::exception<br>&amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(PyExc_RuntimeError, msg); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="942"><td class="num" id="LN942">942</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="943"><td class="num" id="LN943">943</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="944"><td class="num" id="LN944">944</td><td class="line"><span class='keyword'>int</span> THPVariable_set_imag(THPVariable* self, THPVariable *imag, <span class='keyword'>void</span> *unused)</td></tr>
<tr class="codeline" data-linenumber="945"><td class="num" id="LN945">945</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="946"><td class="num" id="LN946">946</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="947"><td class="num" id="LN947">947</td><td class="line">  <span class='keyword'>auto</span>&amp; self_ = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="948"><td class="num" id="LN948">948</td><td class="line">  <span class='keyword'>auto</span> self_imag = at::imag(self_);</td></tr>
<tr class="codeline" data-linenumber="949"><td class="num" id="LN949">949</td><td class="line">  self_imag.copy_(THPVariable_Unpack(imag));</td></tr>
<tr class="codeline" data-linenumber="950"><td class="num" id="LN950">950</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="951"><td class="num" id="LN951">951</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS_RET(-1)<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return -1; }<br> catch (const c10::IndexError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_IndexError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::ValueError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return -1; } catch (const c10<br>::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return -1; } catch (torch::PyTorchError<br> &amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(e.python_type(), msg); return -1; } catch (const std::exception<br>&amp; e) { auto msg = torch::processErrorMsg(e.what()); PyErr_SetString<br>(PyExc_RuntimeError, msg); return -1; }</span></span></td></tr>
<tr class="codeline" data-linenumber="952"><td class="num" id="LN952">952</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="953"><td class="num" id="LN953">953</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="954"><td class="num" id="LN954">954</td><td class="line"><span class='comment'>// properties are registered here because we are currently only able to bind them</span></td></tr>
<tr class="codeline" data-linenumber="955"><td class="num" id="LN955">955</td><td class="line"><span class='comment'>// manually. TODO: make declarable in native_functions</span></td></tr>
<tr class="codeline" data-linenumber="956"><td class="num" id="LN956">956</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="957"><td class="num" id="LN957">957</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>struct</span> PyGetSetDef THPVariable_properties[] = {</td></tr>
<tr class="codeline" data-linenumber="958"><td class="num" id="LN958">958</td><td class="line">  {<span class='string_literal'>"_python_dispatch"</span>, (getter)THPVariable_get_python_dispatch, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="959"><td class="num" id="LN959">959</td><td class="line">  {<span class='string_literal'>"T"</span>, (getter)THPVariable_get_T, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="960"><td class="num" id="LN960">960</td><td class="line">  {<span class='string_literal'>"_cdata"</span>, (getter)THPVariable_get_cdata, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="961"><td class="num" id="LN961">961</td><td class="line">  {<span class='string_literal'>"_version"</span>, (getter)THPVariable_get_version, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="962"><td class="num" id="LN962">962</td><td class="line">  {<span class='string_literal'>"grad_fn"</span>, (getter)THPVariable_get_grad_fn, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="963"><td class="num" id="LN963">963</td><td class="line">  {<span class='string_literal'>"_grad_fn"</span>, (getter)THPVariable_get_grad_fn, (setter)THPVariable_set_grad_fn, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="964"><td class="num" id="LN964">964</td><td class="line">  {<span class='string_literal'>"is_leaf"</span>, (getter)THPVariable_is_leaf, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="965"><td class="num" id="LN965">965</td><td class="line">  {<span class='string_literal'>"retains_grad"</span>, (getter)THPVariable_retains_grad, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="966"><td class="num" id="LN966">966</td><td class="line">  {<span class='string_literal'>"data"</span>, (getter)THPVariable_get_data, (setter)THPVariable_set_data, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="967"><td class="num" id="LN967">967</td><td class="line">  {<span class='string_literal'>"_grad"</span>, (getter)THPVariable_get_grad, (setter)THPVariable_set_grad, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>}, <span class='comment'>// Allows the python class to override .grad</span></td></tr>
<tr class="codeline" data-linenumber="968"><td class="num" id="LN968">968</td><td class="line">  {<span class='string_literal'>"grad"</span>, (getter)THPVariable_get_grad, (setter)THPVariable_set_grad, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="969"><td class="num" id="LN969">969</td><td class="line">  {<span class='string_literal'>"_base"</span>, (getter)THPVariable_get_base, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="970"><td class="num" id="LN970">970</td><td class="line">  {<span class='string_literal'>"volatile"</span>, (getter)THPVariable_get_volatile, (setter)THPVariable_set_volatile, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="971"><td class="num" id="LN971">971</td><td class="line">  {<span class='string_literal'>"output_nr"</span>, (getter)THPVariable_get_output_nr, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="972"><td class="num" id="LN972">972</td><td class="line">  {<span class='string_literal'>"requires_grad"</span>, (getter)THPVariable_get_requires_grad, (setter)THPVariable_set_requires_grad, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="973"><td class="num" id="LN973">973</td><td class="line">  {<span class='string_literal'>"_backward_hooks"</span>, (getter)THPVariable_get_backwards_hooks, (setter)THPVariable_set_backwards_hooks, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="974"><td class="num" id="LN974">974</td><td class="line">  {<span class='string_literal'>"name"</span>, (getter)THPVariable_get_name, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="975"><td class="num" id="LN975">975</td><td class="line">  {<span class='string_literal'>"shape"</span>, (getter)THPVariable_get_shape, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="976"><td class="num" id="LN976">976</td><td class="line">  {<span class='string_literal'>"is_cuda"</span>, (getter)THPVariable_is_cuda, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="977"><td class="num" id="LN977">977</td><td class="line">  {<span class='string_literal'>"is_xpu"</span>, (getter)THPVariable_is_xpu, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="978"><td class="num" id="LN978">978</td><td class="line">  {<span class='string_literal'>"is_sparse"</span>, (getter)THPVariable_is_sparse, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="979"><td class="num" id="LN979">979</td><td class="line">  {<span class='string_literal'>"is_sparse_csr"</span>, (getter)THPVariable_is_sparse_csr, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="980"><td class="num" id="LN980">980</td><td class="line">  {<span class='string_literal'>"is_mkldnn"</span>, (getter)THPVariable_is_mkldnn, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="981"><td class="num" id="LN981">981</td><td class="line">  {<span class='string_literal'>"is_mlc"</span>, (getter)THPVariable_is_mlc, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="982"><td class="num" id="LN982">982</td><td class="line">  {<span class='string_literal'>"is_vulkan"</span>, (getter)THPVariable_is_vulkan, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="983"><td class="num" id="LN983">983</td><td class="line">  {<span class='string_literal'>"is_complex"</span>, (getter)THPVariable_is_complex, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="984"><td class="num" id="LN984">984</td><td class="line">  {<span class='string_literal'>"is_quantized"</span>, (getter)THPVariable_is_quantized, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="985"><td class="num" id="LN985">985</td><td class="line">  {<span class='string_literal'>"is_meta"</span>, (getter)THPVariable_is_meta, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="986"><td class="num" id="LN986">986</td><td class="line">  {<span class='string_literal'>"dtype"</span>, (getter)THPVariable_dtype, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="987"><td class="num" id="LN987">987</td><td class="line">  {<span class='string_literal'>"layout"</span>, (getter)THPVariable_layout, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="988"><td class="num" id="LN988">988</td><td class="line">  {<span class='string_literal'>"device"</span>, (getter)THPVariable_device, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="989"><td class="num" id="LN989">989</td><td class="line">  {<span class='string_literal'>"ndim"</span>, (getter)THPVariable_get_ndim, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="990"><td class="num" id="LN990">990</td><td class="line">  {<span class='string_literal'>"names"</span>, (getter)THPVariable_get_names, (setter)THPVariable_set_names, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="991"><td class="num" id="LN991">991</td><td class="line">  {<span class='string_literal'>"real"</span>, (getter)THPVariable_get_real, (setter)THPVariable_set_real, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="992"><td class="num" id="LN992">992</td><td class="line">  {<span class='string_literal'>"imag"</span>, (getter)THPVariable_get_imag, (setter)THPVariable_set_imag, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="993"><td class="num" id="LN993">993</td><td class="line">  {<span class='keyword'>nullptr</span>}</td></tr>
<tr class="codeline" data-linenumber="994"><td class="num" id="LN994">994</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="995"><td class="num" id="LN995">995</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="996"><td class="num" id="LN996">996</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="997"><td class="num" id="LN997">997</td><td class="line"><span class='keyword'>static</span> PyMappingMethods THPVariable_as_mapping = {</td></tr>
<tr class="codeline" data-linenumber="998"><td class="num" id="LN998">998</td><td class="line">  THPVariable_length,</td></tr>
<tr class="codeline" data-linenumber="999"><td class="num" id="LN999">999</td><td class="line">  THPVariable_getitem,</td></tr>
<tr class="codeline" data-linenumber="1000"><td class="num" id="LN1000">1000</td><td class="line">  THPVariable_setitem,</td></tr>
<tr class="codeline" data-linenumber="1001"><td class="num" id="LN1001">1001</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="1002"><td class="num" id="LN1002">1002</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1003"><td class="num" id="LN1003">1003</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="1004"><td class="num" id="LN1004">1004</td><td class="line"><span class='keyword'>static</span> PyMethodDef extra_methods[] = {</td></tr>
<tr class="codeline" data-linenumber="1005"><td class="num" id="LN1005">1005</td><td class="line">  {<span class='string_literal'>"as_subclass"</span>, castPyCFunctionWithKeywords(THPVariable_as_subclass),</td></tr>
<tr class="codeline" data-linenumber="1006"><td class="num" id="LN1006">1006</td><td class="line">    <span class='macro'>METH_VARARGS<span class='macro_popup'>0x0001</span></span> | <span class='macro'>METH_KEYWORDS<span class='macro_popup'>0x0002</span></span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="1007"><td class="num" id="LN1007">1007</td><td class="line">  {<span class='string_literal'>"_make_subclass"</span>, castPyCFunctionWithKeywords(THPVariable_make_subclass),</td></tr>
<tr class="codeline" data-linenumber="1008"><td class="num" id="LN1008">1008</td><td class="line">    <span class='macro'>METH_STATIC<span class='macro_popup'>0x0020</span></span> | <span class='macro'>METH_VARARGS<span class='macro_popup'>0x0001</span></span> | <span class='macro'>METH_KEYWORDS<span class='macro_popup'>0x0002</span></span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="1009"><td class="num" id="LN1009">1009</td><td class="line">  {<span class='string_literal'>"_fix_weakref"</span>, THPVariable_fix_weakref,</td></tr>
<tr class="codeline" data-linenumber="1010"><td class="num" id="LN1010">1010</td><td class="line">    <span class='macro'>METH_NOARGS<span class='macro_popup'>0x0004</span></span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="1011"><td class="num" id="LN1011">1011</td><td class="line">  {<span class='keyword'>nullptr</span>}</td></tr>
<tr class="codeline" data-linenumber="1012"><td class="num" id="LN1012">1012</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="1013"><td class="num" id="LN1013">1013</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1014"><td class="num" id="LN1014">1014</td><td class="line"><span class='comment'>/* From https://github.com/python/cpython/blob/v3.7.0/Modules/xxsubtype.c</span></td></tr>
<tr class="codeline" data-linenumber="1015"><td class="num" id="LN1015">1015</td><td class="line">   <span class='comment'>If compiled as a shared library instead, some compilers don't allow addresses</span></td></tr>
<tr class="codeline" data-linenumber="1016"><td class="num" id="LN1016">1016</td><td class="line">   <span class='comment'>of Python objects defined in other libraries to be used in static</span></td></tr>
<tr class="codeline" data-linenumber="1017"><td class="num" id="LN1017">1017</td><td class="line">   <span class='comment'>initializers here.  The DEFERRED_ADDRESS macro is used to tag the slots where</span></td></tr>
<tr class="codeline" data-linenumber="1018"><td class="num" id="LN1018">1018</td><td class="line">   <span class='comment'>such addresses appear; the module init function must fill in the tagged slots</span></td></tr>
<tr class="codeline" data-linenumber="1019"><td class="num" id="LN1019">1019</td><td class="line">   <span class='comment'>at runtime.  The argument is for documentation -- the macro ignores it.</span></td></tr>
<tr class="codeline" data-linenumber="1020"><td class="num" id="LN1020">1020</td><td class="line"><span class='comment'>*/</span></td></tr>
<tr class="codeline" data-linenumber="1021"><td class="num" id="LN1021">1021</td><td class="line"><span class='directive'>#define <span class='macro'>DEFERRED_ADDRESS(ADDR)<span class='macro_popup'>nullptr</span></span> nullptr</span></td></tr>
<tr class="codeline" data-linenumber="1022"><td class="num" id="LN1022">1022</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1023"><td class="num" id="LN1023">1023</td><td class="line"><span class='keyword'>struct</span> THPVariableMeta {</td></tr>
<tr class="codeline" data-linenumber="1024"><td class="num" id="LN1024">1024</td><td class="line">  PyHeapTypeObject base;</td></tr>
<tr class="codeline" data-linenumber="1025"><td class="num" id="LN1025">1025</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="1026"><td class="num" id="LN1026">1026</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1027"><td class="num" id="LN1027">1027</td><td class="line"><span class='keyword'>int</span> THPVariableMetaType_init(PyObject *cls, PyObject *args, PyObject *kwargs);</td></tr>
<tr class="codeline" data-linenumber="1028"><td class="num" id="LN1028">1028</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1029"><td class="num" id="LN1029">1029</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="1030"><td class="num" id="LN1030">1030</td><td class="line">PyTypeObject THPVariableMetaType = {</td></tr>
<tr class="codeline" data-linenumber="1031"><td class="num" id="LN1031">1031</td><td class="line">  <span class='macro'>PyVarObject_HEAD_INIT(DEFERRED_ADDRESS(&amp;PyType_Type), 0)<span class='macro_popup'>{ { 1, nullptr }, 0 },</span></span></td></tr>
<tr class="codeline" data-linenumber="1032"><td class="num" id="LN1032">1032</td><td class="line">  <span class='string_literal'>"torch._C._TensorMeta"</span>,                      <span class='comment'>/* tp_name */</span></td></tr>
<tr class="codeline" data-linenumber="1033"><td class="num" id="LN1033">1033</td><td class="line">  <span class='keyword'>sizeof</span>(THPVariableMeta),                     <span class='comment'>/* tp_basicsize */</span></td></tr>
<tr class="codeline" data-linenumber="1034"><td class="num" id="LN1034">1034</td><td class="line">  0,                                           <span class='comment'>/* tp_itemsize */</span></td></tr>
<tr class="codeline" data-linenumber="1035"><td class="num" id="LN1035">1035</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_dealloc */</span></td></tr>
<tr class="codeline" data-linenumber="1036"><td class="num" id="LN1036">1036</td><td class="line">  <span class='comment'>// NOLINTNEXTLINE(modernize-use-nullptr)</span></td></tr>
<tr class="codeline" data-linenumber="1037"><td class="num" id="LN1037">1037</td><td class="line">  0,                                           <span class='comment'>/* tp_vectorcall_offset */</span></td></tr>
<tr class="codeline" data-linenumber="1038"><td class="num" id="LN1038">1038</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_getattr */</span></td></tr>
<tr class="codeline" data-linenumber="1039"><td class="num" id="LN1039">1039</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_setattr */</span></td></tr>
<tr class="codeline" data-linenumber="1040"><td class="num" id="LN1040">1040</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_reserved */</span></td></tr>
<tr class="codeline" data-linenumber="1041"><td class="num" id="LN1041">1041</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_repr */</span></td></tr>
<tr class="codeline" data-linenumber="1042"><td class="num" id="LN1042">1042</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_as_number */</span></td></tr>
<tr class="codeline" data-linenumber="1043"><td class="num" id="LN1043">1043</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_as_sequence */</span></td></tr>
<tr class="codeline" data-linenumber="1044"><td class="num" id="LN1044">1044</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_as_mapping */</span></td></tr>
<tr class="codeline" data-linenumber="1045"><td class="num" id="LN1045">1045</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_hash  */</span></td></tr>
<tr class="codeline" data-linenumber="1046"><td class="num" id="LN1046">1046</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_call */</span></td></tr>
<tr class="codeline" data-linenumber="1047"><td class="num" id="LN1047">1047</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_str */</span></td></tr>
<tr class="codeline" data-linenumber="1048"><td class="num" id="LN1048">1048</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_getattro */</span></td></tr>
<tr class="codeline" data-linenumber="1049"><td class="num" id="LN1049">1049</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_setattro */</span></td></tr>
<tr class="codeline" data-linenumber="1050"><td class="num" id="LN1050">1050</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_as_buffer */</span></td></tr>
<tr class="codeline" data-linenumber="1051"><td class="num" id="LN1051">1051</td><td class="line">  <span class='macro'>Py_TPFLAGS_DEFAULT<span class='macro_popup'>( 0 | (1UL &lt;&lt; 18) | 0)</span></span> | <span class='macro'>Py_TPFLAGS_BASETYPE<span class='macro_popup'>(1UL &lt;&lt; 10)</span></span>,    <span class='comment'>/* tp_flags */</span></td></tr>
<tr class="codeline" data-linenumber="1052"><td class="num" id="LN1052">1052</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_doc */</span></td></tr>
<tr class="codeline" data-linenumber="1053"><td class="num" id="LN1053">1053</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_traverse */</span></td></tr>
<tr class="codeline" data-linenumber="1054"><td class="num" id="LN1054">1054</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_clear */</span></td></tr>
<tr class="codeline" data-linenumber="1055"><td class="num" id="LN1055">1055</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_richcompare */</span></td></tr>
<tr class="codeline" data-linenumber="1056"><td class="num" id="LN1056">1056</td><td class="line">  0,                                           <span class='comment'>/* tp_weaklistoffset */</span></td></tr>
<tr class="codeline" data-linenumber="1057"><td class="num" id="LN1057">1057</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_iter */</span></td></tr>
<tr class="codeline" data-linenumber="1058"><td class="num" id="LN1058">1058</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_iternext */</span></td></tr>
<tr class="codeline" data-linenumber="1059"><td class="num" id="LN1059">1059</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_methods */</span></td></tr>
<tr class="codeline" data-linenumber="1060"><td class="num" id="LN1060">1060</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_members */</span></td></tr>
<tr class="codeline" data-linenumber="1061"><td class="num" id="LN1061">1061</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_getset */</span></td></tr>
<tr class="codeline" data-linenumber="1062"><td class="num" id="LN1062">1062</td><td class="line">  <span class='macro'>DEFERRED_ADDRESS(&amp;PyType_Type)<span class='macro_popup'>nullptr</span></span>,              <span class='comment'>/* tp_base */</span></td></tr>
<tr class="codeline" data-linenumber="1063"><td class="num" id="LN1063">1063</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_dict */</span></td></tr>
<tr class="codeline" data-linenumber="1064"><td class="num" id="LN1064">1064</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_descr_get */</span></td></tr>
<tr class="codeline" data-linenumber="1065"><td class="num" id="LN1065">1065</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_descr_set */</span></td></tr>
<tr class="codeline" data-linenumber="1066"><td class="num" id="LN1066">1066</td><td class="line">  0,                                           <span class='comment'>/* tp_dictoffset */</span></td></tr>
<tr class="codeline" data-linenumber="1067"><td class="num" id="LN1067">1067</td><td class="line">  THPVariableMetaType_init,                    <span class='comment'>/* tp_init */</span></td></tr>
<tr class="codeline" data-linenumber="1068"><td class="num" id="LN1068">1068</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_alloc */</span></td></tr>
<tr class="codeline" data-linenumber="1069"><td class="num" id="LN1069">1069</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_new */</span></td></tr>
<tr class="codeline" data-linenumber="1070"><td class="num" id="LN1070">1070</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="1071"><td class="num" id="LN1071">1071</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1072"><td class="num" id="LN1072">1072</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="1073"><td class="num" id="LN1073">1073</td><td class="line">PyTypeObject THPVariableType = {</td></tr>
<tr class="codeline" data-linenumber="1074"><td class="num" id="LN1074">1074</td><td class="line">    <span class='macro'>PyVarObject_HEAD_INIT(<span class='macro_popup'>{ { 1, &amp;THPVariableMetaType }, 0 },</span></span></td></tr>
<tr class="codeline" data-linenumber="1075"><td class="num" id="LN1075">1075</td><td class="line">        <span class='macro'>&amp;THPVariableMetaType,<span class='macro_popup'>{ { 1, &amp;THPVariableMetaType }, 0 },</span></span></td></tr>
<tr class="codeline" data-linenumber="1076"><td class="num" id="LN1076">1076</td><td class="line">        <span class='macro'>0)<span class='macro_popup'>{ { 1, &amp;THPVariableMetaType }, 0 },</span></span> <span class='string_literal'>"torch._C._TensorBase"</span>, <span class='comment'>/* tp_name */</span></td></tr>
<tr class="codeline" data-linenumber="1077"><td class="num" id="LN1077">1077</td><td class="line">    <span class='keyword'>sizeof</span>(THPVariable), <span class='comment'>/* tp_basicsize */</span></td></tr>
<tr class="codeline" data-linenumber="1078"><td class="num" id="LN1078">1078</td><td class="line">    0, <span class='comment'>/* tp_itemsize */</span></td></tr>
<tr class="codeline" data-linenumber="1079"><td class="num" id="LN1079">1079</td><td class="line">    <span class='comment'>// This is unspecified, because it is illegal to create a THPVariableType</span></td></tr>
<tr class="codeline" data-linenumber="1080"><td class="num" id="LN1080">1080</td><td class="line">    <span class='comment'>// directly.  Subclasses will have their tp_dealloc set appropriately</span></td></tr>
<tr class="codeline" data-linenumber="1081"><td class="num" id="LN1081">1081</td><td class="line">    <span class='comment'>// by the metaclass</span></td></tr>
<tr class="codeline" data-linenumber="1082"><td class="num" id="LN1082">1082</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_dealloc */</span></td></tr>
<tr class="codeline" data-linenumber="1083"><td class="num" id="LN1083">1083</td><td class="line">    <span class='comment'>// NOLINTNEXTLINE(modernize-use-nullptr)</span></td></tr>
<tr class="codeline" data-linenumber="1084"><td class="num" id="LN1084">1084</td><td class="line">    0, <span class='comment'>/* tp_vectorcall_offset */</span></td></tr>
<tr class="codeline" data-linenumber="1085"><td class="num" id="LN1085">1085</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_getattr */</span></td></tr>
<tr class="codeline" data-linenumber="1086"><td class="num" id="LN1086">1086</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_setattr */</span></td></tr>
<tr class="codeline" data-linenumber="1087"><td class="num" id="LN1087">1087</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_reserved */</span></td></tr>
<tr class="codeline" data-linenumber="1088"><td class="num" id="LN1088">1088</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_repr */</span></td></tr>
<tr class="codeline" data-linenumber="1089"><td class="num" id="LN1089">1089</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_as_number */</span></td></tr>
<tr class="codeline" data-linenumber="1090"><td class="num" id="LN1090">1090</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_as_sequence */</span></td></tr>
<tr class="codeline" data-linenumber="1091"><td class="num" id="LN1091">1091</td><td class="line">    &amp;THPVariable_as_mapping, <span class='comment'>/* tp_as_mapping */</span></td></tr>
<tr class="codeline" data-linenumber="1092"><td class="num" id="LN1092">1092</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_hash  */</span></td></tr>
<tr class="codeline" data-linenumber="1093"><td class="num" id="LN1093">1093</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_call */</span></td></tr>
<tr class="codeline" data-linenumber="1094"><td class="num" id="LN1094">1094</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_str */</span></td></tr>
<tr class="codeline" data-linenumber="1095"><td class="num" id="LN1095">1095</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_getattro */</span></td></tr>
<tr class="codeline" data-linenumber="1096"><td class="num" id="LN1096">1096</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_setattro */</span></td></tr>
<tr class="codeline" data-linenumber="1097"><td class="num" id="LN1097">1097</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_as_buffer */</span></td></tr>
<tr class="codeline" data-linenumber="1098"><td class="num" id="LN1098">1098</td><td class="line">    <span class='macro'>Py_TPFLAGS_DEFAULT<span class='macro_popup'>( 0 | (1UL &lt;&lt; 18) | 0)</span></span> | <span class='macro'>Py_TPFLAGS_BASETYPE<span class='macro_popup'>(1UL &lt;&lt; 10)</span></span> |</td></tr>
<tr class="codeline" data-linenumber="1099"><td class="num" id="LN1099">1099</td><td class="line">        <span class='macro'>Py_TPFLAGS_HAVE_GC<span class='macro_popup'>(1UL &lt;&lt; 14)</span></span>, <span class='comment'>/* tp_flags */</span></td></tr>
<tr class="codeline" data-linenumber="1100"><td class="num" id="LN1100">1100</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_doc */</span></td></tr>
<tr class="codeline" data-linenumber="1101"><td class="num" id="LN1101">1101</td><td class="line">    <span class='comment'>// Also set by metaclass</span></td></tr>
<tr class="codeline" data-linenumber="1102"><td class="num" id="LN1102">1102</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_traverse */</span></td></tr>
<tr class="codeline" data-linenumber="1103"><td class="num" id="LN1103">1103</td><td class="line">    (inquiry)THPVariable_clear, <span class='comment'>/* tp_clear */</span></td></tr>
<tr class="codeline" data-linenumber="1104"><td class="num" id="LN1104">1104</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_richcompare */</span></td></tr>
<tr class="codeline" data-linenumber="1105"><td class="num" id="LN1105">1105</td><td class="line">    0, <span class='comment'>/* tp_weaklistoffset */</span></td></tr>
<tr class="codeline" data-linenumber="1106"><td class="num" id="LN1106">1106</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_iter */</span></td></tr>
<tr class="codeline" data-linenumber="1107"><td class="num" id="LN1107">1107</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_iternext */</span></td></tr>
<tr class="codeline" data-linenumber="1108"><td class="num" id="LN1108">1108</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_methods */</span></td></tr>
<tr class="codeline" data-linenumber="1109"><td class="num" id="LN1109">1109</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_members */</span></td></tr>
<tr class="codeline" data-linenumber="1110"><td class="num" id="LN1110">1110</td><td class="line">    THPVariable_properties, <span class='comment'>/* tp_getset */</span></td></tr>
<tr class="codeline" data-linenumber="1111"><td class="num" id="LN1111">1111</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_base */</span></td></tr>
<tr class="codeline" data-linenumber="1112"><td class="num" id="LN1112">1112</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_dict */</span></td></tr>
<tr class="codeline" data-linenumber="1113"><td class="num" id="LN1113">1113</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_descr_get */</span></td></tr>
<tr class="codeline" data-linenumber="1114"><td class="num" id="LN1114">1114</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_descr_set */</span></td></tr>
<tr class="codeline" data-linenumber="1115"><td class="num" id="LN1115">1115</td><td class="line">    0, <span class='comment'>/* tp_dictoffset */</span></td></tr>
<tr class="codeline" data-linenumber="1116"><td class="num" id="LN1116">1116</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_init */</span></td></tr>
<tr class="codeline" data-linenumber="1117"><td class="num" id="LN1117">1117</td><td class="line">    <span class='keyword'>nullptr</span>, <span class='comment'>/* tp_alloc */</span></td></tr>
<tr class="codeline" data-linenumber="1118"><td class="num" id="LN1118">1118</td><td class="line">    <span class='comment'>// Although new is provided here, it is illegal to call this with cls ==</span></td></tr>
<tr class="codeline" data-linenumber="1119"><td class="num" id="LN1119">1119</td><td class="line">    <span class='comment'>// THPVariableMeta.  Instead, subclass it first and then construct it</span></td></tr>
<tr class="codeline" data-linenumber="1120"><td class="num" id="LN1120">1120</td><td class="line">    THPVariable_pynew, <span class='comment'>/* tp_new */</span></td></tr>
<tr class="codeline" data-linenumber="1121"><td class="num" id="LN1121">1121</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="1122"><td class="num" id="LN1122">1122</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1123"><td class="num" id="LN1123">1123</td><td class="line">PyObject *THPVariable_pynew(PyTypeObject *type, PyObject *args, PyObject *kwargs)</td></tr>
<tr class="codeline" data-linenumber="1124"><td class="num" id="LN1124">1124</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="1125"><td class="num" id="LN1125">1125</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="1126"><td class="num" id="LN1126">1126</td><td class="line">  <span class='macro'>TORCH_CHECK(type != &amp;THPVariableType, <span class='string_literal'>"Cannot directly construct _TensorBase; subclass it and then construct that"</span>)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(type != &amp;<br>THPVariableType)), 0))) { ::c10::detail::torchCheckFail( __func__<br>, "../torch/csrc/autograd/python_variable.cpp", static_cast&lt;<br>uint32_t&gt;(1126), (::c10::detail::torchCheckMsgImpl( "Expected "<br> "type != &amp;THPVariableType" " to be true, but got false.  "<br> "(Could this error message be improved?  If so, " "please report an enhancement request to PyTorch.)"<br>, "Cannot directly construct _TensorBase; subclass it and then construct that"<br>))); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1127"><td class="num" id="LN1127">1127</td><td class="line">  jit::tracer::warn(<span class='string_literal'>"torch.Tensor"</span>, jit::tracer::WARN_CONSTRUCTOR);</td></tr>
<tr class="codeline" data-linenumber="1128"><td class="num" id="LN1128">1128</td><td class="line">  <span class='keyword'>auto</span> tensor = torch::utils::legacy_tensor_ctor(torch::tensors::get_default_dispatch_key(), torch::tensors::get_default_scalar_type(), args, kwargs);</td></tr>
<tr class="codeline" data-linenumber="1129"><td class="num" id="LN1129">1129</td><td class="line">  <span class='comment'>// WARNING: tensor is NOT guaranteed to be a fresh tensor; e.g., if it was</span></td></tr>
<tr class="codeline" data-linenumber="1130"><td class="num" id="LN1130">1130</td><td class="line">  <span class='comment'>// given a raw pointer that will refcount bump</span></td></tr>
<tr class="codeline" data-linenumber="1131"><td class="num" id="LN1131">1131</td><td class="line">  <span class='keyword'>return</span> THPVariable_NewWithVar(</td></tr>
<tr class="codeline" data-linenumber="1132"><td class="num" id="LN1132">1132</td><td class="line">      type,</td></tr>
<tr class="codeline" data-linenumber="1133"><td class="num" id="LN1133">1133</td><td class="line">      std::move(tensor),</td></tr>
<tr class="codeline" data-linenumber="1134"><td class="num" id="LN1134">1134</td><td class="line">      c10::impl::PyInterpreterStatus::MAYBE_UNINITIALIZED);</td></tr>
<tr class="codeline" data-linenumber="1135"><td class="num" id="LN1135">1135</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="1136"><td class="num" id="LN1136">1136</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1137"><td class="num" id="LN1137">1137</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1138"><td class="num" id="LN1138">1138</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>void</span> clear_slots(PyTypeObject* type, PyObject* self) {</td></tr>
<tr class="codeline" data-linenumber="1139"><td class="num" id="LN1139">1139</td><td class="line">  Py_ssize_t i, n;</td></tr>
<tr class="codeline" data-linenumber="1140"><td class="num" id="LN1140">1140</td><td class="line">  PyMemberDef* mp;</td></tr>
<tr class="codeline" data-linenumber="1141"><td class="num" id="LN1141">1141</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1142"><td class="num" id="LN1142">1142</td><td class="line">  n = <span class='macro'>Py_SIZE(type)<span class='macro_popup'>(((PyVarObject*)(type))-&gt;ob_size)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1143"><td class="num" id="LN1143">1143</td><td class="line">  mp = <span class='macro'>PyHeapType_GET_MEMBERS((PyHeapTypeObject*)type)<span class='macro_popup'>((PyMemberDef *)(((char *)(PyHeapTypeObject*)type) + (((PyObject<br>*)((PyHeapTypeObject*)type))-&gt;ob_type)-&gt;tp_basicsize))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1144"><td class="num" id="LN1144">1144</td><td class="line">  <span class='keyword'>for</span> (i = 0; i &lt; n; i++, mp++) {</td></tr>
<tr class="codeline" data-linenumber="1145"><td class="num" id="LN1145">1145</td><td class="line">    <span class='keyword'>if</span> (mp-&gt;type == <span class='macro'>T_OBJECT_EX<span class='macro_popup'>16</span></span> &amp;&amp; !(mp-&gt;flags &amp; <span class='macro'>READONLY<span class='macro_popup'>1</span></span>)) {</td></tr>
<tr class="codeline" data-linenumber="1146"><td class="num" id="LN1146">1146</td><td class="line">      <span class='keyword'>char</span>* addr = (<span class='keyword'>char</span>*)self + mp-&gt;offset;</td></tr>
<tr class="codeline" data-linenumber="1147"><td class="num" id="LN1147">1147</td><td class="line">      PyObject* obj = *(PyObject**)addr;</td></tr>
<tr class="codeline" data-linenumber="1148"><td class="num" id="LN1148">1148</td><td class="line">      <span class='keyword'>if</span> (obj != <span class='macro'>NULL<span class='macro_popup'>__null</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="1149"><td class="num" id="LN1149">1149</td><td class="line">        *(PyObject**)addr = <span class='macro'>NULL<span class='macro_popup'>__null</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1150"><td class="num" id="LN1150">1150</td><td class="line">        <span class='macro'>Py_DECREF(obj)<span class='macro_popup'>_Py_DECREF(((PyObject*)(obj)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1151"><td class="num" id="LN1151">1151</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="1152"><td class="num" id="LN1152">1152</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1153"><td class="num" id="LN1153">1153</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1154"><td class="num" id="LN1154">1154</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1155"><td class="num" id="LN1155">1155</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1156"><td class="num" id="LN1156">1156</td><td class="line"><span class='comment'>// NB: this is not the tp_dealloc on THPVariable; instead, its the dealloc</span></td></tr>
<tr class="codeline" data-linenumber="1157"><td class="num" id="LN1157">1157</td><td class="line"><span class='comment'>// on subclasses.  It's never valid to construct a THPVariable so it's not</span></td></tr>
<tr class="codeline" data-linenumber="1158"><td class="num" id="LN1158">1158</td><td class="line"><span class='comment'>// necessary to implement the dealloc for that case</span></td></tr>
<tr class="codeline" data-linenumber="1159"><td class="num" id="LN1159">1159</td><td class="line"><span class='keyword'>void</span> THPVariable_subclass_dealloc(PyObject* self) {</td></tr>
<tr class="codeline" data-linenumber="1160"><td class="num" id="LN1160">1160</td><td class="line">  <span class='keyword'>if</span> (THPVariable_tryResurrect((THPVariable*)self))</td></tr>
<tr class="codeline" data-linenumber="1161"><td class="num" id="LN1161">1161</td><td class="line">    <span class='keyword'>return</span>;</td></tr>
<tr class="codeline" data-linenumber="1162"><td class="num" id="LN1162">1162</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1163"><td class="num" id="LN1163">1163</td><td class="line">  <span class='comment'>// This is like a crappy version of subtype_dealloc.</span></td></tr>
<tr class="codeline" data-linenumber="1164"><td class="num" id="LN1164">1164</td><td class="line">  <span class='comment'>// Unfortunately, we cannot directly delegate to</span></td></tr>
<tr class="codeline" data-linenumber="1165"><td class="num" id="LN1165">1165</td><td class="line">  <span class='comment'>// subtype_dealloc as it will start walking the parent</span></td></tr>
<tr class="codeline" data-linenumber="1166"><td class="num" id="LN1166">1166</td><td class="line">  <span class='comment'>// chain *starting with* the type of self, which will cause</span></td></tr>
<tr class="codeline" data-linenumber="1167"><td class="num" id="LN1167">1167</td><td class="line">  <span class='comment'>// us to go back to our custom dealloc.</span></td></tr>
<tr class="codeline" data-linenumber="1168"><td class="num" id="LN1168">1168</td><td class="line">  <span class='comment'>//</span></td></tr>
<tr class="codeline" data-linenumber="1169"><td class="num" id="LN1169">1169</td><td class="line">  <span class='comment'>// We have to replicate the subtype_dealloc logic to ensure</span></td></tr>
<tr class="codeline" data-linenumber="1170"><td class="num" id="LN1170">1170</td><td class="line">  <span class='comment'>// that finalizers are handled correctly</span></td></tr>
<tr class="codeline" data-linenumber="1171"><td class="num" id="LN1171">1171</td><td class="line">  PyTypeObject* type = <span class='macro'>Py_TYPE(self)<span class='macro_popup'>(((PyObject*)(self))-&gt;ob_type)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1172"><td class="num" id="LN1172">1172</td><td class="line">  <span class='macro'>TORCH_INTERNAL_ASSERT(type-&gt;tp_flags &amp; Py_TPFLAGS_HEAPTYPE)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(type-&gt;tp_flags<br> &amp; (1UL &lt;&lt; 9))), 0))) { ::c10::detail::torchInternalAssertFail<br>( __func__, "../torch/csrc/autograd/python_variable.cpp", static_cast<br>&lt;uint32_t&gt;(1172), "type-&gt;tp_flags &amp; Py_TPFLAGS_HEAPTYPE"<br> "INTERNAL ASSERT FAILED at " "\"../torch/csrc/autograd/python_variable.cpp\""<br> ":" "1172" ", please report a bug to PyTorch. ", c10::str())<br>; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1173"><td class="num" id="LN1173">1173</td><td class="line">  <span class='macro'>TORCH_INTERNAL_ASSERT(PyType_IS_GC(type), <span class='string_literal'>"GC types not implemented"</span>)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(((((type))-&gt;<br>tp_flags &amp; ((1UL &lt;&lt; 14))) != 0))), 0))) { ::c10::detail<br>::torchInternalAssertFail( __func__, "../torch/csrc/autograd/python_variable.cpp"<br>, static_cast&lt;uint32_t&gt;(1173), "PyType_IS_GC(type)" "INTERNAL ASSERT FAILED at "<br> "\"../torch/csrc/autograd/python_variable.cpp\"" ":" "1173" ", please report a bug to PyTorch. "<br>, c10::str("GC types not implemented")); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1174"><td class="num" id="LN1174">1174</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1175"><td class="num" id="LN1175">1175</td><td class="line">  PyObject_GC_UnTrack(self);</td></tr>
<tr class="codeline" data-linenumber="1176"><td class="num" id="LN1176">1176</td><td class="line">  <span class='comment'>// TODO: consider using trash can</span></td></tr>
<tr class="codeline" data-linenumber="1177"><td class="num" id="LN1177">1177</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1178"><td class="num" id="LN1178">1178</td><td class="line">  <span class='keyword'>bool</span> has_finalizer = type-&gt;tp_finalize || type-&gt;tp_del;</td></tr>
<tr class="codeline" data-linenumber="1179"><td class="num" id="LN1179">1179</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1180"><td class="num" id="LN1180">1180</td><td class="line">  <span class='keyword'>if</span> (type-&gt;tp_finalize) {</td></tr>
<tr class="codeline" data-linenumber="1181"><td class="num" id="LN1181">1181</td><td class="line">    PyObject_GC_Track(self);</td></tr>
<tr class="codeline" data-linenumber="1182"><td class="num" id="LN1182">1182</td><td class="line">    <span class='keyword'>if</span> (PyObject_CallFinalizerFromDealloc(self) &lt; 0) {</td></tr>
<tr class="codeline" data-linenumber="1183"><td class="num" id="LN1183">1183</td><td class="line">      <span class='comment'>/* Resurrected */</span></td></tr>
<tr class="codeline" data-linenumber="1184"><td class="num" id="LN1184">1184</td><td class="line">      <span class='keyword'>return</span>;</td></tr>
<tr class="codeline" data-linenumber="1185"><td class="num" id="LN1185">1185</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1186"><td class="num" id="LN1186">1186</td><td class="line">    PyObject_GC_UnTrack(self);</td></tr>
<tr class="codeline" data-linenumber="1187"><td class="num" id="LN1187">1187</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1188"><td class="num" id="LN1188">1188</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1189"><td class="num" id="LN1189">1189</td><td class="line">  <span class='comment'>// base test is unnecessary as THPVariable does not set this</span></td></tr>
<tr class="codeline" data-linenumber="1190"><td class="num" id="LN1190">1190</td><td class="line">  <span class='keyword'>if</span> (type-&gt;tp_weaklistoffset) {</td></tr>
<tr class="codeline" data-linenumber="1191"><td class="num" id="LN1191">1191</td><td class="line">    PyObject_ClearWeakRefs(self);</td></tr>
<tr class="codeline" data-linenumber="1192"><td class="num" id="LN1192">1192</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1193"><td class="num" id="LN1193">1193</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1194"><td class="num" id="LN1194">1194</td><td class="line">  <span class='keyword'>if</span> (type-&gt;tp_del) {</td></tr>
<tr class="codeline" data-linenumber="1195"><td class="num" id="LN1195">1195</td><td class="line">    PyObject_GC_Track(self);</td></tr>
<tr class="codeline" data-linenumber="1196"><td class="num" id="LN1196">1196</td><td class="line">    type-&gt;tp_del(self);</td></tr>
<tr class="codeline" data-linenumber="1197"><td class="num" id="LN1197">1197</td><td class="line">    <span class='keyword'>if</span> (self-&gt;ob_refcnt &gt; 0) {</td></tr>
<tr class="codeline" data-linenumber="1198"><td class="num" id="LN1198">1198</td><td class="line">      <span class='comment'>/* Resurrected */</span></td></tr>
<tr class="codeline" data-linenumber="1199"><td class="num" id="LN1199">1199</td><td class="line">      <span class='keyword'>return</span>;</td></tr>
<tr class="codeline" data-linenumber="1200"><td class="num" id="LN1200">1200</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1201"><td class="num" id="LN1201">1201</td><td class="line">    PyObject_GC_UnTrack(self);</td></tr>
<tr class="codeline" data-linenumber="1202"><td class="num" id="LN1202">1202</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1203"><td class="num" id="LN1203">1203</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1204"><td class="num" id="LN1204">1204</td><td class="line">  <span class='keyword'>if</span> (has_finalizer) {</td></tr>
<tr class="codeline" data-linenumber="1205"><td class="num" id="LN1205">1205</td><td class="line">    <span class='comment'>/* New weakrefs could be created during the finalizer call.</span></td></tr>
<tr class="codeline" data-linenumber="1206"><td class="num" id="LN1206">1206</td><td class="line">       <span class='comment'>If this occurs, clear them out without calling their</span></td></tr>
<tr class="codeline" data-linenumber="1207"><td class="num" id="LN1207">1207</td><td class="line">       <span class='comment'>finalizers since they might rely on part of the object</span></td></tr>
<tr class="codeline" data-linenumber="1208"><td class="num" id="LN1208">1208</td><td class="line">       <span class='comment'>being finalized that has already been destroyed. */</span></td></tr>
<tr class="codeline" data-linenumber="1209"><td class="num" id="LN1209">1209</td><td class="line">    <span class='keyword'>if</span> (type-&gt;tp_weaklistoffset) {</td></tr>
<tr class="codeline" data-linenumber="1210"><td class="num" id="LN1210">1210</td><td class="line">      <span class='comment'>/* Modeled after GET_WEAKREFS_LISTPTR() */</span></td></tr>
<tr class="codeline" data-linenumber="1211"><td class="num" id="LN1211">1211</td><td class="line">      PyWeakReference** list =</td></tr>
<tr class="codeline" data-linenumber="1212"><td class="num" id="LN1212">1212</td><td class="line">          (PyWeakReference**)<span class='macro'>PyObject_GET_WEAKREFS_LISTPTR(self)<span class='macro_popup'>((PyObject **) (((char *) (self)) + (((PyObject*)(self))-&gt;<br>ob_type)-&gt;tp_weaklistoffset))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1213"><td class="num" id="LN1213">1213</td><td class="line">      <span class='keyword'>while</span> (*list)</td></tr>
<tr class="codeline" data-linenumber="1214"><td class="num" id="LN1214">1214</td><td class="line">        _PyWeakref_ClearRef(*list);</td></tr>
<tr class="codeline" data-linenumber="1215"><td class="num" id="LN1215">1215</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1216"><td class="num" id="LN1216">1216</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1217"><td class="num" id="LN1217">1217</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1218"><td class="num" id="LN1218">1218</td><td class="line">  <span class='comment'>// Clear all slots until we get to base class THPVariableType</span></td></tr>
<tr class="codeline" data-linenumber="1219"><td class="num" id="LN1219">1219</td><td class="line">  {</td></tr>
<tr class="codeline" data-linenumber="1220"><td class="num" id="LN1220">1220</td><td class="line">    PyTypeObject* base = type;</td></tr>
<tr class="codeline" data-linenumber="1221"><td class="num" id="LN1221">1221</td><td class="line">    <span class='keyword'>while</span> (base != &amp;THPVariableType) {</td></tr>
<tr class="codeline" data-linenumber="1222"><td class="num" id="LN1222">1222</td><td class="line">      <span class='keyword'>if</span> (<span class='macro'>Py_SIZE(base)<span class='macro_popup'>(((PyVarObject*)(base))-&gt;ob_size)</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="1223"><td class="num" id="LN1223">1223</td><td class="line">        clear_slots(base, self);</td></tr>
<tr class="codeline" data-linenumber="1224"><td class="num" id="LN1224">1224</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="1225"><td class="num" id="LN1225">1225</td><td class="line">      base = base-&gt;tp_base;</td></tr>
<tr class="codeline" data-linenumber="1226"><td class="num" id="LN1226">1226</td><td class="line">      <span class='macro'>TORCH_INTERNAL_ASSERT(base)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(base)), 0))) {<br> ::c10::detail::torchInternalAssertFail( __func__, "../torch/csrc/autograd/python_variable.cpp"<br>, static_cast&lt;uint32_t&gt;(1226), "base" "INTERNAL ASSERT FAILED at "<br> "\"../torch/csrc/autograd/python_variable.cpp\"" ":" "1226" ", please report a bug to PyTorch. "<br>, c10::str()); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1227"><td class="num" id="LN1227">1227</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1228"><td class="num" id="LN1228">1228</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1229"><td class="num" id="LN1229">1229</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1230"><td class="num" id="LN1230">1230</td><td class="line">  <span class='comment'>// All Python defined classes have __dict__</span></td></tr>
<tr class="codeline" data-linenumber="1231"><td class="num" id="LN1231">1231</td><td class="line">  <span class='keyword'>if</span> (<span class='macro'>C10_LIKELY(type-&gt;tp_dictoffset)<span class='macro_popup'>(__builtin_expect(static_cast&lt;bool&gt;(type-&gt;tp_dictoffset<br>), 1))</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="1232"><td class="num" id="LN1232">1232</td><td class="line">    PyObject** dictptr = _PyObject_GetDictPtr(self);</td></tr>
<tr class="codeline" data-linenumber="1233"><td class="num" id="LN1233">1233</td><td class="line">    <span class='keyword'>if</span> (dictptr != <span class='macro'>NULL<span class='macro_popup'>__null</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="1234"><td class="num" id="LN1234">1234</td><td class="line">      PyObject* dict = *dictptr;</td></tr>
<tr class="codeline" data-linenumber="1235"><td class="num" id="LN1235">1235</td><td class="line">      <span class='keyword'>if</span> (dict != <span class='macro'>NULL<span class='macro_popup'>__null</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="1236"><td class="num" id="LN1236">1236</td><td class="line">        <span class='macro'>Py_DECREF(dict)<span class='macro_popup'>_Py_DECREF(((PyObject*)(dict)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1237"><td class="num" id="LN1237">1237</td><td class="line">        *dictptr = <span class='macro'>NULL<span class='macro_popup'>__null</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1238"><td class="num" id="LN1238">1238</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="1239"><td class="num" id="LN1239">1239</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1240"><td class="num" id="LN1240">1240</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1241"><td class="num" id="LN1241">1241</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1242"><td class="num" id="LN1242">1242</td><td class="line">  <span class='comment'>// subtype_dealloc allows for this but we don't</span></td></tr>
<tr class="codeline" data-linenumber="1243"><td class="num" id="LN1243">1243</td><td class="line">  <span class='macro'>TORCH_INTERNAL_ASSERT(Py_TYPE(self) == type)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!((((PyObject*)<br>(self))-&gt;ob_type) == type)), 0))) { ::c10::detail::torchInternalAssertFail<br>( __func__, "../torch/csrc/autograd/python_variable.cpp", static_cast<br>&lt;uint32_t&gt;(1243), "Py_TYPE(self) == type" "INTERNAL ASSERT FAILED at "<br> "\"../torch/csrc/autograd/python_variable.cpp\"" ":" "1243" ", please report a bug to PyTorch. "<br>, c10::str()); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1244"><td class="num" id="LN1244">1244</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1245"><td class="num" id="LN1245">1245</td><td class="line">  <span class='comment'>// Finally clear out the base THPVariable</span></td></tr>
<tr class="codeline" data-linenumber="1246"><td class="num" id="LN1246">1246</td><td class="line">  THPVariable_clear((THPVariable*)self);</td></tr>
<tr class="codeline" data-linenumber="1247"><td class="num" id="LN1247">1247</td><td class="line">  ((THPVariable*)self)-&gt;cdata.~MaybeOwned&lt;Variable&gt;();</td></tr>
<tr class="codeline" data-linenumber="1248"><td class="num" id="LN1248">1248</td><td class="line">  <span class='macro'>Py_TYPE(self)<span class='macro_popup'>(((PyObject*)(self))-&gt;ob_type)</span></span>-&gt;tp_free(self);</td></tr>
<tr class="codeline" data-linenumber="1249"><td class="num" id="LN1249">1249</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1250"><td class="num" id="LN1250">1250</td><td class="line">  <span class='comment'>// Python defined subclasses should always be on the heap</span></td></tr>
<tr class="codeline" data-linenumber="1251"><td class="num" id="LN1251">1251</td><td class="line">  <span class='macro'>TORCH_INTERNAL_ASSERT(type-&gt;tp_flags &amp; Py_TPFLAGS_HEAPTYPE)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(type-&gt;tp_flags<br> &amp; (1UL &lt;&lt; 9))), 0))) { ::c10::detail::torchInternalAssertFail<br>( __func__, "../torch/csrc/autograd/python_variable.cpp", static_cast<br>&lt;uint32_t&gt;(1251), "type-&gt;tp_flags &amp; Py_TPFLAGS_HEAPTYPE"<br> "INTERNAL ASSERT FAILED at " "\"../torch/csrc/autograd/python_variable.cpp\""<br> ":" "1251" ", please report a bug to PyTorch. ", c10::str())<br>; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1252"><td class="num" id="LN1252">1252</td><td class="line">  <span class='macro'>Py_DECREF(type)<span class='macro_popup'>_Py_DECREF(((PyObject*)(type)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1253"><td class="num" id="LN1253">1253</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1254"><td class="num" id="LN1254">1254</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1255"><td class="num" id="LN1255">1255</td><td class="line"><span class='comment'>/// NOTE [ PyObject Traversal ]</span></td></tr>
<tr class="codeline" data-linenumber="1256"><td class="num" id="LN1256">1256</td><td class="line"><span class='comment'>///</span></td></tr>
<tr class="codeline" data-linenumber="1257"><td class="num" id="LN1257">1257</td><td class="line"><span class='comment'>/// PyObjects that are wrapping c++ objects can lead to non-trivial traverse logic</span></td></tr>
<tr class="codeline" data-linenumber="1258"><td class="num" id="LN1258">1258</td><td class="line"><span class='comment'>/// and it can be tricky to know what to traverse and when. This note tries to</span></td></tr>
<tr class="codeline" data-linenumber="1259"><td class="num" id="LN1259">1259</td><td class="line"><span class='comment'>/// clarify what is the danger here and a simple algorithm to choose how to write</span></td></tr>
<tr class="codeline" data-linenumber="1260"><td class="num" id="LN1260">1260</td><td class="line"><span class='comment'>/// the tp_traverse and tp_clear functions.</span></td></tr>
<tr class="codeline" data-linenumber="1261"><td class="num" id="LN1261">1261</td><td class="line"><span class='comment'>/// If you're not already familiar with how the CPython GC works, you should read this</span></td></tr>
<tr class="codeline" data-linenumber="1262"><td class="num" id="LN1262">1262</td><td class="line"><span class='comment'>/// in-depth description: https://devguide.python.org/garbage_collector/</span></td></tr>
<tr class="codeline" data-linenumber="1263"><td class="num" id="LN1263">1263</td><td class="line"><span class='comment'>///</span></td></tr>
<tr class="codeline" data-linenumber="1264"><td class="num" id="LN1264">1264</td><td class="line"><span class='comment'>/// The complexity for us comes from the fact that some c++ shared_ptr objects</span></td></tr>
<tr class="codeline" data-linenumber="1265"><td class="num" id="LN1265">1265</td><td class="line"><span class='comment'>/// own references to python objects and are also owned both by other python objects</span></td></tr>
<tr class="codeline" data-linenumber="1266"><td class="num" id="LN1266">1266</td><td class="line"><span class='comment'>/// and c++ objects. This means that to allow the GC to collect all cycles, we need to</span></td></tr>
<tr class="codeline" data-linenumber="1267"><td class="num" id="LN1267">1267</td><td class="line"><span class='comment'>/// properly implement the traverse/clear methods that take into account these C++</span></td></tr>
<tr class="codeline" data-linenumber="1268"><td class="num" id="LN1268">1268</td><td class="line"><span class='comment'>/// ownership links.</span></td></tr>
<tr class="codeline" data-linenumber="1269"><td class="num" id="LN1269">1269</td><td class="line"><span class='comment'>///</span></td></tr>
<tr class="codeline" data-linenumber="1270"><td class="num" id="LN1270">1270</td><td class="line"><span class='comment'>/// The main danger here comes from the fact that, while all python-related code is</span></td></tr>
<tr class="codeline" data-linenumber="1271"><td class="num" id="LN1271">1271</td><td class="line"><span class='comment'>/// thread safe wrt the GC execution (thanks to the GIL), other threads might be using</span></td></tr>
<tr class="codeline" data-linenumber="1272"><td class="num" id="LN1272">1272</td><td class="line"><span class='comment'>/// our C++ objects arbitrarily which can lead to shared_ptr ref count going up or down</span></td></tr>
<tr class="codeline" data-linenumber="1273"><td class="num" id="LN1273">1273</td><td class="line"><span class='comment'>/// in between the different traverse/clear invocations.</span></td></tr>
<tr class="codeline" data-linenumber="1274"><td class="num" id="LN1274">1274</td><td class="line"><span class='comment'>/// The one constraint we add here that is not explicitly mentioned in the GC description</span></td></tr>
<tr class="codeline" data-linenumber="1275"><td class="num" id="LN1275">1275</td><td class="line"><span class='comment'>/// above is that for a given GC run (meaning while the GIL is held), the traverse/clear</span></td></tr>
<tr class="codeline" data-linenumber="1276"><td class="num" id="LN1276">1276</td><td class="line"><span class='comment'>/// pair should never report different ownership relations: if traverse visited a given</span></td></tr>
<tr class="codeline" data-linenumber="1277"><td class="num" id="LN1277">1277</td><td class="line"><span class='comment'>/// PyObject, then the clear within that same GC run must still be the sole owner and</span></td></tr>
<tr class="codeline" data-linenumber="1278"><td class="num" id="LN1278">1278</td><td class="line"><span class='comment'>/// clear that PyObject.</span></td></tr>
<tr class="codeline" data-linenumber="1279"><td class="num" id="LN1279">1279</td><td class="line"><span class='comment'>///</span></td></tr>
<tr class="codeline" data-linenumber="1280"><td class="num" id="LN1280">1280</td><td class="line"><span class='comment'>/// A more mechanical algorithm to know what to traverse/clear is as follows:</span></td></tr>
<tr class="codeline" data-linenumber="1281"><td class="num" id="LN1281">1281</td><td class="line"><span class='comment'>///   - Any field on this PyObject that contains a strong reference to another PyObject</span></td></tr>
<tr class="codeline" data-linenumber="1282"><td class="num" id="LN1282">1282</td><td class="line"><span class='comment'>///     must be visited and cleared. An example of that is the "backward_hooks" field of</span></td></tr>
<tr class="codeline" data-linenumber="1283"><td class="num" id="LN1283">1283</td><td class="line"><span class='comment'>///     the THPVariable.</span></td></tr>
<tr class="codeline" data-linenumber="1284"><td class="num" id="LN1284">1284</td><td class="line"><span class='comment'>///   - Any field that contains a C++ object that is uniquely owned by this PyObject (either</span></td></tr>
<tr class="codeline" data-linenumber="1285"><td class="num" id="LN1285">1285</td><td class="line"><span class='comment'>///     a unique_ptr or a shared_ptr with use_count==1) should have all the PyObject it owns</span></td></tr>
<tr class="codeline" data-linenumber="1286"><td class="num" id="LN1286">1286</td><td class="line"><span class='comment'>///     visited and cleared. An example would be here the tensor hooks.</span></td></tr>
<tr class="codeline" data-linenumber="1287"><td class="num" id="LN1287">1287</td><td class="line"><span class='comment'>///   - If that uniquely owned C++ object also uniquely owns other C++ objects, these should be</span></td></tr>
<tr class="codeline" data-linenumber="1288"><td class="num" id="LN1288">1288</td><td class="line"><span class='comment'>///     visited and cleared as well if they contain any PyObject.</span></td></tr>
<tr class="codeline" data-linenumber="1289"><td class="num" id="LN1289">1289</td><td class="line"><span class='comment'>///</span></td></tr>
<tr class="codeline" data-linenumber="1290"><td class="num" id="LN1290">1290</td><td class="line"><span class='comment'>/// Caveat: to avoid slow runtime, we limit the depth of this exploration of C++ objects in</span></td></tr>
<tr class="codeline" data-linenumber="1291"><td class="num" id="LN1291">1291</td><td class="line"><span class='comment'>/// practice and we do not, for example, go through the whole autograd graph, even if it is</span></td></tr>
<tr class="codeline" data-linenumber="1292"><td class="num" id="LN1292">1292</td><td class="line"><span class='comment'>/// uniquely owned. This is a known place where users can create noncollectable cycles as described</span></td></tr>
<tr class="codeline" data-linenumber="1293"><td class="num" id="LN1293">1293</td><td class="line"><span class='comment'>/// in: https://github.com/pytorch/pytorch/issues/7343</span></td></tr>
<tr class="codeline" data-linenumber="1294"><td class="num" id="LN1294">1294</td><td class="line"><span class='comment'>///</span></td></tr>
<tr class="codeline" data-linenumber="1295"><td class="num" id="LN1295">1295</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1296"><td class="num" id="LN1296">1296</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>int</span> traverse_slots(</td></tr>
<tr class="codeline" data-linenumber="1297"><td class="num" id="LN1297">1297</td><td class="line">    PyTypeObject* type,</td></tr>
<tr class="codeline" data-linenumber="1298"><td class="num" id="LN1298">1298</td><td class="line">    PyObject* self,</td></tr>
<tr class="codeline" data-linenumber="1299"><td class="num" id="LN1299">1299</td><td class="line">    visitproc visit,</td></tr>
<tr class="codeline" data-linenumber="1300"><td class="num" id="LN1300">1300</td><td class="line">    <span class='keyword'>void</span>* arg) {</td></tr>
<tr class="codeline" data-linenumber="1301"><td class="num" id="LN1301">1301</td><td class="line">  Py_ssize_t i, n;</td></tr>
<tr class="codeline" data-linenumber="1302"><td class="num" id="LN1302">1302</td><td class="line">  PyMemberDef* mp;</td></tr>
<tr class="codeline" data-linenumber="1303"><td class="num" id="LN1303">1303</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1304"><td class="num" id="LN1304">1304</td><td class="line">  n = <span class='macro'>Py_SIZE(type)<span class='macro_popup'>(((PyVarObject*)(type))-&gt;ob_size)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1305"><td class="num" id="LN1305">1305</td><td class="line">  mp = <span class='macro'>PyHeapType_GET_MEMBERS((PyHeapTypeObject*)type)<span class='macro_popup'>((PyMemberDef *)(((char *)(PyHeapTypeObject*)type) + (((PyObject<br>*)((PyHeapTypeObject*)type))-&gt;ob_type)-&gt;tp_basicsize))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1306"><td class="num" id="LN1306">1306</td><td class="line">  <span class='keyword'>for</span> (i = 0; i &lt; n; i++, mp++) {</td></tr>
<tr class="codeline" data-linenumber="1307"><td class="num" id="LN1307">1307</td><td class="line">    <span class='keyword'>if</span> (mp-&gt;type == <span class='macro'>T_OBJECT_EX<span class='macro_popup'>16</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="1308"><td class="num" id="LN1308">1308</td><td class="line">      <span class='keyword'>char</span>* addr = (<span class='keyword'>char</span>*)self + mp-&gt;offset;</td></tr>
<tr class="codeline" data-linenumber="1309"><td class="num" id="LN1309">1309</td><td class="line">      PyObject* obj = *(PyObject**)addr;</td></tr>
<tr class="codeline" data-linenumber="1310"><td class="num" id="LN1310">1310</td><td class="line">      <span class='keyword'>if</span> (obj != <span class='macro'>NULL<span class='macro_popup'>__null</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="1311"><td class="num" id="LN1311">1311</td><td class="line">        <span class='keyword'>int</span> err = visit(obj, arg);</td></tr>
<tr class="codeline" data-linenumber="1312"><td class="num" id="LN1312">1312</td><td class="line">        <span class='keyword'>if</span> (err)</td></tr>
<tr class="codeline" data-linenumber="1313"><td class="num" id="LN1313">1313</td><td class="line">          <span class='keyword'>return</span> err;</td></tr>
<tr class="codeline" data-linenumber="1314"><td class="num" id="LN1314">1314</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="1315"><td class="num" id="LN1315">1315</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1316"><td class="num" id="LN1316">1316</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1317"><td class="num" id="LN1317">1317</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="1318"><td class="num" id="LN1318">1318</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1319"><td class="num" id="LN1319">1319</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1320"><td class="num" id="LN1320">1320</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>int</span> THPVariable_subclass_traverse(</td></tr>
<tr class="codeline" data-linenumber="1321"><td class="num" id="LN1321">1321</td><td class="line">    PyObject* self,</td></tr>
<tr class="codeline" data-linenumber="1322"><td class="num" id="LN1322">1322</td><td class="line">    visitproc visit,</td></tr>
<tr class="codeline" data-linenumber="1323"><td class="num" id="LN1323">1323</td><td class="line">    <span class='keyword'>void</span>* arg) {</td></tr>
<tr class="codeline" data-linenumber="1324"><td class="num" id="LN1324">1324</td><td class="line">  <span class='comment'>// If the tensor is eligible to be resurrected, don't traverse it; instead</span></td></tr>
<tr class="codeline" data-linenumber="1325"><td class="num" id="LN1325">1325</td><td class="line">  <span class='comment'>// treat all of its references as a root (as they WOULD be a root since we</span></td></tr>
<tr class="codeline" data-linenumber="1326"><td class="num" id="LN1326">1326</td><td class="line">  <span class='comment'>// can treat the inbound C++ references as root owners).</span></td></tr>
<tr class="codeline" data-linenumber="1327"><td class="num" id="LN1327">1327</td><td class="line">  <span class='comment'>//</span></td></tr>
<tr class="codeline" data-linenumber="1328"><td class="num" id="LN1328">1328</td><td class="line">  <span class='comment'>// This works because unlike conventional GCs, Python's GC operates in two</span></td></tr>
<tr class="codeline" data-linenumber="1329"><td class="num" id="LN1329">1329</td><td class="line">  <span class='comment'>// phases: first it uses traverse to discover roots, and then it uses traverse</span></td></tr>
<tr class="codeline" data-linenumber="1330"><td class="num" id="LN1330">1330</td><td class="line">  <span class='comment'>// to do reachability.  Bypassing traverse during root discovery forces Python</span></td></tr>
<tr class="codeline" data-linenumber="1331"><td class="num" id="LN1331">1331</td><td class="line">  <span class='comment'>// to treat self as a root for everything it refers to.  For a full</span></td></tr>
<tr class="codeline" data-linenumber="1332"><td class="num" id="LN1332">1332</td><td class="line">  <span class='comment'>// explanation of the algorithm see</span></td></tr>
<tr class="codeline" data-linenumber="1333"><td class="num" id="LN1333">1333</td><td class="line">  <span class='comment'>// https://devguide.python.org/garbage_collector/</span></td></tr>
<tr class="codeline" data-linenumber="1334"><td class="num" id="LN1334">1334</td><td class="line">  <span class='comment'>//</span></td></tr>
<tr class="codeline" data-linenumber="1335"><td class="num" id="LN1335">1335</td><td class="line">  <span class='comment'>// NB: if we don't hold an owning reference to the underlying Tensor, it is</span></td></tr>
<tr class="codeline" data-linenumber="1336"><td class="num" id="LN1336">1336</td><td class="line">  <span class='comment'>// possible that the underlying Tensor has already gone dead.  In that case,</span></td></tr>
<tr class="codeline" data-linenumber="1337"><td class="num" id="LN1337">1337</td><td class="line">  <span class='comment'>// it's not safe to access it.  But it's also safe to traverse, because if</span></td></tr>
<tr class="codeline" data-linenumber="1338"><td class="num" id="LN1338">1338</td><td class="line">  <span class='comment'>// the underlying Tensor *is* live, then root discovery will determine that</span></td></tr>
<tr class="codeline" data-linenumber="1339"><td class="num" id="LN1339">1339</td><td class="line">  <span class='comment'>// self is live, and nothing will get GC'ed anyway (resurrection cannot happen</span></td></tr>
<tr class="codeline" data-linenumber="1340"><td class="num" id="LN1340">1340</td><td class="line">  <span class='comment'>// if the C++ objects owns the PyObject)</span></td></tr>
<tr class="codeline" data-linenumber="1341"><td class="num" id="LN1341">1341</td><td class="line">  THPVariable* var = <span class='keyword'>reinterpret_cast</span>&lt;THPVariable*&gt;(self);</td></tr>
<tr class="codeline" data-linenumber="1342"><td class="num" id="LN1342">1342</td><td class="line">  <span class='keyword'>if</span> (!var-&gt;cdata.unsafeIsBorrowed()) {</td></tr>
<tr class="codeline" data-linenumber="1343"><td class="num" id="LN1343">1343</td><td class="line">    <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; tensor = THPVariable_Unpack(self);</td></tr>
<tr class="codeline" data-linenumber="1344"><td class="num" id="LN1344">1344</td><td class="line">    <span class='keyword'>if</span> (tensor.defined() &amp;&amp; tensor.use_count() &gt; 1)</td></tr>
<tr class="codeline" data-linenumber="1345"><td class="num" id="LN1345">1345</td><td class="line">      <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="1346"><td class="num" id="LN1346">1346</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1347"><td class="num" id="LN1347">1347</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1348"><td class="num" id="LN1348">1348</td><td class="line">  <span class='comment'>// Crappy version of subtype_traverse; same deal as</span></td></tr>
<tr class="codeline" data-linenumber="1349"><td class="num" id="LN1349">1349</td><td class="line">  <span class='comment'>// THPVariable_subclass_dealloc</span></td></tr>
<tr class="codeline" data-linenumber="1350"><td class="num" id="LN1350">1350</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1351"><td class="num" id="LN1351">1351</td><td class="line">  PyTypeObject* type = <span class='macro'>Py_TYPE(self)<span class='macro_popup'>(((PyObject*)(self))-&gt;ob_type)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1352"><td class="num" id="LN1352">1352</td><td class="line">  <span class='comment'>// Traverse slots until we get to base class THPVariableType</span></td></tr>
<tr class="codeline" data-linenumber="1353"><td class="num" id="LN1353">1353</td><td class="line">  {</td></tr>
<tr class="codeline" data-linenumber="1354"><td class="num" id="LN1354">1354</td><td class="line">    PyTypeObject* base = type;</td></tr>
<tr class="codeline" data-linenumber="1355"><td class="num" id="LN1355">1355</td><td class="line">    <span class='keyword'>while</span> (base != &amp;THPVariableType) {</td></tr>
<tr class="codeline" data-linenumber="1356"><td class="num" id="LN1356">1356</td><td class="line">      <span class='keyword'>if</span> (<span class='macro'>Py_SIZE(base)<span class='macro_popup'>(((PyVarObject*)(base))-&gt;ob_size)</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="1357"><td class="num" id="LN1357">1357</td><td class="line">        <span class='keyword'>int</span> err = traverse_slots(base, self, visit, arg);</td></tr>
<tr class="codeline" data-linenumber="1358"><td class="num" id="LN1358">1358</td><td class="line">        <span class='keyword'>if</span> (err)</td></tr>
<tr class="codeline" data-linenumber="1359"><td class="num" id="LN1359">1359</td><td class="line">          <span class='keyword'>return</span> err;</td></tr>
<tr class="codeline" data-linenumber="1360"><td class="num" id="LN1360">1360</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="1361"><td class="num" id="LN1361">1361</td><td class="line">      base = base-&gt;tp_base;</td></tr>
<tr class="codeline" data-linenumber="1362"><td class="num" id="LN1362">1362</td><td class="line">      <span class='macro'>TORCH_INTERNAL_ASSERT(base)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(base)), 0))) {<br> ::c10::detail::torchInternalAssertFail( __func__, "../torch/csrc/autograd/python_variable.cpp"<br>, static_cast&lt;uint32_t&gt;(1362), "base" "INTERNAL ASSERT FAILED at "<br> "\"../torch/csrc/autograd/python_variable.cpp\"" ":" "1362" ", please report a bug to PyTorch. "<br>, c10::str()); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1363"><td class="num" id="LN1363">1363</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1364"><td class="num" id="LN1364">1364</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1365"><td class="num" id="LN1365">1365</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1366"><td class="num" id="LN1366">1366</td><td class="line">  <span class='comment'>// All Python defined classes have __dict__</span></td></tr>
<tr class="codeline" data-linenumber="1367"><td class="num" id="LN1367">1367</td><td class="line">  <span class='keyword'>if</span> (<span class='macro'>C10_LIKELY(type-&gt;tp_dictoffset)<span class='macro_popup'>(__builtin_expect(static_cast&lt;bool&gt;(type-&gt;tp_dictoffset<br>), 1))</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="1368"><td class="num" id="LN1368">1368</td><td class="line">    PyObject** dictptr = _PyObject_GetDictPtr(self);</td></tr>
<tr class="codeline" data-linenumber="1369"><td class="num" id="LN1369">1369</td><td class="line">    <span class='keyword'>if</span> (dictptr &amp;&amp; *dictptr)</td></tr>
<tr class="codeline" data-linenumber="1370"><td class="num" id="LN1370">1370</td><td class="line">      <span class='macro'>Py_VISIT(*dictptr)<span class='macro_popup'>do { if (*dictptr) { int vret = visit(((PyObject*)(*dictptr))<br>, arg); if (vret) return vret; } } while (0)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1371"><td class="num" id="LN1371">1371</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1372"><td class="num" id="LN1372">1372</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1373"><td class="num" id="LN1373">1373</td><td class="line">  <span class='macro'>TORCH_INTERNAL_ASSERT(type-&gt;tp_flags &amp; Py_TPFLAGS_HEAPTYPE)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(type-&gt;tp_flags<br> &amp; (1UL &lt;&lt; 9))), 0))) { ::c10::detail::torchInternalAssertFail<br>( __func__, "../torch/csrc/autograd/python_variable.cpp", static_cast<br>&lt;uint32_t&gt;(1373), "type-&gt;tp_flags &amp; Py_TPFLAGS_HEAPTYPE"<br> "INTERNAL ASSERT FAILED at " "\"../torch/csrc/autograd/python_variable.cpp\""<br> ":" "1373" ", please report a bug to PyTorch. ", c10::str())<br>; }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1374"><td class="num" id="LN1374">1374</td><td class="line">  <span class='macro'>Py_VISIT(type)<span class='macro_popup'>do { if (type) { int vret = visit(((PyObject*)(type)), arg); if<br> (vret) return vret; } } while (0)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1375"><td class="num" id="LN1375">1375</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1376"><td class="num" id="LN1376">1376</td><td class="line">  <span class='comment'>// Finally traverse THPVariable special stuff</span></td></tr>
<tr class="codeline" data-linenumber="1377"><td class="num" id="LN1377">1377</td><td class="line">  <span class='macro'>Py_VISIT(var-&gt;backward_hooks)<span class='macro_popup'>do { if (var-&gt;backward_hooks) { int vret = visit(((PyObject<br>*)(var-&gt;backward_hooks)), arg); if (vret) return vret; } }<br> while (0)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1378"><td class="num" id="LN1378">1378</td><td class="line">  <span class='keyword'>if</span> (!var-&gt;cdata.unsafeIsBorrowed()) {</td></tr>
<tr class="codeline" data-linenumber="1379"><td class="num" id="LN1379">1379</td><td class="line">    <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; tensor = THPVariable_Unpack(var);</td></tr>
<tr class="codeline" data-linenumber="1380"><td class="num" id="LN1380">1380</td><td class="line">    <span class='keyword'>if</span> (tensor.defined()) {</td></tr>
<tr class="codeline" data-linenumber="1381"><td class="num" id="LN1381">1381</td><td class="line">      <span class='comment'>// WARNING: The grad_fn traversal logic is very subtle, if you change this,</span></td></tr>
<tr class="codeline" data-linenumber="1382"><td class="num" id="LN1382">1382</td><td class="line">      <span class='comment'>// be very careful not to re-introduce this bug:</span></td></tr>
<tr class="codeline" data-linenumber="1383"><td class="num" id="LN1383">1383</td><td class="line">      <span class='comment'>// https://gist.github.com/zou3519/7ac92b84dd7d206dcc6eae55fee8372c</span></td></tr>
<tr class="codeline" data-linenumber="1384"><td class="num" id="LN1384">1384</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1385"><td class="num" id="LN1385">1385</td><td class="line">      <span class='comment'>// We ensure that we follow NOTE [ PyObject Traversal ] he by checking that this</span></td></tr>
<tr class="codeline" data-linenumber="1386"><td class="num" id="LN1386">1386</td><td class="line">      <span class='comment'>// python object is the sole owner of the underlying Tensor and that this Tensor</span></td></tr>
<tr class="codeline" data-linenumber="1387"><td class="num" id="LN1387">1387</td><td class="line">      <span class='comment'>// is the sole owner of its grad_fn.</span></td></tr>
<tr class="codeline" data-linenumber="1388"><td class="num" id="LN1388">1388</td><td class="line">      <span class='comment'>// In this case, the only way to get a new reference to the grad_fn is by using</span></td></tr>
<tr class="codeline" data-linenumber="1389"><td class="num" id="LN1389">1389</td><td class="line">      <span class='comment'>// this python object, which requires the GIL to be accessed.</span></td></tr>
<tr class="codeline" data-linenumber="1390"><td class="num" id="LN1390">1390</td><td class="line">      <span class='comment'>// Note that this is only valid as long as user don't share non-owning references</span></td></tr>
<tr class="codeline" data-linenumber="1391"><td class="num" id="LN1391">1391</td><td class="line">      <span class='comment'>// across different threads (which is crazy and should never be done).</span></td></tr>
<tr class="codeline" data-linenumber="1392"><td class="num" id="LN1392">1392</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1393"><td class="num" id="LN1393">1393</td><td class="line">      <span class='keyword'>if</span> (tensor.use_count() == 1) {</td></tr>
<tr class="codeline" data-linenumber="1394"><td class="num" id="LN1394">1394</td><td class="line">        <span class='keyword'>auto</span> autograd_meta = torch::autograd::impl::get_autograd_meta(tensor);</td></tr>
<tr class="codeline" data-linenumber="1395"><td class="num" id="LN1395">1395</td><td class="line">        <span class='keyword'>if</span> (autograd_meta) {</td></tr>
<tr class="codeline" data-linenumber="1396"><td class="num" id="LN1396">1396</td><td class="line">          <span class='comment'>// Do NOT call grad_fn() here as that might trigger a recompute</span></td></tr>
<tr class="codeline" data-linenumber="1397"><td class="num" id="LN1397">1397</td><td class="line">          <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; grad_fn = autograd_meta-&gt;grad_fn_;</td></tr>
<tr class="codeline" data-linenumber="1398"><td class="num" id="LN1398">1398</td><td class="line">          <span class='keyword'>if</span> (grad_fn &amp;&amp; grad_fn.use_count() == 1) {</td></tr>
<tr class="codeline" data-linenumber="1399"><td class="num" id="LN1399">1399</td><td class="line">            <span class='comment'>// All Node can have a pyobj (stored in "pyobj_")</span></td></tr>
<tr class="codeline" data-linenumber="1400"><td class="num" id="LN1400">1400</td><td class="line">            <span class='macro'>Py_VISIT(grad_fn-&gt;pyobj())<span class='macro_popup'>do { if (grad_fn-&gt;pyobj()) { int vret = visit(((PyObject*)<br>(grad_fn-&gt;pyobj())), arg); if (vret) return vret; } } while<br> (0)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1401"><td class="num" id="LN1401">1401</td><td class="line">            <span class='comment'>// PyNode are special as they also have an "obj" field</span></td></tr>
<tr class="codeline" data-linenumber="1402"><td class="num" id="LN1402">1402</td><td class="line">            <span class='keyword'>if</span> (<span class='keyword'>auto</span> py_node_fn = <span class='keyword'>dynamic_cast</span>&lt;PyNode*&gt;(grad_fn.get())) {</td></tr>
<tr class="codeline" data-linenumber="1403"><td class="num" id="LN1403">1403</td><td class="line">              <span class='macro'>Py_VISIT(py_node_fn-&gt;obj)<span class='macro_popup'>do { if (py_node_fn-&gt;obj) { int vret = visit(((PyObject*)(<br>py_node_fn-&gt;obj)), arg); if (vret) return vret; } } while (<br>0)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1404"><td class="num" id="LN1404">1404</td><td class="line">            }</td></tr>
<tr class="codeline" data-linenumber="1405"><td class="num" id="LN1405">1405</td><td class="line">          }</td></tr>
<tr class="codeline" data-linenumber="1406"><td class="num" id="LN1406">1406</td><td class="line">        }</td></tr>
<tr class="codeline" data-linenumber="1407"><td class="num" id="LN1407">1407</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="1408"><td class="num" id="LN1408">1408</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1409"><td class="num" id="LN1409">1409</td><td class="line">      <span class='keyword'>for</span> (<span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; hook : torch::autograd::impl::hooks(tensor)) {</td></tr>
<tr class="codeline" data-linenumber="1410"><td class="num" id="LN1410">1410</td><td class="line">        <span class='keyword'>if</span> (<span class='keyword'>auto</span> pyhook = <span class='keyword'>dynamic_cast</span>&lt;PyFunctionPreHook*&gt;(hook.get())) {</td></tr>
<tr class="codeline" data-linenumber="1411"><td class="num" id="LN1411">1411</td><td class="line">          <span class='macro'>Py_VISIT(pyhook-&gt;dict)<span class='macro_popup'>do { if (pyhook-&gt;dict) { int vret = visit(((PyObject*)(pyhook<br>-&gt;dict)), arg); if (vret) return vret; } } while (0)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1412"><td class="num" id="LN1412">1412</td><td class="line">        }</td></tr>
<tr class="codeline" data-linenumber="1413"><td class="num" id="LN1413">1413</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="1414"><td class="num" id="LN1414">1414</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1415"><td class="num" id="LN1415">1415</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1416"><td class="num" id="LN1416">1416</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1417"><td class="num" id="LN1417">1417</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="1418"><td class="num" id="LN1418">1418</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1419"><td class="num" id="LN1419">1419</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1420"><td class="num" id="LN1420">1420</td><td class="line"><span class='keyword'>int</span> THPVariableMetaType_init(PyObject *cls, PyObject *args, PyObject *kwargs) {</td></tr>
<tr class="codeline" data-linenumber="1421"><td class="num" id="LN1421">1421</td><td class="line">  <span class='keyword'>if</span> (PyType_Type.tp_init(cls, args, kwargs) &lt; 0) {</td></tr>
<tr class="codeline" data-linenumber="1422"><td class="num" id="LN1422">1422</td><td class="line">    <span class='keyword'>return</span> -1;</td></tr>
<tr class="codeline" data-linenumber="1423"><td class="num" id="LN1423">1423</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1424"><td class="num" id="LN1424">1424</td><td class="line">  ((PyTypeObject*)cls)-&gt;tp_dealloc = (destructor)THPVariable_subclass_dealloc;</td></tr>
<tr class="codeline" data-linenumber="1425"><td class="num" id="LN1425">1425</td><td class="line">  ((PyTypeObject*)cls)-&gt;tp_traverse =</td></tr>
<tr class="codeline" data-linenumber="1426"><td class="num" id="LN1426">1426</td><td class="line">      (traverseproc)THPVariable_subclass_traverse;</td></tr>
<tr class="codeline" data-linenumber="1427"><td class="num" id="LN1427">1427</td><td class="line">  <span class='keyword'>return</span> 0;</td></tr>
<tr class="codeline" data-linenumber="1428"><td class="num" id="LN1428">1428</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1429"><td class="num" id="LN1429">1429</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1430"><td class="num" id="LN1430">1430</td><td class="line"><span class='keyword'>namespace</span> torch { <span class='keyword'>namespace</span> autograd {</td></tr>
<tr class="codeline" data-linenumber="1431"><td class="num" id="LN1431">1431</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1432"><td class="num" id="LN1432">1432</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="1433"><td class="num" id="LN1433">1433</td><td class="line"><span class='keyword'>extern</span> PyMethodDef variable_methods[];</td></tr>
<tr class="codeline" data-linenumber="1434"><td class="num" id="LN1434">1434</td><td class="line"><span class='keyword'>extern</span> <span class='keyword'>void</span> initTorchFunctions(PyObject *module);</td></tr>
<tr class="codeline" data-linenumber="1435"><td class="num" id="LN1435">1435</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1436"><td class="num" id="LN1436">1436</td><td class="line"><span class='keyword'>void</span> initTensorImplConversion(PyObject* module) {</td></tr>
<tr class="codeline" data-linenumber="1437"><td class="num" id="LN1437">1437</td><td class="line">  <span class='keyword'>auto</span> m = py::handle(module).cast&lt;py::module&gt;();</td></tr>
<tr class="codeline" data-linenumber="1438"><td class="num" id="LN1438">1438</td><td class="line">  m.def(<span class='string_literal'>"_wrap_tensor_impl"</span>, [](<span class='keyword'>void</span>* ptr) {</td></tr>
<tr class="codeline" data-linenumber="1439"><td class="num" id="LN1439">1439</td><td class="line">    <span class='keyword'>auto</span> p = c10::intrusive_ptr&lt;c10::TensorImpl, at::UndefinedTensorImpl&gt;::</td></tr>
<tr class="codeline" data-linenumber="1440"><td class="num" id="LN1440">1440</td><td class="line">        unsafe_reclaim_from_nonowning(<span class='keyword'>static_cast</span>&lt;c10::TensorImpl*&gt;(ptr));</td></tr>
<tr class="codeline" data-linenumber="1441"><td class="num" id="LN1441">1441</td><td class="line">    <span class='macro'>TORCH_CHECK(p.defined(), <span class='string_literal'>"Can't wrap undefined tensor"</span>)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(p.defined()))<br>, 0))) { ::c10::detail::torchCheckFail( __func__, "../torch/csrc/autograd/python_variable.cpp"<br>, static_cast&lt;uint32_t&gt;(1441), (::c10::detail::torchCheckMsgImpl<br>( "Expected " "p.defined()" " to be true, but got false.  " "(Could this error message be improved?  If so, "<br> "please report an enhancement request to PyTorch.)", "Can't wrap undefined tensor"<br>))); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1442"><td class="num" id="LN1442">1442</td><td class="line">    <span class='keyword'>auto</span> tensor = at::Tensor::wrap_tensor_impl(std::move(p));</td></tr>
<tr class="codeline" data-linenumber="1443"><td class="num" id="LN1443">1443</td><td class="line">    <span class='comment'>// NOLINTNEXTLINE(performance-move-const-arg)</span></td></tr>
<tr class="codeline" data-linenumber="1444"><td class="num" id="LN1444">1444</td><td class="line">    <span class='keyword'>return</span> py::cast(std::move(tensor));</td></tr>
<tr class="codeline" data-linenumber="1445"><td class="num" id="LN1445">1445</td><td class="line">  });</td></tr>
<tr class="codeline" data-linenumber="1446"><td class="num" id="LN1446">1446</td><td class="line">  <span class='comment'>// set on the module level to avoid mixing pybind and plain CPython extensions</span></td></tr>
<tr class="codeline" data-linenumber="1447"><td class="num" id="LN1447">1447</td><td class="line">  m.def(<span class='string_literal'>"_tensor_impl_raw_handle"</span>, [](torch::autograd::Variable* t) -&gt; <span class='keyword'>void</span>* {</td></tr>
<tr class="codeline" data-linenumber="1448"><td class="num" id="LN1448">1448</td><td class="line">    <span class='comment'>// We return a raw non-owning pointer here, we rely on surrounding</span></td></tr>
<tr class="codeline" data-linenumber="1449"><td class="num" id="LN1449">1449</td><td class="line">    <span class='comment'>// code to keep the original tensor alive</span></td></tr>
<tr class="codeline" data-linenumber="1450"><td class="num" id="LN1450">1450</td><td class="line">    <span class='keyword'>return</span> t-&gt;getIntrusivePtr().get();</td></tr>
<tr class="codeline" data-linenumber="1451"><td class="num" id="LN1451">1451</td><td class="line">  });</td></tr>
<tr class="codeline" data-linenumber="1452"><td class="num" id="LN1452">1452</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1453"><td class="num" id="LN1453">1453</td><td class="line">}}</td></tr>
<tr class="codeline" data-linenumber="1454"><td class="num" id="LN1454">1454</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1455"><td class="num" id="LN1455">1455</td><td class="line"><span class='keyword'>bool</span> THPVariable_initModule(PyObject *module)</td></tr>
<tr class="codeline" data-linenumber="1456"><td class="num" id="LN1456">1456</td><td class="line">{</td></tr>
<tr class="codeline" data-linenumber="1457"><td class="num" id="LN1457">1457</td><td class="line">  THPVariableMetaType.tp_base = &amp;PyType_Type;</td></tr>
<tr class="codeline" data-linenumber="1458"><td class="num" id="LN1458">1458</td><td class="line">  <span class='keyword'>if</span> (PyType_Ready(&amp;THPVariableMetaType) &lt; 0)</td></tr>
<tr class="codeline" data-linenumber="1459"><td class="num" id="LN1459">1459</td><td class="line">    <span class='keyword'>return</span> <span class='keyword'>false</span>;</td></tr>
<tr class="codeline" data-linenumber="1460"><td class="num" id="LN1460">1460</td><td class="line">  <span class='macro'>Py_INCREF(&amp;THPVariableMetaType)<span class='macro_popup'>_Py_INCREF(((PyObject*)(&amp;THPVariableMetaType)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1461"><td class="num" id="LN1461">1461</td><td class="line">  PyModule_AddObject(module, <span class='string_literal'>"_TensorMeta"</span>,   (PyObject *)&amp;THPVariableMetaType);</td></tr>
<tr class="codeline" data-linenumber="1462"><td class="num" id="LN1462">1462</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1463"><td class="num" id="LN1463">1463</td><td class="line">  <span class='keyword'>static</span> std::vector&lt;PyMethodDef&gt; methods;</td></tr>
<tr class="codeline" data-linenumber="1464"><td class="num" id="LN1464">1464</td><td class="line">  THPUtils_addPyMethodDefs(methods, torch::autograd::variable_methods);</td></tr>
<tr class="codeline" data-linenumber="1465"><td class="num" id="LN1465">1465</td><td class="line">  THPUtils_addPyMethodDefs(methods, extra_methods);</td></tr>
<tr class="codeline" data-linenumber="1466"><td class="num" id="LN1466">1466</td><td class="line">  THPVariableType.tp_methods = methods.data();</td></tr>
<tr class="codeline" data-linenumber="1467"><td class="num" id="LN1467">1467</td><td class="line">  <span class='keyword'>if</span> (PyType_Ready(&amp;THPVariableType) &lt; 0)</td></tr>
<tr class="codeline" data-linenumber="1468"><td class="num" id="LN1468">1468</td><td class="line">    <span class='keyword'>return</span> <span class='keyword'>false</span>;</td></tr>
<tr class="codeline" data-linenumber="1469"><td class="num" id="LN1469">1469</td><td class="line">  <span class='macro'>Py_INCREF(&amp;THPVariableType)<span class='macro_popup'>_Py_INCREF(((PyObject*)(&amp;THPVariableType)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1470"><td class="num" id="LN1470">1470</td><td class="line">  PyModule_AddObject(module, <span class='string_literal'>"_TensorBase"</span>,   (PyObject *)&amp;THPVariableType);</td></tr>
<tr class="codeline" data-linenumber="1471"><td class="num" id="LN1471">1471</td><td class="line">  torch::autograd::initTorchFunctions(module);</td></tr>
<tr class="codeline" data-linenumber="1472"><td class="num" id="LN1472">1472</td><td class="line">  torch::autograd::initTensorImplConversion(module);</td></tr>
<tr class="codeline" data-linenumber="1473"><td class="num" id="LN1473">1473</td><td class="line">  <span class='keyword'>return</span> <span class='keyword'>true</span>;</td></tr>
<tr class="codeline" data-linenumber="1474"><td class="num" id="LN1474">1474</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1475"><td class="num" id="LN1475">1475</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1476"><td class="num" id="LN1476">1476</td><td class="line"><span class='keyword'>namespace</span> {</td></tr>
<tr class="codeline" data-linenumber="1477"><td class="num" id="LN1477">1477</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1478"><td class="num" id="LN1478">1478</td><td class="line"><span class='keyword'>bool</span> isPythonTensor(<span class='keyword'>const</span> Tensor&amp; tensor) {</td></tr>
<tr class="codeline" data-linenumber="1479"><td class="num" id="LN1479">1479</td><td class="line">  <span class='keyword'>return</span> tensor.unsafeGetTensorImpl()-&gt;key_set().has(c10::DispatchKey::Python);</td></tr>
<tr class="codeline" data-linenumber="1480"><td class="num" id="LN1480">1480</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1481"><td class="num" id="LN1481">1481</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1482"><td class="num" id="LN1482">1482</td><td class="line"><span class='keyword'>void</span> concrete_dispatch_fn(<span class='keyword'>const</span> c10::impl::PyInterpreter*, <span class='keyword'>const</span> c10::OperatorHandle&amp; op, torch::jit::Stack* stack) {</td></tr>
<tr class="codeline" data-linenumber="1483"><td class="num" id="LN1483">1483</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; schema = op.schema();</td></tr>
<tr class="codeline" data-linenumber="1484"><td class="num" id="LN1484">1484</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span> num_returns = schema.returns().size();</td></tr>
<tr class="codeline" data-linenumber="1485"><td class="num" id="LN1485">1485</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1486"><td class="num" id="LN1486">1486</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span> num_arguments = schema.arguments().size();</td></tr>
<tr class="codeline" data-linenumber="1487"><td class="num" id="LN1487">1487</td><td class="line">  <span class='keyword'>auto</span> arguments = torch::jit::pop(*stack, num_arguments);</td></tr>
<tr class="codeline" data-linenumber="1488"><td class="num" id="LN1488">1488</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1489"><td class="num" id="LN1489">1489</td><td class="line">  <span class='comment'>// Parse the name into namespace and name (no overload_name)</span></td></tr>
<tr class="codeline" data-linenumber="1490"><td class="num" id="LN1490">1490</td><td class="line">  <span class='comment'>// TODO: put this into the library</span></td></tr>
<tr class="codeline" data-linenumber="1491"><td class="num" id="LN1491">1491</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; qualified_name = op.operator_name().name;</td></tr>
<tr class="codeline" data-linenumber="1492"><td class="num" id="LN1492">1492</td><td class="line">  <span class='keyword'>auto</span> pos = qualified_name.find(<span class='string_literal'>"::"</span>);</td></tr>
<tr class="codeline" data-linenumber="1493"><td class="num" id="LN1493">1493</td><td class="line">  <span class='macro'>TORCH_INTERNAL_ASSERT(pos != std::string::npos, qualified_name)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(pos != std::string<br>::npos)), 0))) { ::c10::detail::torchInternalAssertFail( __func__<br>, "../torch/csrc/autograd/python_variable.cpp", static_cast&lt;<br>uint32_t&gt;(1493), "pos != std::string::npos" "INTERNAL ASSERT FAILED at "<br> "\"../torch/csrc/autograd/python_variable.cpp\"" ":" "1493" ", please report a bug to PyTorch. "<br>, c10::str(qualified_name)); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1494"><td class="num" id="LN1494">1494</td><td class="line">  <span class='comment'>// Make me some null terminated strings</span></td></tr>
<tr class="codeline" data-linenumber="1495"><td class="num" id="LN1495">1495</td><td class="line">  std::string ns_str = qualified_name.substr(0, pos);</td></tr>
<tr class="codeline" data-linenumber="1496"><td class="num" id="LN1496">1496</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>char</span>* ns = ns_str.c_str();</td></tr>
<tr class="codeline" data-linenumber="1497"><td class="num" id="LN1497">1497</td><td class="line">  <span class='keyword'>const</span> <span class='keyword'>char</span>* func_name = qualified_name.c_str() + pos + strlen(<span class='string_literal'>"::"</span>);</td></tr>
<tr class="codeline" data-linenumber="1498"><td class="num" id="LN1498">1498</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1499"><td class="num" id="LN1499">1499</td><td class="line">  <span class='comment'>// The plan: convert all the arguments back into PyObjects,</span></td></tr>
<tr class="codeline" data-linenumber="1500"><td class="num" id="LN1500">1500</td><td class="line">  <span class='comment'>// extracting out the tensor handles, then call</span></td></tr>
<tr class="codeline" data-linenumber="1501"><td class="num" id="LN1501">1501</td><td class="line">  <span class='comment'>// handle_torch_function_no_python_arg_parser</span></td></tr>
<tr class="codeline" data-linenumber="1502"><td class="num" id="LN1502">1502</td><td class="line">  <span class='comment'>// NB: at the point arguments are pushed to the stack, ALL defaults</span></td></tr>
<tr class="codeline" data-linenumber="1503"><td class="num" id="LN1503">1503</td><td class="line">  <span class='comment'>// are already present</span></td></tr>
<tr class="codeline" data-linenumber="1504"><td class="num" id="LN1504">1504</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1505"><td class="num" id="LN1505">1505</td><td class="line">  py::gil_scoped_acquire g;</td></tr>
<tr class="codeline" data-linenumber="1506"><td class="num" id="LN1506">1506</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1507"><td class="num" id="LN1507">1507</td><td class="line">  std::vector&lt;py::handle&gt; overloaded_args;</td></tr>
<tr class="codeline" data-linenumber="1508"><td class="num" id="LN1508">1508</td><td class="line">  <span class='keyword'>auto</span> args = py::reinterpret_steal&lt;py::object&gt;(PyTuple_New(num_arguments));</td></tr>
<tr class="codeline" data-linenumber="1509"><td class="num" id="LN1509">1509</td><td class="line">  <span class='comment'>// TODO: actually populate kwargs sometimes?  At the moment, every argument</span></td></tr>
<tr class="codeline" data-linenumber="1510"><td class="num" id="LN1510">1510</td><td class="line">  <span class='comment'>// just gets passed positionally</span></td></tr>
<tr class="codeline" data-linenumber="1511"><td class="num" id="LN1511">1511</td><td class="line">  py::dict kwargs;</td></tr>
<tr class="codeline" data-linenumber="1512"><td class="num" id="LN1512">1512</td><td class="line">  <span class='comment'>// For now, overloads get coalesced.  Might be easier for users if they get</span></td></tr>
<tr class="codeline" data-linenumber="1513"><td class="num" id="LN1513">1513</td><td class="line">  <span class='comment'>// overload resolution but is more complicated (need to expose separate</span></td></tr>
<tr class="codeline" data-linenumber="1514"><td class="num" id="LN1514">1514</td><td class="line">  <span class='comment'>// functions per overload)</span></td></tr>
<tr class="codeline" data-linenumber="1515"><td class="num" id="LN1515">1515</td><td class="line">  py::handle torch_api_function = py::module::import(<span class='string_literal'>"torch"</span>).attr(<span class='string_literal'>"ops"</span>).attr(ns).attr(func_name);</td></tr>
<tr class="codeline" data-linenumber="1516"><td class="num" id="LN1516">1516</td><td class="line">  std::string module_name_str = <span class='string_literal'>"torch.ops."</span> + ns_str;</td></tr>
<tr class="codeline" data-linenumber="1517"><td class="num" id="LN1517">1517</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1518"><td class="num" id="LN1518">1518</td><td class="line">  <span class='keyword'>for</span> (int64_t idx = 0; idx &lt; arguments.size(); idx++) {</td></tr>
<tr class="codeline" data-linenumber="1519"><td class="num" id="LN1519">1519</td><td class="line">    <span class='keyword'>auto</span>&amp; ivalue = arguments[idx];</td></tr>
<tr class="codeline" data-linenumber="1520"><td class="num" id="LN1520">1520</td><td class="line">    <span class='comment'>// Search for Tensors (as they may have the torch functions we need)</span></td></tr>
<tr class="codeline" data-linenumber="1521"><td class="num" id="LN1521">1521</td><td class="line">    <span class='keyword'>if</span> (ivalue.isTensor()) {</td></tr>
<tr class="codeline" data-linenumber="1522"><td class="num" id="LN1522">1522</td><td class="line">      <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; tensor = ivalue.toTensor();</td></tr>
<tr class="codeline" data-linenumber="1523"><td class="num" id="LN1523">1523</td><td class="line">      <span class='keyword'>if</span> (isPythonTensor(tensor)) {</td></tr>
<tr class="codeline" data-linenumber="1524"><td class="num" id="LN1524">1524</td><td class="line">        overloaded_args.emplace_back(py::cast(tensor));</td></tr>
<tr class="codeline" data-linenumber="1525"><td class="num" id="LN1525">1525</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="1526"><td class="num" id="LN1526">1526</td><td class="line">    } <span class='keyword'>else</span> <span class='keyword'>if</span> (ivalue.isList()) {</td></tr>
<tr class="codeline" data-linenumber="1527"><td class="num" id="LN1527">1527</td><td class="line">      <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; list = ivalue.toListRef();</td></tr>
<tr class="codeline" data-linenumber="1528"><td class="num" id="LN1528">1528</td><td class="line">      <span class='keyword'>for</span> (int64_t jdx = 0; jdx &lt; list.size(); jdx++) {</td></tr>
<tr class="codeline" data-linenumber="1529"><td class="num" id="LN1529">1529</td><td class="line">        <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; nv = list[jdx];</td></tr>
<tr class="codeline" data-linenumber="1530"><td class="num" id="LN1530">1530</td><td class="line">        <span class='keyword'>if</span> (nv.isTensor()) {</td></tr>
<tr class="codeline" data-linenumber="1531"><td class="num" id="LN1531">1531</td><td class="line">          <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; tensor = nv.toTensor();</td></tr>
<tr class="codeline" data-linenumber="1532"><td class="num" id="LN1532">1532</td><td class="line">          <span class='keyword'>if</span> (isPythonTensor(tensor)) {</td></tr>
<tr class="codeline" data-linenumber="1533"><td class="num" id="LN1533">1533</td><td class="line">            overloaded_args.emplace_back(py::cast(tensor));</td></tr>
<tr class="codeline" data-linenumber="1534"><td class="num" id="LN1534">1534</td><td class="line">          }</td></tr>
<tr class="codeline" data-linenumber="1535"><td class="num" id="LN1535">1535</td><td class="line">        }</td></tr>
<tr class="codeline" data-linenumber="1536"><td class="num" id="LN1536">1536</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="1537"><td class="num" id="LN1537">1537</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1538"><td class="num" id="LN1538">1538</td><td class="line">    <span class='macro'>PyTuple_SET_ITEM(args.ptr(), idx, torch::jit::toPyObject(std::move(ivalue)).release().ptr())<span class='macro_popup'>PyTuple_SetItem(args.ptr(), idx, torch::jit::toPyObject(std::<br>move(ivalue)).release().ptr())</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1539"><td class="num" id="LN1539">1539</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1540"><td class="num" id="LN1540">1540</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1541"><td class="num" id="LN1541">1541</td><td class="line">  <span class='keyword'>auto</span> out = py::reinterpret_steal&lt;py::object&gt;(handle_torch_function_no_python_arg_parser(</td></tr>
<tr class="codeline" data-linenumber="1542"><td class="num" id="LN1542">1542</td><td class="line">    overloaded_args,</td></tr>
<tr class="codeline" data-linenumber="1543"><td class="num" id="LN1543">1543</td><td class="line">    args.ptr(),</td></tr>
<tr class="codeline" data-linenumber="1544"><td class="num" id="LN1544">1544</td><td class="line">    kwargs.ptr(),</td></tr>
<tr class="codeline" data-linenumber="1545"><td class="num" id="LN1545">1545</td><td class="line">    func_name,</td></tr>
<tr class="codeline" data-linenumber="1546"><td class="num" id="LN1546">1546</td><td class="line">    torch_api_function.ptr(),</td></tr>
<tr class="codeline" data-linenumber="1547"><td class="num" id="LN1547">1547</td><td class="line">    module_name_str.c_str(),</td></tr>
<tr class="codeline" data-linenumber="1548"><td class="num" id="LN1548">1548</td><td class="line">    <span class='string_literal'>"__torch_dispatch__"</span></td></tr>
<tr class="codeline" data-linenumber="1549"><td class="num" id="LN1549">1549</td><td class="line">  ));</td></tr>
<tr class="codeline" data-linenumber="1550"><td class="num" id="LN1550">1550</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1551"><td class="num" id="LN1551">1551</td><td class="line">  <span class='keyword'>if</span> (op.schema().returns().size() == 1) {</td></tr>
<tr class="codeline" data-linenumber="1552"><td class="num" id="LN1552">1552</td><td class="line">    torch::jit::push(stack, torch::jit::toIValue(out.ptr(), op.schema().returns()[0].type()));</td></tr>
<tr class="codeline" data-linenumber="1553"><td class="num" id="LN1553">1553</td><td class="line">  } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="1554"><td class="num" id="LN1554">1554</td><td class="line">    <span class='keyword'>auto</span> outs = py::cast&lt;py::sequence&gt;(out);</td></tr>
<tr class="codeline" data-linenumber="1555"><td class="num" id="LN1555">1555</td><td class="line">    <span class='keyword'>for</span> (<span class='keyword'>unsigned</span> idx = 0; idx &lt; outs.size(); idx++) {</td></tr>
<tr class="codeline" data-linenumber="1556"><td class="num" id="LN1556">1556</td><td class="line">      torch::jit::push(stack, torch::jit::toIValue(outs[idx].ptr(), op.schema().returns()[idx].type()));</td></tr>
<tr class="codeline" data-linenumber="1557"><td class="num" id="LN1557">1557</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="1558"><td class="num" id="LN1558">1558</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="1559"><td class="num" id="LN1559">1559</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1560"><td class="num" id="LN1560">1560</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1561"><td class="num" id="LN1561">1561</td><td class="line">c10::intrusive_ptr&lt;TensorImpl&gt; concrete_detach_fn(<span class='keyword'>const</span> c10::impl::PyInterpreter*, <span class='keyword'>const</span> c10::TensorImpl* self) {</td></tr>
<tr class="codeline" data-linenumber="1562"><td class="num" id="LN1562">1562</td><td class="line">  pybind11::gil_scoped_acquire gil;</td></tr>
<tr class="codeline" data-linenumber="1563"><td class="num" id="LN1563">1563</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1564"><td class="num" id="LN1564">1564</td><td class="line">  <span class='comment'>// Setup the arguments expected for the detach call</span></td></tr>
<tr class="codeline" data-linenumber="1565"><td class="num" id="LN1565">1565</td><td class="line">  std::vector&lt;py::handle&gt; overloaded_args;</td></tr>
<tr class="codeline" data-linenumber="1566"><td class="num" id="LN1566">1566</td><td class="line">  <span class='comment'>// TODO: there should be a shorter way to spell this</span></td></tr>
<tr class="codeline" data-linenumber="1567"><td class="num" id="LN1567">1567</td><td class="line">  <span class='comment'>// TODO: fix the constness of target</span></td></tr>
<tr class="codeline" data-linenumber="1568"><td class="num" id="LN1568">1568</td><td class="line">  Tensor self_t = Tensor(c10::intrusive_ptr&lt;c10::TensorImpl, c10::UndefinedTensorImpl&gt;::unsafe_reclaim_from_nonowning(<span class='keyword'>const_cast</span>&lt;c10::TensorImpl*&gt;(self)));</td></tr>
<tr class="codeline" data-linenumber="1569"><td class="num" id="LN1569">1569</td><td class="line">  <span class='keyword'>auto</span> self_p = py::reinterpret_steal&lt;py::object&gt;(THPVariable_Wrap(self_t));</td></tr>
<tr class="codeline" data-linenumber="1570"><td class="num" id="LN1570">1570</td><td class="line">  overloaded_args.emplace_back(self_p);</td></tr>
<tr class="codeline" data-linenumber="1571"><td class="num" id="LN1571">1571</td><td class="line">  <span class='keyword'>auto</span> args = py::reinterpret_steal&lt;py::object&gt;(PyTuple_New(1));</td></tr>
<tr class="codeline" data-linenumber="1572"><td class="num" id="LN1572">1572</td><td class="line">  <span class='macro'>PyTuple_SET_ITEM(args.ptr(), 0, self_p.release().ptr())<span class='macro_popup'>PyTuple_SetItem(args.ptr(), 0, self_p.release().ptr())</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1573"><td class="num" id="LN1573">1573</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1574"><td class="num" id="LN1574">1574</td><td class="line">  py::dict kwargs;</td></tr>
<tr class="codeline" data-linenumber="1575"><td class="num" id="LN1575">1575</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1576"><td class="num" id="LN1576">1576</td><td class="line">  <span class='keyword'>auto</span> out = py::reinterpret_steal&lt;py::object&gt;(handle_torch_function_no_python_arg_parser(</td></tr>
<tr class="codeline" data-linenumber="1577"><td class="num" id="LN1577">1577</td><td class="line">    overloaded_args,</td></tr>
<tr class="codeline" data-linenumber="1578"><td class="num" id="LN1578">1578</td><td class="line">    args.ptr(),</td></tr>
<tr class="codeline" data-linenumber="1579"><td class="num" id="LN1579">1579</td><td class="line">    kwargs.ptr(),</td></tr>
<tr class="codeline" data-linenumber="1580"><td class="num" id="LN1580">1580</td><td class="line">    <span class='string_literal'>"detach"</span>,</td></tr>
<tr class="codeline" data-linenumber="1581"><td class="num" id="LN1581">1581</td><td class="line">    py::module::import(<span class='string_literal'>"torch"</span>).attr(<span class='string_literal'>"ops"</span>).attr(<span class='string_literal'>"aten"</span>).attr(<span class='string_literal'>"detach"</span>).ptr(),</td></tr>
<tr class="codeline" data-linenumber="1582"><td class="num" id="LN1582">1582</td><td class="line">    <span class='string_literal'>"torch.ops.aten"</span>,</td></tr>
<tr class="codeline" data-linenumber="1583"><td class="num" id="LN1583">1583</td><td class="line">    <span class='string_literal'>"__torch_dispatch__"</span></td></tr>
<tr class="codeline" data-linenumber="1584"><td class="num" id="LN1584">1584</td><td class="line">  ));</td></tr>
<tr class="codeline" data-linenumber="1585"><td class="num" id="LN1585">1585</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1586"><td class="num" id="LN1586">1586</td><td class="line">  <span class='macro'>TORCH_CHECK(THPVariable_Check(out.ptr()), <span class='string_literal'>"detach returned invalid type "</span>, py::detail::get_fully_qualified_tp_name(Py_TYPE(out.ptr())), <span class='string_literal'>", expected Tensor"</span>)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(THPVariable_Check<br>(out.ptr()))), 0))) { ::c10::detail::torchCheckFail( __func__<br>, "../torch/csrc/autograd/python_variable.cpp", static_cast&lt;<br>uint32_t&gt;(1586), (::c10::detail::torchCheckMsgImpl( "Expected "<br> "THPVariable_Check(out.ptr())" " to be true, but got false.  "<br> "(Could this error message be improved?  If so, " "please report an enhancement request to PyTorch.)"<br>, "detach returned invalid type ", py::detail::get_fully_qualified_tp_name<br>((((PyObject*)(out.ptr()))-&gt;ob_type)), ", expected Tensor"<br>))); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="1587"><td class="num" id="LN1587">1587</td><td class="line">  <span class='keyword'>const</span> Tensor&amp; res_t = THPVariable_Unpack(out.ptr());</td></tr>
<tr class="codeline" data-linenumber="1588"><td class="num" id="LN1588">1588</td><td class="line">  <span class='keyword'>return</span> res_t.getIntrusivePtr();</td></tr>
<tr class="codeline" data-linenumber="1589"><td class="num" id="LN1589">1589</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="1590"><td class="num" id="LN1590">1590</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="1591"><td class="num" id="LN1591">1591</td><td class="line">} <span class='comment'>// anonymous namespace</span></td></tr>
</table><hr class=divider>
<div id=File193750>
<div class=FileNav><a href="#File1">&#x2190;</a></div><h4 class=FileName>/home/alan/temp/cpython/Python-3.8.5/Doc/build/models/PyTuple_New.model</h4>
</div>
<table class="code" data-fileid="193750">
<tr class="codeline" data-linenumber="1"><td class="num" id="LN1">1</td><td class="line"><span class='directive'>#ifndef PyTuple_New</span></td></tr>
<tr class="codeline" data-linenumber="2"><td class="num" id="LN2">2</td><td class="line"><span class='keyword'>struct</span> _object;</td></tr>
<tr class="codeline" data-linenumber="3"><td class="num" id="LN3">3</td><td class="line"><span class='keyword'>typedef</span> <span class='keyword'>struct</span> _object PyObject;</td></tr>
<tr class="codeline" data-linenumber="4"><td class="num" id="LN4">4</td><td class="line">PyObject* clang_analyzer_PyObject_New_Reference();</td></tr>
<tr class="codeline" data-linenumber="5"><td class="num" id="LN5">5</td><td class="line">PyObject* PyTuple_New(Py_ssize_t len) {</td></tr>
<tr class="codeline" data-linenumber="6"><td class="num" id="LN6">6</td><td class="line">  <span class='keyword'>return</span> <span class="mrange">clang_analyzer_PyObject_New_Reference()</span>;</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path4" class="msg msgEvent" style="margin-left:10ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">4</div></td><td><div class="PathNav"><a href="#Path3" title="Previous event (3)">&#x2190;</a></div></td><td>Setting reference count to 1</td><td><div class="PathNav"><a href="#Path5" title="Next event (5)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="7"><td class="num" id="LN7">7</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="8"><td class="num" id="LN8">8</td><td class="line"><span class='directive'>#else</span></td></tr>
<tr class="codeline" data-linenumber="9"><td class="num" id="LN9">9</td><td class="line"><span class='directive'>#warning "API PyTuple_New is defined as a macro."</span></td></tr>
<tr class="codeline" data-linenumber="10"><td class="num" id="LN10">10</td><td class="line"><span class='directive'>#endif</span></td></tr></table></body></html>
