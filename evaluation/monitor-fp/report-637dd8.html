<!doctype html>
<html>
<head>
<title>../torch/csrc/tensor/python_tensor.cpp</title>

<style type="text/css">
body { color:#000000; background-color:#ffffff }
body { font-family:Helvetica, sans-serif; font-size:10pt }
h1 { font-size:14pt }
.FileName { margin-top: 5px; margin-bottom: 5px; display: inline; }
.FileNav { margin-left: 5px; margin-right: 5px; display: inline; }
.FileNav a { text-decoration:none; font-size: larger; }
.divider { margin-top: 30px; margin-bottom: 30px; height: 15px; }
.divider { background-color: gray; }
.code { border-collapse:collapse; width:100%; }
.code { font-family: "Monospace", monospace; font-size:10pt }
.code { line-height: 1.2em }
.comment { color: green; font-style: oblique }
.keyword { color: blue }
.string_literal { color: red }
.directive { color: darkmagenta }

/* Macros and variables could have pop-up notes hidden by default.
  - Macro pop-up:    expansion of the macro
  - Variable pop-up: value (table) of the variable */
.macro_popup, .variable_popup { display: none; }

/* Pop-up appears on mouse-hover event. */
.macro:hover .macro_popup, .variable:hover .variable_popup {
  display: block;
  padding: 2px;
  -webkit-border-radius:5px;
  -webkit-box-shadow:1px 1px 7px #000;
  border-radius:5px;
  box-shadow:1px 1px 7px #000;
  position: absolute;
  top: -1em;
  left:10em;
  z-index: 1
}

.macro_popup {
  border: 2px solid red;
  background-color:#FFF0F0;
  font-weight: normal;
}

.variable_popup {
  border: 2px solid blue;
  background-color:#F0F0FF;
  font-weight: bold;
  font-family: Helvetica, sans-serif;
  font-size: 9pt;
}

/* Pop-up notes needs a relative position as a base where they pops up. */
.macro, .variable {
  background-color: PaleGoldenRod;
  position: relative;
}
.macro { color: DarkMagenta; }

#tooltiphint {
  position: fixed;
  width: 50em;
  margin-left: -25em;
  left: 50%;
  padding: 10px;
  border: 1px solid #b0b0b0;
  border-radius: 2px;
  box-shadow: 1px 1px 7px black;
  background-color: #c0c0c0;
  z-index: 2;
}

.num { width:2.5em; padding-right:2ex; background-color:#eeeeee }
.num { text-align:right; font-size:8pt }
.num { color:#444444 }
.line { padding-left: 1ex; border-left: 3px solid #ccc }
.line { white-space: pre }
.msg { -webkit-box-shadow:1px 1px 7px #000 }
.msg { box-shadow:1px 1px 7px #000 }
.msg { -webkit-border-radius:5px }
.msg { border-radius:5px }
.msg { font-family:Helvetica, sans-serif; font-size:8pt }
.msg { float:left }
.msg { padding:0.25em 1ex 0.25em 1ex }
.msg { margin-top:10px; margin-bottom:10px }
.msg { font-weight:bold }
.msg { max-width:60em; word-wrap: break-word; white-space: pre-wrap }
.msgT { padding:0x; spacing:0x }
.msgEvent { background-color:#fff8b4; color:#000000 }
.msgControl { background-color:#bbbbbb; color:#000000 }
.msgNote { background-color:#ddeeff; color:#000000 }
.mrange { background-color:#dfddf3 }
.mrange { border-bottom:1px solid #6F9DBE }
.PathIndex { font-weight: bold; padding:0px 5px; margin-right:5px; }
.PathIndex { -webkit-border-radius:8px }
.PathIndex { border-radius:8px }
.PathIndexEvent { background-color:#bfba87 }
.PathIndexControl { background-color:#8c8c8c }
.PathIndexPopUp { background-color: #879abc; }
.PathNav a { text-decoration:none; font-size: larger }
.CodeInsertionHint { font-weight: bold; background-color: #10dd10 }
.CodeRemovalHint { background-color:#de1010 }
.CodeRemovalHint { border-bottom:1px solid #6F9DBE }
.selected{ background-color:orange !important; }

table.simpletable {
  padding: 5px;
  font-size:12pt;
  margin:20px;
  border-collapse: collapse; border-spacing: 0px;
}
td.rowname {
  text-align: right;
  vertical-align: top;
  font-weight: bold;
  color:#444444;
  padding-right:2ex;
}

/* Hidden text. */
input.spoilerhider + label {
  cursor: pointer;
  text-decoration: underline;
  display: block;
}
input.spoilerhider {
 display: none;
}
input.spoilerhider ~ .spoiler {
  overflow: hidden;
  margin: 10px auto 0;
  height: 0;
  opacity: 0;
}
input.spoilerhider:checked + label + .spoiler{
  height: auto;
  opacity: 1;
}
</style>
</head>
<body>
<!-- BUGDESC PyObject ownership leak with reference count of 1 -->

<!-- BUGTYPE Non-Zero Dead Object -->

<!-- BUGCATEGORY Python Memory Error -->

<!-- BUGFILE /tmp/pyrefcon/pytorch/build/../torch/csrc/tensor/python_tensor.cpp -->

<!-- FILENAME python_tensor.cpp -->

<!-- FUNCTIONNAME py_bind_tensor_types -->

<!-- ISSUEHASHCONTENTOFLINEINCONTEXT e0bb8090b94fd2bf1ff7eefe91ff7088 -->

<!-- BUGLINE 359 -->

<!-- BUGCOLUMN 38 -->

<!-- BUGPATHLENGTH 11 -->

<!-- BUGMETAEND -->
<!-- REPORTHEADER -->
<h3>Bug Summary</h3>
<table class="simpletable">
<tr><td class="rowname">File:</td><td>build/../torch/csrc/tensor/python_tensor.cpp</td></tr>
<tr><td class="rowname">Warning:</td><td><a href="#EndPath">line 359, column 38</a><br />PyObject ownership leak with reference count of 1</td></tr>

</table>
<!-- REPORTSUMMARYEXTRA -->
<h3>Annotated Source Code</h3>
<p>Press <a href="#" onclick="toggleHelp(); return false;">'?'</a>
   to see keyboard shortcuts</p>
<input type="checkbox" class="spoilerhider" id="showinvocation" />
<label for="showinvocation" >Show analyzer invocation</label>
<div class="spoiler">clang -cc1 -cc1 -triple x86_64-unknown-linux-gnu -analyze -disable-free -disable-llvm-verifier -discard-value-names -main-file-name python_tensor.cpp -analyzer-store=region -analyzer-opt-analyze-nested-blocks -analyzer-checker=core -analyzer-checker=apiModeling -analyzer-checker=unix -analyzer-checker=deadcode -analyzer-checker=cplusplus -analyzer-checker=security.insecureAPI.UncheckedReturn -analyzer-checker=security.insecureAPI.getpw -analyzer-checker=security.insecureAPI.gets -analyzer-checker=security.insecureAPI.mktemp -analyzer-checker=security.insecureAPI.mkstemp -analyzer-checker=security.insecureAPI.vfork -analyzer-checker=nullability.NullPassedToNonnull -analyzer-checker=nullability.NullReturnedFromNonnull -analyzer-output plist -w -analyzer-output=html -analyzer-checker=python -analyzer-disable-checker=deadcode -analyzer-config prune-paths=true,suppress-c++-stdlib=true,suppress-inlined-defensive-checks=false,suppress-null-return-paths=false,crosscheck-with-z3=true,model-path=/opt/pyrefcon/lib/pyrefcon/models/models -analyzer-config experimental-enable-naive-ctu-analysis=true,ctu-dir=/tmp/pyrefcon/pytorch/csa-scan,ctu-index-name=/tmp/pyrefcon/pytorch/csa-scan/externalDefMap.txt,ctu-invocation-list=/tmp/pyrefcon/pytorch/csa-scan/invocations.yaml,display-ctu-progress=false -setup-static-analyzer -analyzer-config-compatibility-mode=true -mrelocation-model pic -pic-level 2 -fhalf-no-semantic-interposition -mframe-pointer=none -relaxed-aliasing -fno-rounding-math -ffp-exception-behavior=ignore -mconstructor-aliases -munwind-tables -target-cpu x86-64 -tune-cpu generic -debugger-tuning=gdb -fcoverage-compilation-dir=/tmp/pyrefcon/pytorch/build -resource-dir /opt/pyrefcon/lib/clang/13.0.0 -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem ../third_party/protobuf/src -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party/XNNPACK/include -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /opt/pyrefcon/lib/pyrefcon/models/python3.8 -isystem /usr/lib/python3/dist-packages/numpy/core/include -isystem ../cmake/../third_party/pybind11/include -isystem /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -isystem /usr/lib/x86_64-linux-gnu/openmpi/include -isystem ../third_party/ideep/mkl-dnn/include -isystem ../third_party/ideep/include -D BUILDING_TESTS -D FMT_HEADER_ONLY=1 -D HAVE_MALLOC_USABLE_SIZE=1 -D HAVE_MMAP=1 -D HAVE_SHM_OPEN=1 -D HAVE_SHM_UNLINK=1 -D MINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -D ONNXIFI_ENABLE_EXT=1 -D ONNX_ML=1 -D ONNX_NAMESPACE=onnx_torch -D THP_BUILD_MAIN_LIB -D USE_C10D -D USE_C10D_GLOO -D USE_C10D_MPI -D USE_DISTRIBUTED -D USE_EXTERNAL_MZCRC -D USE_NUMPY -D USE_RPC -D USE_TENSORPIPE -D USE_VALGRIND -D _FILE_OFFSET_BITS=64 -D torch_python_EXPORTS -I aten/src -I ../aten/src -I . -I ../ -I ../cmake/../third_party/benchmark/include -I caffe2/contrib/aten -I ../third_party/onnx -I third_party/onnx -I ../third_party/foxi -I third_party/foxi -I ../torch/.. -I ../torch/../aten/src -I ../torch/../aten/src/TH -I caffe2/aten/src -I third_party -I ../torch/../third_party/valgrind-headers -I ../torch/../third_party/gloo -I ../torch/../third_party/onnx -I ../torch/csrc -I ../torch/csrc/api/include -I ../torch/lib -I ../torch/lib/libshm -I ../torch/csrc/distributed -I ../torch/csrc/api -I ../c10/.. -I third_party/ideep/mkl-dnn/include -I ../third_party/ideep/mkl-dnn/src/../include -I ../torch/lib/libshm/../../../torch/lib -I ../third_party/fmt/include -D USE_PTHREADPOOL -D NDEBUG -D USE_KINETO -D LIBKINETO_NOCUPTI -D USE_FBGEMM -D USE_QNNPACK -D USE_PYTORCH_QNNPACK -D USE_XNNPACK -D SYMBOLICATE_MOBILE_DEBUG_HANDLE -D HAVE_AVX_CPU_DEFINITION -D HAVE_AVX2_CPU_DEFINITION -D NDEBUG -D NDEBUG -D CAFFE2_USE_GLOO -D HAVE_GCC_GET_CPUID -D USE_AVX -D USE_AVX2 -D TH_HAVE_THREAD -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10 -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/x86_64-linux-gnu/c++/10 -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/backward -internal-isystem /opt/pyrefcon/lib/clang/13.0.0/include -internal-isystem /usr/local/include -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../x86_64-linux-gnu/include -internal-externc-isystem /usr/include/x86_64-linux-gnu -internal-externc-isystem /include -internal-externc-isystem /usr/include -O3 -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -Wno-unused-but-set-variable -Wno-maybe-uninitialized -Werror=format -Werror=cast-function-type -Wno-stringop-overflow -Wno-write-strings -Wno-strict-aliasing -w -std=gnu++14 -fdeprecated-macro -fdebug-compilation-dir=/tmp/pyrefcon/pytorch/build -ferror-limit 19 -fvisibility-inlines-hidden -fopenmp -fopenmp-cuda-parallel-target-regions -pthread -fgnuc-version=4.2.1 -fcxx-exceptions -fexceptions -faligned-allocation -fcolor-diagnostics -vectorize-loops -vectorize-slp -faddrsig -D__GCC_HAVE_DWARF2_CFI_ASM=1 -o /tmp/pyrefcon/pytorch/csa-scan/reports -x c++ ../torch/csrc/tensor/python_tensor.cpp
</div>
<div id='tooltiphint' hidden="true">
  <p>Keyboard shortcuts: </p>
  <ul>
    <li>Use 'j/k' keys for keyboard navigation</li>
    <li>Use 'Shift+S' to show/hide relevant lines</li>
    <li>Use '?' to toggle this window</li>
  </ul>
  <a href="#" onclick="toggleHelp(); return false;">Close</a>
</div>
<script type='text/javascript'>
var relevant_lines = {"1": {"161": 1, "162": 1, "163": 1, "164": 1, "165": 1, "166": 1, "241": 1, "242": 1, "243": 1, "245": 1, "246": 1, "248": 1, "249": 1, "251": 1, "252": 1, "254": 1, "257": 1, "261": 1, "309": 1, "311": 1, "312": 1, "314": 1, "329": 1, "332": 1, "337": 1, "342": 1, "345": 1, "352": 1, "355": 1, "356": 1, "357": 1, "359": 1, "360": 1, "362": 1, "363": 1, "364": 1, "365": 1, "366": 1, "368": 1, "369": 1}, "34138": {"1109": 1, "1110": 1, "1111": 1, "1112": 1, "1113": 1}, "35068": {"525": 1}, "124936": {"9": 1, "10": 1, "12": 1, "13": 1, "16": 1}, "172920": {"5": 1}, "172922": {"5": 1, "6": 1}, "172924": {"5": 1}};

var filterCounterexample = function (hide) {
  var tables = document.getElementsByClassName("code");
  for (var t=0; t<tables.length; t++) {
    var table = tables[t];
    var file_id = table.getAttribute("data-fileid");
    var lines_in_fid = relevant_lines[file_id];
    if (!lines_in_fid) {
      lines_in_fid = {};
    }
    var lines = table.getElementsByClassName("codeline");
    for (var i=0; i<lines.length; i++) {
        var el = lines[i];
        var lineNo = el.getAttribute("data-linenumber");
        if (!lines_in_fid[lineNo]) {
          if (hide) {
            el.setAttribute("hidden", "");
          } else {
            el.removeAttribute("hidden");
          }
        }
    }
  }
}

window.addEventListener("keydown", function (event) {
  if (event.defaultPrevented) {
    return;
  }
  if (event.key == "S") {
    var checked = document.getElementsByName("showCounterexample")[0].checked;
    filterCounterexample(!checked);
    document.getElementsByName("showCounterexample")[0].checked = !checked;
  } else {
    return;
  }
  event.preventDefault();
}, true);

document.addEventListener("DOMContentLoaded", function() {
    document.querySelector('input[name="showCounterexample"]').onchange=
        function (event) {
      filterCounterexample(this.checked);
    };
});
</script>

<form>
    <input type="checkbox" name="showCounterexample" id="showCounterexample" />
    <label for="showCounterexample">
       Show only relevant lines
    </label>
</form>

<script type='text/javascript'>
var digitMatcher = new RegExp("[0-9]+");

var querySelectorAllArray = function(selector) {
  return Array.prototype.slice.call(
    document.querySelectorAll(selector));
}

document.addEventListener("DOMContentLoaded", function() {
    querySelectorAllArray(".PathNav > a").forEach(
        function(currentValue, currentIndex) {
            var hrefValue = currentValue.getAttribute("href");
            currentValue.onclick = function() {
                scrollTo(document.querySelector(hrefValue));
                return false;
            };
        });
});

var findNum = function() {
    var s = document.querySelector(".selected");
    if (!s || s.id == "EndPath") {
        return 0;
    }
    var out = parseInt(digitMatcher.exec(s.id)[0]);
    return out;
};

var scrollTo = function(el) {
    querySelectorAllArray(".selected").forEach(function(s) {
        s.classList.remove("selected");
    });
    el.classList.add("selected");
    window.scrollBy(0, el.getBoundingClientRect().top -
        (window.innerHeight / 2));
}

var move = function(num, up, numItems) {
  if (num == 1 && up || num == numItems - 1 && !up) {
    return 0;
  } else if (num == 0 && up) {
    return numItems - 1;
  } else if (num == 0 && !up) {
    return 1 % numItems;
  }
  return up ? num - 1 : num + 1;
}

var numToId = function(num) {
  if (num == 0) {
    return document.getElementById("EndPath")
  }
  return document.getElementById("Path" + num);
};

var navigateTo = function(up) {
  var numItems = document.querySelectorAll(
      ".line > .msgEvent, .line > .msgControl").length;
  var currentSelected = findNum();
  var newSelected = move(currentSelected, up, numItems);
  var newEl = numToId(newSelected, numItems);

  // Scroll element into center.
  scrollTo(newEl);
};

window.addEventListener("keydown", function (event) {
  if (event.defaultPrevented) {
    return;
  }
  if (event.key == "j") {
    navigateTo(/*up=*/false);
  } else if (event.key == "k") {
    navigateTo(/*up=*/true);
  } else {
    return;
  }
  event.preventDefault();
}, true);
</script>
  
<script type='text/javascript'>

var toggleHelp = function() {
    var hint = document.querySelector("#tooltiphint");
    var attributeName = "hidden";
    if (hint.hasAttribute(attributeName)) {
      hint.removeAttribute(attributeName);
    } else {
      hint.setAttribute("hidden", "true");
    }
};
window.addEventListener("keydown", function (event) {
  if (event.defaultPrevented) {
    return;
  }
  if (event.key == "?") {
    toggleHelp();
  } else {
    return;
  }
  event.preventDefault();
});
</script>
<div id=File1>
<h4 class=FileName>../torch/csrc/tensor/python_tensor.cpp</h4>
<div class=FileNav><a href="#File172922">&#x2192;</a></div></div>
<table class="code" data-fileid="1">
<tr class="codeline" data-linenumber="1"><td class="num" id="LN1">1</td><td class="line"><span class='directive'>#include &lt;torch/csrc/tensor/python_tensor.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="2"><td class="num" id="LN2">2</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="3"><td class="num" id="LN3">3</td><td class="line"><span class='directive'>#include &lt;structmember.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="4"><td class="num" id="LN4">4</td><td class="line"><span class='directive'>#include &lt;pybind11/pybind11.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="5"><td class="num" id="LN5">5</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="6"><td class="num" id="LN6">6</td><td class="line"><span class='directive'>#include &lt;torch/csrc/Dtype.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="7"><td class="num" id="LN7">7</td><td class="line"><span class='directive'>#include &lt;torch/csrc/DynamicTypes.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="8"><td class="num" id="LN8">8</td><td class="line"><span class='directive'>#include &lt;torch/csrc/Exceptions.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="9"><td class="num" id="LN9">9</td><td class="line"><span class='directive'>#include &lt;torch/csrc/Layout.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="10"><td class="num" id="LN10">10</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/variable.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="11"><td class="num" id="LN11">11</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/python_variable.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="12"><td class="num" id="LN12">12</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/generated/VariableType.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="13"><td class="num" id="LN13">13</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/utils/wrap_outputs.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="14"><td class="num" id="LN14">14</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/cuda_enabled.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="15"><td class="num" id="LN15">15</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/cuda_lazy_init.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="16"><td class="num" id="LN16">16</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/python_strings.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="17"><td class="num" id="LN17">17</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/tensor_new.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="18"><td class="num" id="LN18">18</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/tensor_types.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="19"><td class="num" id="LN19">19</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="20"><td class="num" id="LN20">20</td><td class="line"><span class='directive'>#include &lt;ATen/ATen.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="21"><td class="num" id="LN21">21</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="22"><td class="num" id="LN22">22</td><td class="line"><span class='directive'>#include &lt;sstream&gt;</span></td></tr>
<tr class="codeline" data-linenumber="23"><td class="num" id="LN23">23</td><td class="line"><span class='directive'>#include &lt;string&gt;</span></td></tr>
<tr class="codeline" data-linenumber="24"><td class="num" id="LN24">24</td><td class="line"><span class='directive'>#include &lt;type_traits&gt;</span></td></tr>
<tr class="codeline" data-linenumber="25"><td class="num" id="LN25">25</td><td class="line"><span class='directive'>#include &lt;vector&gt;</span></td></tr>
<tr class="codeline" data-linenumber="26"><td class="num" id="LN26">26</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="27"><td class="num" id="LN27">27</td><td class="line"><span class='keyword'>namespace</span> torch { <span class='keyword'>namespace</span> tensors {</td></tr>
<tr class="codeline" data-linenumber="28"><td class="num" id="LN28">28</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="29"><td class="num" id="LN29">29</td><td class="line"><span class='keyword'>using</span> <span class='keyword'>namespace</span> at;</td></tr>
<tr class="codeline" data-linenumber="30"><td class="num" id="LN30">30</td><td class="line"><span class='keyword'>using</span> <span class='keyword'>namespace</span> torch::autograd;</td></tr>
<tr class="codeline" data-linenumber="31"><td class="num" id="LN31">31</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="32"><td class="num" id="LN32">32</td><td class="line"><span class='keyword'>struct</span> PyTensorType {</td></tr>
<tr class="codeline" data-linenumber="33"><td class="num" id="LN33">33</td><td class="line">  PyTypeObject py_type;</td></tr>
<tr class="codeline" data-linenumber="34"><td class="num" id="LN34">34</td><td class="line">  THPDtype* dtype;</td></tr>
<tr class="codeline" data-linenumber="35"><td class="num" id="LN35">35</td><td class="line">  THPLayout* layout;</td></tr>
<tr class="codeline" data-linenumber="36"><td class="num" id="LN36">36</td><td class="line">  <span class='keyword'>bool</span> is_cuda;</td></tr>
<tr class="codeline" data-linenumber="37"><td class="num" id="LN37">37</td><td class="line">  <span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-magic-numbers,modernize-avoid-c-arrays)</span></td></tr>
<tr class="codeline" data-linenumber="38"><td class="num" id="LN38">38</td><td class="line">  <span class='keyword'>char</span> name[64];</td></tr>
<tr class="codeline" data-linenumber="39"><td class="num" id="LN39">39</td><td class="line">  <span class='keyword'>int</span> backend;</td></tr>
<tr class="codeline" data-linenumber="40"><td class="num" id="LN40">40</td><td class="line">  <span class='keyword'>int</span> scalar_type;</td></tr>
<tr class="codeline" data-linenumber="41"><td class="num" id="LN41">41</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="42"><td class="num" id="LN42">42</td><td class="line">  Backend get_backend() <span class='keyword'>const</span> {</td></tr>
<tr class="codeline" data-linenumber="43"><td class="num" id="LN43">43</td><td class="line">    <span class='keyword'>return</span> <span class='keyword'>static_cast</span>&lt;Backend&gt;(backend);</td></tr>
<tr class="codeline" data-linenumber="44"><td class="num" id="LN44">44</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="45"><td class="num" id="LN45">45</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="46"><td class="num" id="LN46">46</td><td class="line">  DispatchKey get_dispatch_key() <span class='keyword'>const</span> {</td></tr>
<tr class="codeline" data-linenumber="47"><td class="num" id="LN47">47</td><td class="line">    <span class='keyword'>return</span> backendToDispatchKey(<span class='keyword'>static_cast</span>&lt;Backend&gt;(backend));</td></tr>
<tr class="codeline" data-linenumber="48"><td class="num" id="LN48">48</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="49"><td class="num" id="LN49">49</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="50"><td class="num" id="LN50">50</td><td class="line">  ScalarType get_scalar_type() <span class='keyword'>const</span> {</td></tr>
<tr class="codeline" data-linenumber="51"><td class="num" id="LN51">51</td><td class="line">    <span class='keyword'>return</span> <span class='keyword'>static_cast</span>&lt;ScalarType&gt;(scalar_type);</td></tr>
<tr class="codeline" data-linenumber="52"><td class="num" id="LN52">52</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="53"><td class="num" id="LN53">53</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="54"><td class="num" id="LN54">54</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="55"><td class="num" id="LN55">55</td><td class="line"><span class='keyword'>static_assert</span>(std::is_standard_layout&lt;PyTensorType&gt;::value, <span class='string_literal'>"PyTensorType must be standard layout"</span>);</td></tr>
<tr class="codeline" data-linenumber="56"><td class="num" id="LN56">56</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="57"><td class="num" id="LN57">57</td><td class="line"><span class='comment'>// This is always an instance of VariableType</span></td></tr>
<tr class="codeline" data-linenumber="58"><td class="num" id="LN58">58</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="59"><td class="num" id="LN59">59</td><td class="line"><span class='keyword'>static</span> PyTensorType* default_tensor_type;</td></tr>
<tr class="codeline" data-linenumber="60"><td class="num" id="LN60">60</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="61"><td class="num" id="LN61">61</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>void</span> py_bind_tensor_types(<span class='keyword'>const</span> std::vector&lt;PyTensorType*&gt;&amp; tensor_types);</td></tr>
<tr class="codeline" data-linenumber="62"><td class="num" id="LN62">62</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="63"><td class="num" id="LN63">63</td><td class="line"><span class='keyword'>static</span> TypeError unavailable_type(<span class='keyword'>const</span> PyTensorType&amp; type) {</td></tr>
<tr class="codeline" data-linenumber="64"><td class="num" id="LN64">64</td><td class="line">  <span class='keyword'>return</span> TypeError(<span class='string_literal'>"type %s not available. Torch not compiled with CUDA enabled."</span>, type.name);</td></tr>
<tr class="codeline" data-linenumber="65"><td class="num" id="LN65">65</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="66"><td class="num" id="LN66">66</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="67"><td class="num" id="LN67">67</td><td class="line"><span class='keyword'>static</span> PyObject* Tensor_new(PyTypeObject *type, PyObject *args, PyObject *kwargs) {</td></tr>
<tr class="codeline" data-linenumber="68"><td class="num" id="LN68">68</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="69"><td class="num" id="LN69">69</td><td class="line">  <span class='keyword'>auto</span>&amp; tensor_type = *((PyTensorType*)type);</td></tr>
<tr class="codeline" data-linenumber="70"><td class="num" id="LN70">70</td><td class="line">  <span class='keyword'>if</span> (tensor_type.is_cuda &amp;&amp; !torch::utils::cuda_enabled()) {</td></tr>
<tr class="codeline" data-linenumber="71"><td class="num" id="LN71">71</td><td class="line">    <span class='keyword'>throw</span> unavailable_type(tensor_type);</td></tr>
<tr class="codeline" data-linenumber="72"><td class="num" id="LN72">72</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="73"><td class="num" id="LN73">73</td><td class="line">  <span class='keyword'>return</span> THPVariable_Wrap(torch::utils::legacy_tensor_ctor(tensor_type.get_dispatch_key(), tensor_type.get_scalar_type(), args, kwargs));</td></tr>
<tr class="codeline" data-linenumber="74"><td class="num" id="LN74">74</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="75"><td class="num" id="LN75">75</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="76"><td class="num" id="LN76">76</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="77"><td class="num" id="LN77">77</td><td class="line"><span class='comment'>// TODO: Deprecate this instancecheck entirely.  It's here to make</span></td></tr>
<tr class="codeline" data-linenumber="78"><td class="num" id="LN78">78</td><td class="line"><span class='comment'>// instanceof(t, torch.FloatTensor) work, but we are not going to keep</span></td></tr>
<tr class="codeline" data-linenumber="79"><td class="num" id="LN79">79</td><td class="line"><span class='comment'>// adding torch.QuantizedIntTensor classes for every new tensor type</span></td></tr>
<tr class="codeline" data-linenumber="80"><td class="num" id="LN80">80</td><td class="line"><span class='comment'>// we add...</span></td></tr>
<tr class="codeline" data-linenumber="81"><td class="num" id="LN81">81</td><td class="line"><span class='keyword'>static</span> PyObject* Tensor_instancecheck(PyObject* _self, PyObject* arg) {</td></tr>
<tr class="codeline" data-linenumber="82"><td class="num" id="LN82">82</td><td class="line">  <span class='macro'>HANDLE_TH_ERRORS<span class='macro_popup'>try { torch::PyWarningHandler __enforce_warning_buffer; try {</span></span></td></tr>
<tr class="codeline" data-linenumber="83"><td class="num" id="LN83">83</td><td class="line">  <span class='keyword'>auto</span> self = (PyTensorType*)_self;</td></tr>
<tr class="codeline" data-linenumber="84"><td class="num" id="LN84">84</td><td class="line">  <span class='keyword'>if</span> (THPVariable_Check(arg)) {</td></tr>
<tr class="codeline" data-linenumber="85"><td class="num" id="LN85">85</td><td class="line">    <span class='keyword'>const</span> <span class='keyword'>auto</span>&amp; var = THPVariable_Unpack(arg);</td></tr>
<tr class="codeline" data-linenumber="86"><td class="num" id="LN86">86</td><td class="line">    <span class='comment'>// NB: This is a little unfortunate, in that if I do an isinstance check</span></td></tr>
<tr class="codeline" data-linenumber="87"><td class="num" id="LN87">87</td><td class="line">    <span class='comment'>// against torch.cuda.FloatTensor, this will immediately initialize CUDA.</span></td></tr>
<tr class="codeline" data-linenumber="88"><td class="num" id="LN88">88</td><td class="line">    <span class='comment'>// I originally thought that it would not be possible for aten_type_ to</span></td></tr>
<tr class="codeline" data-linenumber="89"><td class="num" id="LN89">89</td><td class="line">    <span class='comment'>// be nullptr if you had a tensor of some type, in which case you can</span></td></tr>
<tr class="codeline" data-linenumber="90"><td class="num" id="LN90">90</td><td class="line">    <span class='comment'>// skip initializing aten_type(), but TestAutograd.test_type_conversions</span></td></tr>
<tr class="codeline" data-linenumber="91"><td class="num" id="LN91">91</td><td class="line">    <span class='comment'>// seems to violate this property (for whatever reason.)</span></td></tr>
<tr class="codeline" data-linenumber="92"><td class="num" id="LN92">92</td><td class="line">    <span class='comment'>//</span></td></tr>
<tr class="codeline" data-linenumber="93"><td class="num" id="LN93">93</td><td class="line">    <span class='comment'>// TODO: Stop using legacyExtractDispatchKey here (probably need to build</span></td></tr>
<tr class="codeline" data-linenumber="94"><td class="num" id="LN94">94</td><td class="line">    <span class='comment'>// in instanceof checking to Tensor class itself)</span></td></tr>
<tr class="codeline" data-linenumber="95"><td class="num" id="LN95">95</td><td class="line">    <span class='keyword'>if</span> (legacyExtractDispatchKey(var.key_set()) == self-&gt;get_dispatch_key() &amp;&amp;</td></tr>
<tr class="codeline" data-linenumber="96"><td class="num" id="LN96">96</td><td class="line">        var.scalar_type() == <span class='keyword'>static_cast</span>&lt;ScalarType&gt;(self-&gt;scalar_type)) {</td></tr>
<tr class="codeline" data-linenumber="97"><td class="num" id="LN97">97</td><td class="line">      <span class='macro'>Py_RETURN_TRUE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_TrueStruct<br>)))), ((PyObject *) &amp;_Py_TrueStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="98"><td class="num" id="LN98">98</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="99"><td class="num" id="LN99">99</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="100"><td class="num" id="LN100">100</td><td class="line">  <span class='macro'>Py_RETURN_FALSE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_FalseStruct<br>)))), ((PyObject *) &amp;_Py_FalseStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="101"><td class="num" id="LN101">101</td><td class="line">  <span class='macro'>END_HANDLE_TH_ERRORS<span class='macro_popup'>} catch(...) { __enforce_warning_buffer.set_in_exception(); throw<br>; } } catch (python_error &amp; e) { e.restore(); return nullptr<br>; } catch (const c10::IndexError&amp; e) { auto msg = torch::<br>get_cpp_stacktraces_enabled() ? e.what() : e.what_without_backtrace<br>(); PyErr_SetString(PyExc_IndexError, torch::processErrorMsg(<br>msg)); return nullptr; } catch (const c10::ValueError&amp; e)<br> { auto msg = torch::get_cpp_stacktraces_enabled() ? e.what()<br> : e.what_without_backtrace(); PyErr_SetString(PyExc_ValueError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::TypeError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_TypeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::NotImplementedError&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_NotImplementedError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (const<br> c10::Error&amp; e) { auto msg = torch::get_cpp_stacktraces_enabled<br>() ? e.what() : e.what_without_backtrace(); PyErr_SetString(PyExc_RuntimeError<br>, torch::processErrorMsg(msg)); return nullptr; } catch (torch<br>::PyTorchError &amp; e) { auto msg = torch::processErrorMsg(e<br>.what()); PyErr_SetString(e.python_type(), msg); return nullptr<br>; } catch (const std::exception&amp; e) { auto msg = torch::processErrorMsg<br>(e.what()); PyErr_SetString(PyExc_RuntimeError, msg); return nullptr<br>; }</span></span></td></tr>
<tr class="codeline" data-linenumber="102"><td class="num" id="LN102">102</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="103"><td class="num" id="LN103">103</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="104"><td class="num" id="LN104">104</td><td class="line">PyObject *Tensor_dtype(PyTensorType* self, <span class='keyword'>void</span> *unused) {</td></tr>
<tr class="codeline" data-linenumber="105"><td class="num" id="LN105">105</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self-&gt;dtype);</td></tr>
<tr class="codeline" data-linenumber="106"><td class="num" id="LN106">106</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="107"><td class="num" id="LN107">107</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="108"><td class="num" id="LN108">108</td><td class="line">PyObject *Tensor_layout(PyTensorType* self, <span class='keyword'>void</span> *unused) {</td></tr>
<tr class="codeline" data-linenumber="109"><td class="num" id="LN109">109</td><td class="line">  <span class='keyword'>return</span> torch::autograd::utils::wrap(self-&gt;layout);</td></tr>
<tr class="codeline" data-linenumber="110"><td class="num" id="LN110">110</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="111"><td class="num" id="LN111">111</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="112"><td class="num" id="LN112">112</td><td class="line">PyObject *Tensor_is_cuda(PyTensorType* self, <span class='keyword'>void</span> *unused) {</td></tr>
<tr class="codeline" data-linenumber="113"><td class="num" id="LN113">113</td><td class="line">  <span class='keyword'>if</span> (self-&gt;is_cuda) {</td></tr>
<tr class="codeline" data-linenumber="114"><td class="num" id="LN114">114</td><td class="line">    <span class='macro'>Py_RETURN_TRUE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_TrueStruct<br>)))), ((PyObject *) &amp;_Py_TrueStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="115"><td class="num" id="LN115">115</td><td class="line">  } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="116"><td class="num" id="LN116">116</td><td class="line">    <span class='macro'>Py_RETURN_FALSE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_FalseStruct<br>)))), ((PyObject *) &amp;_Py_FalseStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="117"><td class="num" id="LN117">117</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="118"><td class="num" id="LN118">118</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="119"><td class="num" id="LN119">119</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="120"><td class="num" id="LN120">120</td><td class="line">PyObject *Tensor_is_sparse(PyTensorType *self, <span class='keyword'>void</span> *unused) {</td></tr>
<tr class="codeline" data-linenumber="121"><td class="num" id="LN121">121</td><td class="line">  <span class='keyword'>if</span> (self-&gt;layout-&gt;layout == at::Layout::Strided) {</td></tr>
<tr class="codeline" data-linenumber="122"><td class="num" id="LN122">122</td><td class="line">    <span class='macro'>Py_RETURN_FALSE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_FalseStruct<br>)))), ((PyObject *) &amp;_Py_FalseStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="123"><td class="num" id="LN123">123</td><td class="line">  } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="124"><td class="num" id="LN124">124</td><td class="line">    <span class='macro'>Py_RETURN_TRUE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_TrueStruct<br>)))), ((PyObject *) &amp;_Py_TrueStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="125"><td class="num" id="LN125">125</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="126"><td class="num" id="LN126">126</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="127"><td class="num" id="LN127">127</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="128"><td class="num" id="LN128">128</td><td class="line">PyObject *Tensor_is_sparse_csr(PyTensorType *self, <span class='keyword'>void</span> *unused) {</td></tr>
<tr class="codeline" data-linenumber="129"><td class="num" id="LN129">129</td><td class="line">  <span class='keyword'>if</span> (self-&gt;layout-&gt;layout == at::Layout::SparseCsr) {</td></tr>
<tr class="codeline" data-linenumber="130"><td class="num" id="LN130">130</td><td class="line">    <span class='macro'>Py_RETURN_TRUE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_TrueStruct<br>)))), ((PyObject *) &amp;_Py_TrueStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="131"><td class="num" id="LN131">131</td><td class="line">  } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="132"><td class="num" id="LN132">132</td><td class="line">    <span class='macro'>Py_RETURN_FALSE<span class='macro_popup'>return _Py_INCREF(((PyObject*)(((PyObject *) &amp;_Py_FalseStruct<br>)))), ((PyObject *) &amp;_Py_FalseStruct)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="133"><td class="num" id="LN133">133</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="134"><td class="num" id="LN134">134</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="135"><td class="num" id="LN135">135</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="136"><td class="num" id="LN136">136</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables,modernize-avoid-c-arrays)</span></td></tr>
<tr class="codeline" data-linenumber="137"><td class="num" id="LN137">137</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>struct</span> PyMethodDef metaclass_methods[] = {</td></tr>
<tr class="codeline" data-linenumber="138"><td class="num" id="LN138">138</td><td class="line">  {<span class='string_literal'>"__instancecheck__"</span>, Tensor_instancecheck, <span class='macro'>METH_O<span class='macro_popup'>0x0008</span></span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="139"><td class="num" id="LN139">139</td><td class="line">  {<span class='keyword'>nullptr</span>}</td></tr>
<tr class="codeline" data-linenumber="140"><td class="num" id="LN140">140</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="141"><td class="num" id="LN141">141</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="142"><td class="num" id="LN142">142</td><td class="line"><span class='keyword'>typedef</span> PyObject *(*getter)(PyObject *, <span class='keyword'>void</span> *);</td></tr>
<tr class="codeline" data-linenumber="143"><td class="num" id="LN143">143</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="144"><td class="num" id="LN144">144</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables,modernize-avoid-c-arrays)</span></td></tr>
<tr class="codeline" data-linenumber="145"><td class="num" id="LN145">145</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>struct</span> PyGetSetDef metaclass_properties[] = {</td></tr>
<tr class="codeline" data-linenumber="146"><td class="num" id="LN146">146</td><td class="line">  {<span class='string_literal'>"dtype"</span>,        (getter)Tensor_dtype, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="147"><td class="num" id="LN147">147</td><td class="line">  {<span class='string_literal'>"layout"</span>,       (getter)Tensor_layout, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="148"><td class="num" id="LN148">148</td><td class="line">  {<span class='string_literal'>"is_cuda"</span>,      (getter)Tensor_is_cuda, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="149"><td class="num" id="LN149">149</td><td class="line">  {<span class='string_literal'>"is_sparse"</span>,    (getter)Tensor_is_sparse, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="150"><td class="num" id="LN150">150</td><td class="line">  {<span class='string_literal'>"is_sparse_csr"</span>,(getter)Tensor_is_sparse_csr, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>, <span class='keyword'>nullptr</span>},</td></tr>
<tr class="codeline" data-linenumber="151"><td class="num" id="LN151">151</td><td class="line">  {<span class='keyword'>nullptr</span>}</td></tr>
<tr class="codeline" data-linenumber="152"><td class="num" id="LN152">152</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="153"><td class="num" id="LN153">153</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="154"><td class="num" id="LN154">154</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="155"><td class="num" id="LN155">155</td><td class="line"><span class='keyword'>static</span> PyTypeObject metaclass = {</td></tr>
<tr class="codeline" data-linenumber="156"><td class="num" id="LN156">156</td><td class="line">  <span class='macro'>PyVarObject_HEAD_INIT(<span class='keyword'>nullptr</span>, 0)<span class='macro_popup'>{ { 1, nullptr }, 0 },</span></span></td></tr>
<tr class="codeline" data-linenumber="157"><td class="num" id="LN157">157</td><td class="line">  <span class='string_literal'>"torch.tensortype"</span>,                          <span class='comment'>/* tp_name */</span></td></tr>
<tr class="codeline" data-linenumber="158"><td class="num" id="LN158">158</td><td class="line">  <span class='keyword'>sizeof</span>(PyTypeObject)                         <span class='comment'>/* tp_basicsize */</span></td></tr>
<tr class="codeline" data-linenumber="159"><td class="num" id="LN159">159</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="160"><td class="num" id="LN160">160</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="161"><td class="num" id="LN161">161</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>void</span> py_initialize_metaclass(PyTypeObject&amp; metaclass) {</td></tr>
<tr class="codeline" data-linenumber="162"><td class="num" id="LN162">162</td><td class="line">  metaclass.tp_flags = <span class='macro'>Py_TPFLAGS_DEFAULT<span class='macro_popup'>( 0 | (1UL &lt;&lt; 18) | 0)</span></span> | <span class='macro'>Py_TPFLAGS_BASETYPE<span class='macro_popup'>(1UL &lt;&lt; 10)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="163"><td class="num" id="LN163">163</td><td class="line">  metaclass.tp_methods = metaclass_methods;</td></tr>
<tr class="codeline" data-linenumber="164"><td class="num" id="LN164">164</td><td class="line">  metaclass.tp_getset = metaclass_properties;</td></tr>
<tr class="codeline" data-linenumber="165"><td class="num" id="LN165">165</td><td class="line">  metaclass.tp_base = &amp;PyType_Type;</td></tr>
<tr class="codeline" data-linenumber="166"><td class="num" id="LN166">166</td><td class="line">  <span class='keyword'>if</span> (PyType_Ready(&amp;metaclass) &lt; 0) {</td></tr>
<tr class="codeline" data-linenumber="167"><td class="num" id="LN167">167</td><td class="line">    <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="168"><td class="num" id="LN168">168</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="169"><td class="num" id="LN169">169</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="170"><td class="num" id="LN170">170</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="171"><td class="num" id="LN171">171</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="172"><td class="num" id="LN172">172</td><td class="line"><span class='keyword'>static</span> PyTypeObject tensor_type_prototype = {</td></tr>
<tr class="codeline" data-linenumber="173"><td class="num" id="LN173">173</td><td class="line">  <span class='macro'>PyVarObject_HEAD_INIT(&amp;metaclass, 0)<span class='macro_popup'>{ { 1, &amp;metaclass }, 0 },</span></span></td></tr>
<tr class="codeline" data-linenumber="174"><td class="num" id="LN174">174</td><td class="line">  <span class='keyword'>nullptr</span>,                                     <span class='comment'>/* tp_name */</span></td></tr>
<tr class="codeline" data-linenumber="175"><td class="num" id="LN175">175</td><td class="line">  <span class='keyword'>sizeof</span>(PyTensorType)                         <span class='comment'>/* tp_basicsize */</span></td></tr>
<tr class="codeline" data-linenumber="176"><td class="num" id="LN176">176</td><td class="line">};</td></tr>
<tr class="codeline" data-linenumber="177"><td class="num" id="LN177">177</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="178"><td class="num" id="LN178">178</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>void</span> py_initialize_tensor_type(PyTypeObject&amp; type, <span class='keyword'>const</span> <span class='keyword'>char</span>* name, PyObject* tp_dict) {</td></tr>
<tr class="codeline" data-linenumber="179"><td class="num" id="LN179">179</td><td class="line">  <span class='comment'>// NOTE: we don't use the typical static declaration of PyTypeObject because</span></td></tr>
<tr class="codeline" data-linenumber="180"><td class="num" id="LN180">180</td><td class="line">  <span class='comment'>// we need to initialize as many types as there are VariableType instances.</span></td></tr>
<tr class="codeline" data-linenumber="181"><td class="num" id="LN181">181</td><td class="line">  <span class='comment'>// We copy the basic object fields from a prototype definition and initialize</span></td></tr>
<tr class="codeline" data-linenumber="182"><td class="num" id="LN182">182</td><td class="line">  <span class='comment'>// the remaining fields below.</span></td></tr>
<tr class="codeline" data-linenumber="183"><td class="num" id="LN183">183</td><td class="line">  memcpy(&amp;type, &amp;tensor_type_prototype, <span class='keyword'>sizeof</span>(PyTypeObject));</td></tr>
<tr class="codeline" data-linenumber="184"><td class="num" id="LN184">184</td><td class="line">  <span class='comment'>// Subclassing from torch.&lt;ScalarType&gt;Tensor isn't supported.</span></td></tr>
<tr class="codeline" data-linenumber="185"><td class="num" id="LN185">185</td><td class="line">  <span class='comment'>// (Py_TPFLAGS_BASETYPE omitted). Subclassing torch.Tensor still allowed.</span></td></tr>
<tr class="codeline" data-linenumber="186"><td class="num" id="LN186">186</td><td class="line">  type.tp_flags = <span class='macro'>Py_TPFLAGS_DEFAULT<span class='macro_popup'>( 0 | (1UL &lt;&lt; 18) | 0)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="187"><td class="num" id="LN187">187</td><td class="line">  type.tp_name = name;</td></tr>
<tr class="codeline" data-linenumber="188"><td class="num" id="LN188">188</td><td class="line">  type.tp_new = Tensor_new;</td></tr>
<tr class="codeline" data-linenumber="189"><td class="num" id="LN189">189</td><td class="line">  <span class='keyword'>if</span> (PyType_Ready(&amp;type) &lt; 0) {</td></tr>
<tr class="codeline" data-linenumber="190"><td class="num" id="LN190">190</td><td class="line">    <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="191"><td class="num" id="LN191">191</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="192"><td class="num" id="LN192">192</td><td class="line">  <span class='keyword'>if</span> (PyDict_Merge(type.tp_dict, tp_dict, 0) &lt; 0) {</td></tr>
<tr class="codeline" data-linenumber="193"><td class="num" id="LN193">193</td><td class="line">    <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="194"><td class="num" id="LN194">194</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="195"><td class="num" id="LN195">195</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="196"><td class="num" id="LN196">196</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="197"><td class="num" id="LN197">197</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>const</span> <span class='keyword'>char</span>* get_module(Backend backend) {</td></tr>
<tr class="codeline" data-linenumber="198"><td class="num" id="LN198">198</td><td class="line">  <span class='keyword'>switch</span> (backend) {</td></tr>
<tr class="codeline" data-linenumber="199"><td class="num" id="LN199">199</td><td class="line">    <span class='keyword'>case</span> Backend::CPU: <span class='keyword'>return</span> <span class='string_literal'>"torch"</span>;</td></tr>
<tr class="codeline" data-linenumber="200"><td class="num" id="LN200">200</td><td class="line">    <span class='keyword'>case</span> Backend::CUDA: <span class='keyword'>return</span> <span class='string_literal'>"torch.cuda"</span>;</td></tr>
<tr class="codeline" data-linenumber="201"><td class="num" id="LN201">201</td><td class="line">    <span class='keyword'>case</span> Backend::SparseCPU: <span class='keyword'>return</span> <span class='string_literal'>"torch.sparse"</span>;</td></tr>
<tr class="codeline" data-linenumber="202"><td class="num" id="LN202">202</td><td class="line">    <span class='keyword'>case</span> Backend::SparseCUDA: <span class='keyword'>return</span> <span class='string_literal'>"torch.cuda.sparse"</span>;</td></tr>
<tr class="codeline" data-linenumber="203"><td class="num" id="LN203">203</td><td class="line">    <span class='keyword'>default</span>: <span class='macro'>AT_ERROR(<span class='string_literal'>"invalid backend: "</span>, toString(backend))<span class='macro_popup'>do { ::c10::detail::deprecated_AT_ERROR(); if ((__builtin_expect<br>(static_cast&lt;bool&gt;(!(false)), 0))) { ::c10::detail::torchCheckFail<br>( __func__, "../torch/csrc/tensor/python_tensor.cpp", static_cast<br>&lt;uint32_t&gt;(203), (::c10::detail::torchCheckMsgImpl( "Expected "<br> "false" " to be true, but got false.  " "(Could this error message be improved?  If so, "<br> "please report an enhancement request to PyTorch.)", ::c10::<br>str("invalid backend: ", toString(backend))))); }; } while (false<br>)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="204"><td class="num" id="LN204">204</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="205"><td class="num" id="LN205">205</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="206"><td class="num" id="LN206">206</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="207"><td class="num" id="LN207">207</td><td class="line"><span class='keyword'>static</span> std::string get_name(Backend backend, ScalarType scalarType) {</td></tr>
<tr class="codeline" data-linenumber="208"><td class="num" id="LN208">208</td><td class="line">  std::ostringstream ss;</td></tr>
<tr class="codeline" data-linenumber="209"><td class="num" id="LN209">209</td><td class="line">  ss &lt;&lt; get_module(backend) &lt;&lt; <span class='string_literal'>"."</span> &lt;&lt; toString(scalarType) &lt;&lt; <span class='string_literal'>"Tensor"</span>;</td></tr>
<tr class="codeline" data-linenumber="210"><td class="num" id="LN210">210</td><td class="line">  <span class='keyword'>return</span> ss.str();</td></tr>
<tr class="codeline" data-linenumber="211"><td class="num" id="LN211">211</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="212"><td class="num" id="LN212">212</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="213"><td class="num" id="LN213">213</td><td class="line"><span class='keyword'>static</span> THPObjectPtr get_storage_obj(PyTensorType* type) {</td></tr>
<tr class="codeline" data-linenumber="214"><td class="num" id="LN214">214</td><td class="line">  <span class='keyword'>auto</span> module_name = get_module(type-&gt;get_backend());</td></tr>
<tr class="codeline" data-linenumber="215"><td class="num" id="LN215">215</td><td class="line">  <span class='keyword'>auto</span> module_obj = THPObjectPtr(PyImport_ImportModule(module_name));</td></tr>
<tr class="codeline" data-linenumber="216"><td class="num" id="LN216">216</td><td class="line">  <span class='keyword'>if</span> (!module_obj) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="217"><td class="num" id="LN217">217</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="218"><td class="num" id="LN218">218</td><td class="line">  <span class='keyword'>auto</span> storage_name = std::string(toString(type-&gt;get_scalar_type())) + <span class='string_literal'>"Storage"</span>;</td></tr>
<tr class="codeline" data-linenumber="219"><td class="num" id="LN219">219</td><td class="line">  THPObjectPtr storage(PyObject_GetAttrString(module_obj.get(), storage_name.c_str()));</td></tr>
<tr class="codeline" data-linenumber="220"><td class="num" id="LN220">220</td><td class="line">  <span class='keyword'>if</span> (!storage.get()) {</td></tr>
<tr class="codeline" data-linenumber="221"><td class="num" id="LN221">221</td><td class="line">    <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"couldn't find storage object %s"</span>, storage_name.c_str());</td></tr>
<tr class="codeline" data-linenumber="222"><td class="num" id="LN222">222</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="223"><td class="num" id="LN223">223</td><td class="line">  <span class='keyword'>return</span> storage;</td></tr>
<tr class="codeline" data-linenumber="224"><td class="num" id="LN224">224</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="225"><td class="num" id="LN225">225</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="226"><td class="num" id="LN226">226</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>void</span> set_type(PyTensorType&amp; type_obj, Backend backend, ScalarType scalarType) {</td></tr>
<tr class="codeline" data-linenumber="227"><td class="num" id="LN227">227</td><td class="line">  <span class='comment'>// This field is lazily initialized from backend and scalar_type</span></td></tr>
<tr class="codeline" data-linenumber="228"><td class="num" id="LN228">228</td><td class="line">  type_obj.backend = <span class='keyword'>static_cast</span>&lt;<span class='keyword'>int</span>&gt;(backend);</td></tr>
<tr class="codeline" data-linenumber="229"><td class="num" id="LN229">229</td><td class="line">  type_obj.scalar_type = <span class='keyword'>static_cast</span>&lt;<span class='keyword'>int</span>&gt;(scalarType);</td></tr>
<tr class="codeline" data-linenumber="230"><td class="num" id="LN230">230</td><td class="line">  type_obj.layout = torch::getTHPLayout(layout_from_backend(backend));</td></tr>
<tr class="codeline" data-linenumber="231"><td class="num" id="LN231">231</td><td class="line">  type_obj.dtype = torch::getTHPDtype(scalarType);</td></tr>
<tr class="codeline" data-linenumber="232"><td class="num" id="LN232">232</td><td class="line">  type_obj.is_cuda = (backend == at::Backend::CUDA || backend == at::Backend::SparseCUDA);</td></tr>
<tr class="codeline" data-linenumber="233"><td class="num" id="LN233">233</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="234"><td class="num" id="LN234">234</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="235"><td class="num" id="LN235">235</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>void</span> set_name(PyTensorType&amp; type_obj, <span class='keyword'>const</span> std::string&amp; name) {</td></tr>
<tr class="codeline" data-linenumber="236"><td class="num" id="LN236">236</td><td class="line">  size_t n = <span class='keyword'>sizeof</span>(type_obj.name);</td></tr>
<tr class="codeline" data-linenumber="237"><td class="num" id="LN237">237</td><td class="line">  strncpy(type_obj.name, name.c_str(), n);</td></tr>
<tr class="codeline" data-linenumber="238"><td class="num" id="LN238">238</td><td class="line">  type_obj.name[n - 1] = '\0';</td></tr>
<tr class="codeline" data-linenumber="239"><td class="num" id="LN239">239</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="240"><td class="num" id="LN240">240</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="241"><td class="num" id="LN241">241</td><td class="line"><span class='keyword'>static</span> THPObjectPtr get_tensor_dict() {</td></tr>
<tr class="codeline" data-linenumber="242"><td class="num" id="LN242">242</td><td class="line">  <span class='keyword'>auto</span> torch = THPObjectPtr(PyImport_ImportModule(<span class='string_literal'>"torch"</span>));</td></tr>
<tr class="codeline" data-linenumber="243"><td class="num" id="LN243">243</td><td class="line">  <span class='keyword'>if</span> (!torch) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="244"><td class="num" id="LN244">244</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="245"><td class="num" id="LN245">245</td><td class="line">  <span class='keyword'>auto</span> tensor_class = THPObjectPtr(PyObject_GetAttrString(torch, <span class='string_literal'>"Tensor"</span>));</td></tr>
<tr class="codeline" data-linenumber="246"><td class="num" id="LN246">246</td><td class="line">  <span class='keyword'>if</span> (!tensor_class) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="247"><td class="num" id="LN247">247</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="248"><td class="num" id="LN248">248</td><td class="line">  <span class='keyword'>auto</span> tensor_type = (PyTypeObject*)tensor_class.get();</td></tr>
<tr class="codeline" data-linenumber="249"><td class="num" id="LN249">249</td><td class="line">  <span class='macro'>TORCH_CHECK(tensor_type-&gt;tp_base, <span class='string_literal'>"missing base type for Tensor"</span>)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(tensor_type-&gt;<br>tp_base)), 0))) { ::c10::detail::torchCheckFail( __func__, "../torch/csrc/tensor/python_tensor.cpp"<br>, static_cast&lt;uint32_t&gt;(249), (::c10::detail::torchCheckMsgImpl<br>( "Expected " "tensor_type-&gt;tp_base" " to be true, but got false.  "<br> "(Could this error message be improved?  If so, " "please report an enhancement request to PyTorch.)"<br>, "missing base type for Tensor"))); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="250"><td class="num" id="LN250">250</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="251"><td class="num" id="LN251">251</td><td class="line">  <span class='keyword'>auto</span> res = THPObjectPtr(PyDict_New());</td></tr>
<tr class="codeline" data-linenumber="252"><td class="num" id="LN252">252</td><td class="line">  <span class='keyword'>if</span> (!res) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="253"><td class="num" id="LN253">253</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="254"><td class="num" id="LN254">254</td><td class="line">  <span class='keyword'>if</span> (PyDict_Merge(res.get(), tensor_type-&gt;tp_dict, 0) &lt; 0) {</td></tr>
<tr class="codeline" data-linenumber="255"><td class="num" id="LN255">255</td><td class="line">    <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="256"><td class="num" id="LN256">256</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="257"><td class="num" id="LN257">257</td><td class="line">  <span class='keyword'>if</span> (PyDict_Merge(res.get(), tensor_type-&gt;tp_base-&gt;tp_dict, 0) &lt; 0) {</td></tr>
<tr class="codeline" data-linenumber="258"><td class="num" id="LN258">258</td><td class="line">    <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="259"><td class="num" id="LN259">259</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="260"><td class="num" id="LN260">260</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="261"><td class="num" id="LN261">261</td><td class="line">  <span class='keyword'>return</span> res;</td></tr>
<tr class="codeline" data-linenumber="262"><td class="num" id="LN262">262</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="263"><td class="num" id="LN263">263</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="264"><td class="num" id="LN264">264</td><td class="line"><span class='comment'>// A note about the lifetime of the various PyTensorType: normally</span></td></tr>
<tr class="codeline" data-linenumber="265"><td class="num" id="LN265">265</td><td class="line"><span class='comment'>// PyTypeObject instances are statically allocated, but we want to create them</span></td></tr>
<tr class="codeline" data-linenumber="266"><td class="num" id="LN266">266</td><td class="line"><span class='comment'>// dynamically at init time, because their exact number depends on</span></td></tr>
<tr class="codeline" data-linenumber="267"><td class="num" id="LN267">267</td><td class="line"><span class='comment'>// torch::utils::all_declared_types(). The memory for each PyTensorType is</span></td></tr>
<tr class="codeline" data-linenumber="268"><td class="num" id="LN268">268</td><td class="line"><span class='comment'>// allocated by initialize_aten_types() and never freed: technically it's a</span></td></tr>
<tr class="codeline" data-linenumber="269"><td class="num" id="LN269">269</td><td class="line"><span class='comment'>// leak, but it's not a problem since we want them to be alive for the whole time</span></td></tr>
<tr class="codeline" data-linenumber="270"><td class="num" id="LN270">270</td><td class="line"><span class='comment'>// of the process anyway.</span></td></tr>
<tr class="codeline" data-linenumber="271"><td class="num" id="LN271">271</td><td class="line"><span class='comment'>//</span></td></tr>
<tr class="codeline" data-linenumber="272"><td class="num" id="LN272">272</td><td class="line"><span class='comment'>// An alternative is to use a std::vector&lt;PyTensorType&gt; instead, and let</span></td></tr>
<tr class="codeline" data-linenumber="273"><td class="num" id="LN273">273</td><td class="line"><span class='comment'>// std::vector to manage the lifetime of its items. This is problematic</span></td></tr>
<tr class="codeline" data-linenumber="274"><td class="num" id="LN274">274</td><td class="line"><span class='comment'>// though, because it means that the memory of PyTensorType is deallocated at</span></td></tr>
<tr class="codeline" data-linenumber="275"><td class="num" id="LN275">275</td><td class="line"><span class='comment'>// some point during the exit: if by chance we have another global destructor</span></td></tr>
<tr class="codeline" data-linenumber="276"><td class="num" id="LN276">276</td><td class="line"><span class='comment'>// and/or atexit() function which tries to access the PyTensorTypes, we risk</span></td></tr>
<tr class="codeline" data-linenumber="277"><td class="num" id="LN277">277</td><td class="line"><span class='comment'>// an use-after-free error. This happens for example if we embed CPython and</span></td></tr>
<tr class="codeline" data-linenumber="278"><td class="num" id="LN278">278</td><td class="line"><span class='comment'>// call Py_Finalize inside an atexit() function which was registered before</span></td></tr>
<tr class="codeline" data-linenumber="279"><td class="num" id="LN279">279</td><td class="line"><span class='comment'>// importing torch.</span></td></tr>
<tr class="codeline" data-linenumber="280"><td class="num" id="LN280">280</td><td class="line"><span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)</span></td></tr>
<tr class="codeline" data-linenumber="281"><td class="num" id="LN281">281</td><td class="line"><span class='keyword'>static</span> std::vector&lt;PyTensorType*&gt; tensor_types;</td></tr>
<tr class="codeline" data-linenumber="282"><td class="num" id="LN282">282</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="283"><td class="num" id="LN283">283</td><td class="line"><span class='keyword'>void</span> set_default_tensor_type(PyTensorType* type) {</td></tr>
<tr class="codeline" data-linenumber="284"><td class="num" id="LN284">284</td><td class="line">  <span class='keyword'>if</span> (!at::isFloatingType(type-&gt;get_scalar_type())) {</td></tr>
<tr class="codeline" data-linenumber="285"><td class="num" id="LN285">285</td><td class="line">    <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"only floating-point types are supported as the default type"</span>);</td></tr>
<tr class="codeline" data-linenumber="286"><td class="num" id="LN286">286</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="287"><td class="num" id="LN287">287</td><td class="line">  <span class='keyword'>if</span> (type-&gt;get_backend() == Backend::Undefined) {</td></tr>
<tr class="codeline" data-linenumber="288"><td class="num" id="LN288">288</td><td class="line">    <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"default type cannot be undefined"</span>);</td></tr>
<tr class="codeline" data-linenumber="289"><td class="num" id="LN289">289</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="290"><td class="num" id="LN290">290</td><td class="line">  <span class='keyword'>if</span> (isSparse(type-&gt;get_backend())) {</td></tr>
<tr class="codeline" data-linenumber="291"><td class="num" id="LN291">291</td><td class="line">    <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"only dense types are supported as the default type"</span>);</td></tr>
<tr class="codeline" data-linenumber="292"><td class="num" id="LN292">292</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="293"><td class="num" id="LN293">293</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="294"><td class="num" id="LN294">294</td><td class="line">  <span class='comment'>// get the storage first, so if it doesn't exist we don't change the default tensor type</span></td></tr>
<tr class="codeline" data-linenumber="295"><td class="num" id="LN295">295</td><td class="line">  THPObjectPtr storage = get_storage_obj(type);</td></tr>
<tr class="codeline" data-linenumber="296"><td class="num" id="LN296">296</td><td class="line">  <span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)</span></td></tr>
<tr class="codeline" data-linenumber="297"><td class="num" id="LN297">297</td><td class="line">  default_tensor_type = type;</td></tr>
<tr class="codeline" data-linenumber="298"><td class="num" id="LN298">298</td><td class="line">  at::set_default_dtype(scalarTypeToTypeMeta(type-&gt;get_scalar_type()));</td></tr>
<tr class="codeline" data-linenumber="299"><td class="num" id="LN299">299</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="300"><td class="num" id="LN300">300</td><td class="line">  <span class='keyword'>auto</span> torch_module = THPObjectPtr(PyImport_ImportModule(<span class='string_literal'>"torch"</span>));</td></tr>
<tr class="codeline" data-linenumber="301"><td class="num" id="LN301">301</td><td class="line">  <span class='keyword'>if</span> (!torch_module) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="302"><td class="num" id="LN302">302</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="303"><td class="num" id="LN303">303</td><td class="line">  <span class='keyword'>if</span> (PyObject_SetAttrString(torch_module.get(), <span class='string_literal'>"Storage"</span>, storage) != 0) {</td></tr>
<tr class="codeline" data-linenumber="304"><td class="num" id="LN304">304</td><td class="line">    <span class='comment'>// technically, we should undo the change of default tensor type.</span></td></tr>
<tr class="codeline" data-linenumber="305"><td class="num" id="LN305">305</td><td class="line">    <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="306"><td class="num" id="LN306">306</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="307"><td class="num" id="LN307">307</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="308"><td class="num" id="LN308">308</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="309"><td class="num" id="LN309">309</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>void</span> initialize_aten_types(std::vector&lt;PyTensorType*&gt;&amp; tensor_types) {</td></tr>
<tr class="codeline" data-linenumber="310"><td class="num" id="LN310">310</td><td class="line">  <span class='comment'>// includes CUDA types even when PyTorch is not built with CUDA</span></td></tr>
<tr class="codeline" data-linenumber="311"><td class="num" id="LN311">311</td><td class="line">  <span class='keyword'>auto</span> declared_types = torch::utils::all_declared_types();</td></tr>
<tr class="codeline" data-linenumber="312"><td class="num" id="LN312">312</td><td class="line">  tensor_types.resize(declared_types.size());</td></tr>
<tr class="codeline" data-linenumber="313"><td class="num" id="LN313">313</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="314"><td class="num" id="LN314">314</td><td class="line">  <span class='keyword'>for</span> (size_t i = 0, end = declared_types.size(); i != end; i++) {</td></tr>
<tr class="codeline" data-linenumber="315"><td class="num" id="LN315">315</td><td class="line">    tensor_types[i] = <span class='keyword'>new</span> PyTensorType();</td></tr>
<tr class="codeline" data-linenumber="316"><td class="num" id="LN316">316</td><td class="line">    <span class='keyword'>auto</span>&amp; tensor_type = *tensor_types[i];</td></tr>
<tr class="codeline" data-linenumber="317"><td class="num" id="LN317">317</td><td class="line">    Backend backend = declared_types[i].first;</td></tr>
<tr class="codeline" data-linenumber="318"><td class="num" id="LN318">318</td><td class="line">    ScalarType scalar_type = declared_types[i].second;</td></tr>
<tr class="codeline" data-linenumber="319"><td class="num" id="LN319">319</td><td class="line">    set_type(tensor_type, backend, scalar_type);</td></tr>
<tr class="codeline" data-linenumber="320"><td class="num" id="LN320">320</td><td class="line">    set_name(tensor_type, get_name(backend, scalar_type));</td></tr>
<tr class="codeline" data-linenumber="321"><td class="num" id="LN321">321</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="322"><td class="num" id="LN322">322</td><td class="line">    <span class='comment'>// Use torch.float32 as the default tensor type</span></td></tr>
<tr class="codeline" data-linenumber="323"><td class="num" id="LN323">323</td><td class="line">    <span class='keyword'>if</span> (backend == Backend::CPU &amp;&amp; scalar_type == at::kFloat) {</td></tr>
<tr class="codeline" data-linenumber="324"><td class="num" id="LN324">324</td><td class="line">      set_default_tensor_type(&amp;tensor_type);</td></tr>
<tr class="codeline" data-linenumber="325"><td class="num" id="LN325">325</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="326"><td class="num" id="LN326">326</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="327"><td class="num" id="LN327">327</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="328"><td class="num" id="LN328">328</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="329"><td class="num" id="LN329">329</td><td class="line"><span class='keyword'>void</span> initialize_python_bindings() {</td></tr>
<tr class="codeline" data-linenumber="330"><td class="num" id="LN330">330</td><td class="line">  <span class='comment'>// Initialize the at::Type* pointers, name, and properties of the PyTensorType</span></td></tr>
<tr class="codeline" data-linenumber="331"><td class="num" id="LN331">331</td><td class="line">  <span class='comment'>// vector. After this call, the vector must not be resized.</span></td></tr>
<tr class="codeline" data-linenumber="332"><td class="num" id="LN332">332</td><td class="line">  initialize_aten_types(tensor_types);</td></tr>
<tr class="codeline" data-linenumber="333"><td class="num" id="LN333">333</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="334"><td class="num" id="LN334">334</td><td class="line">  <span class='comment'>// Initialize the Python metaclass for the torch.FloatTensor, etc. types.</span></td></tr>
<tr class="codeline" data-linenumber="335"><td class="num" id="LN335">335</td><td class="line">  <span class='comment'>// The metaclass handles __instancecheck__ checks and binds the dtype property</span></td></tr>
<tr class="codeline" data-linenumber="336"><td class="num" id="LN336">336</td><td class="line">  <span class='comment'>// on the type objects.</span></td></tr>
<tr class="codeline" data-linenumber="337"><td class="num" id="LN337">337</td><td class="line">  py_initialize_metaclass(metaclass);</td></tr>
<tr class="codeline" data-linenumber="338"><td class="num" id="LN338">338</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="339"><td class="num" id="LN339">339</td><td class="line">  <span class='comment'>// Get the tp_dict of the Variable class. We copy function definitions</span></td></tr>
<tr class="codeline" data-linenumber="340"><td class="num" id="LN340">340</td><td class="line">  <span class='comment'>// onto each Tensor type object so that they can be accessed via e.g.</span></td></tr>
<tr class="codeline" data-linenumber="341"><td class="num" id="LN341">341</td><td class="line">  <span class='comment'>// `torch.FloatTensor.add`.</span></td></tr>
<tr class="codeline" data-linenumber="342"><td class="num" id="LN342">342</td><td class="line">  <span class='keyword'>auto</span> tensor_dict = get_tensor_dict();</td></tr>
<tr class="codeline" data-linenumber="343"><td class="num" id="LN343">343</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="344"><td class="num" id="LN344">344</td><td class="line">  <span class='comment'>// Initialize each Python type object torch.FloatTensor, torch.DoubleTensor, etc.</span></td></tr>
<tr class="codeline" data-linenumber="345"><td class="num" id="LN345">345</td><td class="line">  <span class='keyword'>for</span> (<span class='keyword'>auto</span>&amp; tensor_type : tensor_types) {</td></tr>
<tr class="codeline" data-linenumber="346"><td class="num" id="LN346">346</td><td class="line">    py_initialize_tensor_type(tensor_type-&gt;py_type, tensor_type-&gt;name, tensor_dict.get());</td></tr>
<tr class="codeline" data-linenumber="347"><td class="num" id="LN347">347</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="348"><td class="num" id="LN348">348</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="349"><td class="num" id="LN349">349</td><td class="line">  <span class='comment'>// Add the type objects to their corresponding modules. e.g. torch.FloatTensor</span></td></tr>
<tr class="codeline" data-linenumber="350"><td class="num" id="LN350">350</td><td class="line">  <span class='comment'>// is added to the `torch` module as `FloatTensor`. Also add all the type</span></td></tr>
<tr class="codeline" data-linenumber="351"><td class="num" id="LN351">351</td><td class="line">  <span class='comment'>// objects to the set torch._tensor_classes.</span></td></tr>
<tr class="codeline" data-linenumber="352"><td class="num" id="LN352">352</td><td class="line">  <span class="mrange">py_bind_tensor_types(tensor_types)</span>;</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path1" class="msg msgEvent" style="margin-left:3ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">1</div></td><td>Calling 'py_bind_tensor_types'</td><td><div class="PathNav"><a href="#Path2" title="Next event (2)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="353"><td class="num" id="LN353">353</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="354"><td class="num" id="LN354">354</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="355"><td class="num" id="LN355">355</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>void</span> py_bind_tensor_types(<span class='keyword'>const</span> std::vector&lt;PyTensorType*&gt;&amp; tensor_types) {</td></tr>
<tr class="codeline" data-linenumber="356"><td class="num" id="LN356">356</td><td class="line">  <span class='keyword'>auto</span> torch_module = THPObjectPtr(PyImport_ImportModule(<span class='string_literal'>"torch"</span>));</td></tr>
<tr class="codeline" data-linenumber="357"><td class="num" id="LN357">357</td><td class="line">  <span class='keyword'>if</span> (<span class="mrange">!torch_module</span>) <span class='keyword'>throw</span> python_error();</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path2" class="msg msgEvent" style="margin-left:7ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">2</div></td><td><div class="PathNav"><a href="#Path1" title="Previous event (1)">&#x2190;</a></div></td><td>Assuming the condition is false</td><td><div class="PathNav"><a href="#Path3" title="Next event (3)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path3" class="msg msgControl" style="margin-left:3ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexControl">3</div></td><td><div class="PathNav"><a href="#Path2" title="Previous event (2)">&#x2190;</a></div></td><td>Taking false branch</td><td><div class="PathNav"><a href="#Path4" title="Next event (4)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="358"><td class="num" id="LN358">358</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="359"><td class="num" id="LN359">359</td><td class="line">  <span class='keyword'>auto</span> tensor_classes = THPObjectPtr(<span class="mrange"><span class="mrange">PyObject_GetAttrString(torch_module.get(), <span class='string_literal'>"_tensor_classes"</span>)</span></span>);</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path4" class="msg msgEvent" style="margin-left:38ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">4</div></td><td><div class="PathNav"><a href="#Path3" title="Previous event (3)">&#x2190;</a></div></td><td>Calling 'PyObject_GetAttrString'</td><td><div class="PathNav"><a href="#Path5" title="Next event (5)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path6" class="msg msgEvent" style="margin-left:38ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">6</div></td><td><div class="PathNav"><a href="#Path5" title="Previous event (5)">&#x2190;</a></div></td><td>Returning from 'PyObject_GetAttrString'</td><td><div class="PathNav"><a href="#Path7" title="Next event (7)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="EndPath" class="msg msgEvent" style="margin-left:38ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">11</div></td><td><div class="PathNav"><a href="#Path10" title="Previous event (10)">&#x2190;</a></div></td><td>PyObject ownership leak with reference count of 1</td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="360"><td class="num" id="LN360">360</td><td class="line">  <span class='keyword'>if</span> (<span class="mrange">!tensor_classes</span>) <span class='keyword'>throw</span> python_error();</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path7" class="msg msgEvent" style="margin-left:7ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">7</div></td><td><div class="PathNav"><a href="#Path6" title="Previous event (6)">&#x2190;</a></div></td><td>Assuming the condition is false</td><td><div class="PathNav"><a href="#Path8" title="Next event (8)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path8" class="msg msgControl" style="margin-left:3ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexControl">8</div></td><td><div class="PathNav"><a href="#Path7" title="Previous event (7)">&#x2190;</a></div></td><td>Taking false branch</td><td><div class="PathNav"><a href="#Path9" title="Next event (9)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="361"><td class="num" id="LN361">361</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="362"><td class="num" id="LN362">362</td><td class="line">  <span class='keyword'>for</span> (<span class='keyword'>auto</span>&amp; tensor_type : tensor_types) {</td></tr>
<tr class="codeline" data-linenumber="363"><td class="num" id="LN363">363</td><td class="line">    <span class='keyword'>auto</span> name = std::string(tensor_type-&gt;name);</td></tr>
<tr class="codeline" data-linenumber="364"><td class="num" id="LN364">364</td><td class="line">    <span class='keyword'>auto</span> idx = name.rfind('.');</td></tr>
<tr class="codeline" data-linenumber="365"><td class="num" id="LN365">365</td><td class="line">    <span class='keyword'>auto</span> type_name = name.substr(idx + 1);</td></tr>
<tr class="codeline" data-linenumber="366"><td class="num" id="LN366">366</td><td class="line">    <span class='keyword'>auto</span> module_name = name.substr(0, idx);</td></tr>
<tr class="codeline" data-linenumber="367"><td class="num" id="LN367">367</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="368"><td class="num" id="LN368">368</td><td class="line">    <span class='keyword'>auto</span> module_obj = THPObjectPtr(PyImport_ImportModule(module_name.c_str()));</td></tr>
<tr class="codeline" data-linenumber="369"><td class="num" id="LN369">369</td><td class="line">    <span class='keyword'>if</span> (<span class="mrange">!module_obj</span>) <span class='keyword'>throw</span> python_error();</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path9" class="msg msgEvent" style="margin-left:9ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">9</div></td><td><div class="PathNav"><a href="#Path8" title="Previous event (8)">&#x2190;</a></div></td><td>Assuming the condition is true</td><td><div class="PathNav"><a href="#Path10" title="Next event (10)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path10" class="msg msgControl" style="margin-left:5ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexControl">10</div></td><td><div class="PathNav"><a href="#Path9" title="Previous event (9)">&#x2190;</a></div></td><td>Taking true branch</td><td><div class="PathNav"><a href="#EndPath" title="Next event (11)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="370"><td class="num" id="LN370">370</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="371"><td class="num" id="LN371">371</td><td class="line">    PyObject* type_obj = (PyObject*)tensor_type;</td></tr>
<tr class="codeline" data-linenumber="372"><td class="num" id="LN372">372</td><td class="line">    <span class='macro'>Py_INCREF(type_obj)<span class='macro_popup'>_Py_INCREF(((PyObject*)(type_obj)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="373"><td class="num" id="LN373">373</td><td class="line">    <span class='keyword'>if</span> (PyModule_AddObject(module_obj.get(), type_name.c_str(), type_obj) &lt; 0) {</td></tr>
<tr class="codeline" data-linenumber="374"><td class="num" id="LN374">374</td><td class="line">      <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="375"><td class="num" id="LN375">375</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="376"><td class="num" id="LN376">376</td><td class="line">    <span class='keyword'>if</span> (PySet_Add(tensor_classes.get(), type_obj) &lt; 0) {</td></tr>
<tr class="codeline" data-linenumber="377"><td class="num" id="LN377">377</td><td class="line">      <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="378"><td class="num" id="LN378">378</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="379"><td class="num" id="LN379">379</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="380"><td class="num" id="LN380">380</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="381"><td class="num" id="LN381">381</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="382"><td class="num" id="LN382">382</td><td class="line"><span class='keyword'>static</span> <span class='keyword'>bool</span> PyTensorType_Check(PyObject* obj) {</td></tr>
<tr class="codeline" data-linenumber="383"><td class="num" id="LN383">383</td><td class="line">  <span class='keyword'>auto</span> it = std::find_if(tensor_types.begin(), tensor_types.end(),</td></tr>
<tr class="codeline" data-linenumber="384"><td class="num" id="LN384">384</td><td class="line">    [obj](PyTensorType *x) {</td></tr>
<tr class="codeline" data-linenumber="385"><td class="num" id="LN385">385</td><td class="line">      <span class='keyword'>return</span> (PyObject*)x == obj;</td></tr>
<tr class="codeline" data-linenumber="386"><td class="num" id="LN386">386</td><td class="line">    });</td></tr>
<tr class="codeline" data-linenumber="387"><td class="num" id="LN387">387</td><td class="line">  <span class='keyword'>return</span> it != tensor_types.end();</td></tr>
<tr class="codeline" data-linenumber="388"><td class="num" id="LN388">388</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="389"><td class="num" id="LN389">389</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="390"><td class="num" id="LN390">390</td><td class="line"><span class='keyword'>void</span> py_set_default_tensor_type(PyObject* obj) {</td></tr>
<tr class="codeline" data-linenumber="391"><td class="num" id="LN391">391</td><td class="line">  <span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-init-variables)</span></td></tr>
<tr class="codeline" data-linenumber="392"><td class="num" id="LN392">392</td><td class="line">  PyTensorType *type;</td></tr>
<tr class="codeline" data-linenumber="393"><td class="num" id="LN393">393</td><td class="line">  <span class='keyword'>if</span> (PyTensorType_Check(obj)) {</td></tr>
<tr class="codeline" data-linenumber="394"><td class="num" id="LN394">394</td><td class="line">    type = (PyTensorType*)obj;</td></tr>
<tr class="codeline" data-linenumber="395"><td class="num" id="LN395">395</td><td class="line">  } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="396"><td class="num" id="LN396">396</td><td class="line">    <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"invalid type object"</span>);</td></tr>
<tr class="codeline" data-linenumber="397"><td class="num" id="LN397">397</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="398"><td class="num" id="LN398">398</td><td class="line">  <span class='keyword'>if</span> (type-&gt;is_cuda &amp;&amp; !torch::utils::cuda_enabled()) {</td></tr>
<tr class="codeline" data-linenumber="399"><td class="num" id="LN399">399</td><td class="line">    <span class='keyword'>throw</span> unavailable_type(*type);</td></tr>
<tr class="codeline" data-linenumber="400"><td class="num" id="LN400">400</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="401"><td class="num" id="LN401">401</td><td class="line">  set_default_tensor_type(type);</td></tr>
<tr class="codeline" data-linenumber="402"><td class="num" id="LN402">402</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="403"><td class="num" id="LN403">403</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="404"><td class="num" id="LN404">404</td><td class="line"><span class='keyword'>void</span> py_set_default_dtype(PyObject* obj) {</td></tr>
<tr class="codeline" data-linenumber="405"><td class="num" id="LN405">405</td><td class="line">  <span class='keyword'>if</span> (THPDtype_Check(obj)) {</td></tr>
<tr class="codeline" data-linenumber="406"><td class="num" id="LN406">406</td><td class="line">    <span class='keyword'>auto</span> scalar_type = ((THPDtype*)obj)-&gt;scalar_type;</td></tr>
<tr class="codeline" data-linenumber="407"><td class="num" id="LN407">407</td><td class="line">    <span class='keyword'>auto</span> backend = default_tensor_type-&gt;get_backend();</td></tr>
<tr class="codeline" data-linenumber="408"><td class="num" id="LN408">408</td><td class="line">    <span class='keyword'>auto</span> it = std::find_if(tensor_types.begin(), tensor_types.end(),</td></tr>
<tr class="codeline" data-linenumber="409"><td class="num" id="LN409">409</td><td class="line">      [backend, scalar_type](PyTensorType *x) {</td></tr>
<tr class="codeline" data-linenumber="410"><td class="num" id="LN410">410</td><td class="line">        <span class='keyword'>return</span> x-&gt;get_backend() == backend &amp;&amp; x-&gt;get_scalar_type() == scalar_type;</td></tr>
<tr class="codeline" data-linenumber="411"><td class="num" id="LN411">411</td><td class="line">      });</td></tr>
<tr class="codeline" data-linenumber="412"><td class="num" id="LN412">412</td><td class="line">    set_default_tensor_type(*it);</td></tr>
<tr class="codeline" data-linenumber="413"><td class="num" id="LN413">413</td><td class="line">  } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="414"><td class="num" id="LN414">414</td><td class="line">    <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"invalid dtype object"</span>);</td></tr>
<tr class="codeline" data-linenumber="415"><td class="num" id="LN415">415</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="416"><td class="num" id="LN416">416</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="417"><td class="num" id="LN417">417</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="418"><td class="num" id="LN418">418</td><td class="line">c10::DispatchKey get_default_dispatch_key() {</td></tr>
<tr class="codeline" data-linenumber="419"><td class="num" id="LN419">419</td><td class="line">  <span class='macro'>AT_ASSERT(default_tensor_type)<span class='macro_popup'>do { ::c10::detail::deprecated_AT_ASSERT(); if ((__builtin_expect<br>(static_cast&lt;bool&gt;(!(default_tensor_type)), 0))) { ::c10<br>::detail::torchInternalAssertFail( __func__, "../torch/csrc/tensor/python_tensor.cpp"<br>, static_cast&lt;uint32_t&gt;(419), "default_tensor_type" "INTERNAL ASSERT FAILED at "<br> "\"../torch/csrc/tensor/python_tensor.cpp\"" ":" "419" ", please report a bug to PyTorch. "<br>, c10::str()); }; } while (false)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="420"><td class="num" id="LN420">420</td><td class="line">  <span class='keyword'>return</span> default_tensor_type-&gt;get_dispatch_key();</td></tr>
<tr class="codeline" data-linenumber="421"><td class="num" id="LN421">421</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="422"><td class="num" id="LN422">422</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="423"><td class="num" id="LN423">423</td><td class="line">ScalarType get_default_scalar_type() {</td></tr>
<tr class="codeline" data-linenumber="424"><td class="num" id="LN424">424</td><td class="line">  <span class='keyword'>return</span> typeMetaToScalarType(get_default_dtype());</td></tr>
<tr class="codeline" data-linenumber="425"><td class="num" id="LN425">425</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="426"><td class="num" id="LN426">426</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="427"><td class="num" id="LN427">427</td><td class="line">}} <span class='comment'>// namespace torch::tensors</span></td></tr>
</table><hr class=divider>
<div id=File172922>
<div class=FileNav><a href="#File1">&#x2190;</a></div><h4 class=FileName>/opt/pyrefcon/lib/pyrefcon/models/models/PyObject_GetAttrString.model</h4>
</div>
<table class="code" data-fileid="172922">
<tr class="codeline" data-linenumber="1"><td class="num" id="LN1">1</td><td class="line"><span class='directive'>#ifndef PyObject_GetAttrString</span></td></tr>
<tr class="codeline" data-linenumber="2"><td class="num" id="LN2">2</td><td class="line"><span class='keyword'>struct</span> _object;</td></tr>
<tr class="codeline" data-linenumber="3"><td class="num" id="LN3">3</td><td class="line"><span class='keyword'>typedef</span> <span class='keyword'>struct</span> _object PyObject;</td></tr>
<tr class="codeline" data-linenumber="4"><td class="num" id="LN4">4</td><td class="line">PyObject* clang_analyzer_PyObject_New_Reference();</td></tr>
<tr class="codeline" data-linenumber="5"><td class="num" id="LN5">5</td><td class="line">PyObject* PyObject_GetAttrString(PyObject *o, <span class='keyword'>const</span> <span class='keyword'>char</span> *attr_name) {</td></tr>
<tr class="codeline" data-linenumber="6"><td class="num" id="LN6">6</td><td class="line">  <span class='keyword'>return</span> <span class="mrange">clang_analyzer_PyObject_New_Reference()</span>;</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path5" class="msg msgEvent" style="margin-left:10ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">5</div></td><td><div class="PathNav"><a href="#Path4" title="Previous event (4)">&#x2190;</a></div></td><td>Setting reference count to 1</td><td><div class="PathNav"><a href="#Path6" title="Next event (6)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="7"><td class="num" id="LN7">7</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="8"><td class="num" id="LN8">8</td><td class="line"><span class='directive'>#else</span></td></tr>
<tr class="codeline" data-linenumber="9"><td class="num" id="LN9">9</td><td class="line"><span class='directive'>#warning "API PyObject_GetAttrString is defined as a macro."</span></td></tr>
<tr class="codeline" data-linenumber="10"><td class="num" id="LN10">10</td><td class="line"><span class='directive'>#endif</span></td></tr></table></body></html>
