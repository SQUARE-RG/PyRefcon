<!doctype html>
<html>
<head>
<title>../torch/csrc/utils/tensor_numpy.cpp</title>

<style type="text/css">
body { color:#000000; background-color:#ffffff }
body { font-family:Helvetica, sans-serif; font-size:10pt }
h1 { font-size:14pt }
.FileName { margin-top: 5px; margin-bottom: 5px; display: inline; }
.FileNav { margin-left: 5px; margin-right: 5px; display: inline; }
.FileNav a { text-decoration:none; font-size: larger; }
.divider { margin-top: 30px; margin-bottom: 30px; height: 15px; }
.divider { background-color: gray; }
.code { border-collapse:collapse; width:100%; }
.code { font-family: "Monospace", monospace; font-size:10pt }
.code { line-height: 1.2em }
.comment { color: green; font-style: oblique }
.keyword { color: blue }
.string_literal { color: red }
.directive { color: darkmagenta }

/* Macros and variables could have pop-up notes hidden by default.
  - Macro pop-up:    expansion of the macro
  - Variable pop-up: value (table) of the variable */
.macro_popup, .variable_popup { display: none; }

/* Pop-up appears on mouse-hover event. */
.macro:hover .macro_popup, .variable:hover .variable_popup {
  display: block;
  padding: 2px;
  -webkit-border-radius:5px;
  -webkit-box-shadow:1px 1px 7px #000;
  border-radius:5px;
  box-shadow:1px 1px 7px #000;
  position: absolute;
  top: -1em;
  left:10em;
  z-index: 1
}

.macro_popup {
  border: 2px solid red;
  background-color:#FFF0F0;
  font-weight: normal;
}

.variable_popup {
  border: 2px solid blue;
  background-color:#F0F0FF;
  font-weight: bold;
  font-family: Helvetica, sans-serif;
  font-size: 9pt;
}

/* Pop-up notes needs a relative position as a base where they pops up. */
.macro, .variable {
  background-color: PaleGoldenRod;
  position: relative;
}
.macro { color: DarkMagenta; }

#tooltiphint {
  position: fixed;
  width: 50em;
  margin-left: -25em;
  left: 50%;
  padding: 10px;
  border: 1px solid #b0b0b0;
  border-radius: 2px;
  box-shadow: 1px 1px 7px black;
  background-color: #c0c0c0;
  z-index: 2;
}

.num { width:2.5em; padding-right:2ex; background-color:#eeeeee }
.num { text-align:right; font-size:8pt }
.num { color:#444444 }
.line { padding-left: 1ex; border-left: 3px solid #ccc }
.line { white-space: pre }
.msg { -webkit-box-shadow:1px 1px 7px #000 }
.msg { box-shadow:1px 1px 7px #000 }
.msg { -webkit-border-radius:5px }
.msg { border-radius:5px }
.msg { font-family:Helvetica, sans-serif; font-size:8pt }
.msg { float:left }
.msg { padding:0.25em 1ex 0.25em 1ex }
.msg { margin-top:10px; margin-bottom:10px }
.msg { font-weight:bold }
.msg { max-width:60em; word-wrap: break-word; white-space: pre-wrap }
.msgT { padding:0x; spacing:0x }
.msgEvent { background-color:#fff8b4; color:#000000 }
.msgControl { background-color:#bbbbbb; color:#000000 }
.msgNote { background-color:#ddeeff; color:#000000 }
.mrange { background-color:#dfddf3 }
.mrange { border-bottom:1px solid #6F9DBE }
.PathIndex { font-weight: bold; padding:0px 5px; margin-right:5px; }
.PathIndex { -webkit-border-radius:8px }
.PathIndex { border-radius:8px }
.PathIndexEvent { background-color:#bfba87 }
.PathIndexControl { background-color:#8c8c8c }
.PathIndexPopUp { background-color: #879abc; }
.PathNav a { text-decoration:none; font-size: larger }
.CodeInsertionHint { font-weight: bold; background-color: #10dd10 }
.CodeRemovalHint { background-color:#de1010 }
.CodeRemovalHint { border-bottom:1px solid #6F9DBE }
.selected{ background-color:orange !important; }

table.simpletable {
  padding: 5px;
  font-size:12pt;
  margin:20px;
  border-collapse: collapse; border-spacing: 0px;
}
td.rowname {
  text-align: right;
  vertical-align: top;
  font-weight: bold;
  color:#444444;
  padding-right:2ex;
}

/* Hidden text. */
input.spoilerhider + label {
  cursor: pointer;
  text-decoration: underline;
  display: block;
}
input.spoilerhider {
 display: none;
}
input.spoilerhider ~ .spoiler {
  overflow: hidden;
  margin: 10px auto 0;
  height: 0;
  opacity: 0;
}
input.spoilerhider:checked + label + .spoiler{
  height: auto;
  opacity: 1;
}
</style>
</head>
<body>
<!-- BUGDESC PyObject ownership leak with reference count of 1 -->

<!-- BUGTYPE Non-Zero Dead Object -->

<!-- BUGCATEGORY Python Memory Error -->

<!-- BUGFILE /tmp/pyrefcon/pytorch/build/../torch/csrc/utils/tensor_numpy.cpp -->

<!-- FILENAME tensor_numpy.cpp -->

<!-- FUNCTIONNAME tensor_from_cuda_array_interface -->

<!-- ISSUEHASHCONTENTOFLINEINCONTEXT cdabd9b4055159d02286bc899f53e0d7 -->

<!-- BUGLINE 298 -->

<!-- BUGCOLUMN 33 -->

<!-- BUGPATHLENGTH 9 -->

<!-- BUGMETAEND -->
<!-- REPORTHEADER -->
<h3>Bug Summary</h3>
<table class="simpletable">
<tr><td class="rowname">File:</td><td>build/../torch/csrc/utils/tensor_numpy.cpp</td></tr>
<tr><td class="rowname">Warning:</td><td><a href="#EndPath">line 298, column 33</a><br />PyObject ownership leak with reference count of 1</td></tr>

</table>
<!-- REPORTSUMMARYEXTRA -->
<h3>Annotated Source Code</h3>
<p>Press <a href="#" onclick="toggleHelp(); return false;">'?'</a>
   to see keyboard shortcuts</p>
<input type="checkbox" class="spoilerhider" id="showinvocation" />
<label for="showinvocation" >Show analyzer invocation</label>
<div class="spoiler">clang -cc1 -cc1 -triple x86_64-unknown-linux-gnu -analyze -disable-free -disable-llvm-verifier -discard-value-names -main-file-name tensor_numpy.cpp -analyzer-store=region -analyzer-opt-analyze-nested-blocks -analyzer-checker=core -analyzer-checker=apiModeling -analyzer-checker=unix -analyzer-checker=deadcode -analyzer-checker=cplusplus -analyzer-checker=security.insecureAPI.UncheckedReturn -analyzer-checker=security.insecureAPI.getpw -analyzer-checker=security.insecureAPI.gets -analyzer-checker=security.insecureAPI.mktemp -analyzer-checker=security.insecureAPI.mkstemp -analyzer-checker=security.insecureAPI.vfork -analyzer-checker=nullability.NullPassedToNonnull -analyzer-checker=nullability.NullReturnedFromNonnull -analyzer-output plist -w -analyzer-output=html -analyzer-checker=python -analyzer-disable-checker=deadcode -analyzer-config prune-paths=true,suppress-c++-stdlib=true,suppress-inlined-defensive-checks=false,suppress-null-return-paths=false,crosscheck-with-z3=true,model-path=/opt/pyrefcon/lib/pyrefcon/models/models -analyzer-config experimental-enable-naive-ctu-analysis=true,ctu-dir=/tmp/pyrefcon/pytorch/csa-scan,ctu-index-name=/tmp/pyrefcon/pytorch/csa-scan/externalDefMap.txt,ctu-invocation-list=/tmp/pyrefcon/pytorch/csa-scan/invocations.yaml,display-ctu-progress=false -setup-static-analyzer -analyzer-config-compatibility-mode=true -mrelocation-model pic -pic-level 2 -fhalf-no-semantic-interposition -mframe-pointer=none -relaxed-aliasing -fno-rounding-math -ffp-exception-behavior=ignore -mconstructor-aliases -munwind-tables -target-cpu x86-64 -tune-cpu generic -debugger-tuning=gdb -fcoverage-compilation-dir=/tmp/pyrefcon/pytorch/build -resource-dir /opt/pyrefcon/lib/clang/13.0.0 -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem ../third_party/protobuf/src -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party/XNNPACK/include -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /opt/pyrefcon/lib/pyrefcon/models/python3.8 -isystem /usr/lib/python3/dist-packages/numpy/core/include -isystem ../cmake/../third_party/pybind11/include -isystem /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -isystem /usr/lib/x86_64-linux-gnu/openmpi/include -isystem ../third_party/ideep/mkl-dnn/include -isystem ../third_party/ideep/include -D BUILDING_TESTS -D FMT_HEADER_ONLY=1 -D HAVE_MALLOC_USABLE_SIZE=1 -D HAVE_MMAP=1 -D HAVE_SHM_OPEN=1 -D HAVE_SHM_UNLINK=1 -D MINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -D ONNXIFI_ENABLE_EXT=1 -D ONNX_ML=1 -D ONNX_NAMESPACE=onnx_torch -D THP_BUILD_MAIN_LIB -D USE_C10D -D USE_C10D_GLOO -D USE_C10D_MPI -D USE_DISTRIBUTED -D USE_EXTERNAL_MZCRC -D USE_NUMPY -D USE_RPC -D USE_TENSORPIPE -D USE_VALGRIND -D _FILE_OFFSET_BITS=64 -D torch_python_EXPORTS -I aten/src -I ../aten/src -I . -I ../ -I ../cmake/../third_party/benchmark/include -I caffe2/contrib/aten -I ../third_party/onnx -I third_party/onnx -I ../third_party/foxi -I third_party/foxi -I ../torch/.. -I ../torch/../aten/src -I ../torch/../aten/src/TH -I caffe2/aten/src -I third_party -I ../torch/../third_party/valgrind-headers -I ../torch/../third_party/gloo -I ../torch/../third_party/onnx -I ../torch/csrc -I ../torch/csrc/api/include -I ../torch/lib -I ../torch/lib/libshm -I ../torch/csrc/distributed -I ../torch/csrc/api -I ../c10/.. -I third_party/ideep/mkl-dnn/include -I ../third_party/ideep/mkl-dnn/src/../include -I ../torch/lib/libshm/../../../torch/lib -I ../third_party/fmt/include -D USE_PTHREADPOOL -D NDEBUG -D USE_KINETO -D LIBKINETO_NOCUPTI -D USE_FBGEMM -D USE_QNNPACK -D USE_PYTORCH_QNNPACK -D USE_XNNPACK -D SYMBOLICATE_MOBILE_DEBUG_HANDLE -D HAVE_AVX_CPU_DEFINITION -D HAVE_AVX2_CPU_DEFINITION -D NDEBUG -D NDEBUG -D CAFFE2_USE_GLOO -D HAVE_GCC_GET_CPUID -D USE_AVX -D USE_AVX2 -D TH_HAVE_THREAD -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10 -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/x86_64-linux-gnu/c++/10 -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/backward -internal-isystem /opt/pyrefcon/lib/clang/13.0.0/include -internal-isystem /usr/local/include -internal-isystem /usr/lib/gcc/x86_64-linux-gnu/10/../../../../x86_64-linux-gnu/include -internal-externc-isystem /usr/include/x86_64-linux-gnu -internal-externc-isystem /include -internal-externc-isystem /usr/include -O3 -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -Wno-unused-but-set-variable -Wno-maybe-uninitialized -Werror=format -Werror=cast-function-type -Wno-stringop-overflow -Wno-write-strings -Wno-strict-aliasing -w -std=gnu++14 -fdeprecated-macro -fdebug-compilation-dir=/tmp/pyrefcon/pytorch/build -ferror-limit 19 -fvisibility-inlines-hidden -fopenmp -fopenmp-cuda-parallel-target-regions -pthread -fgnuc-version=4.2.1 -fcxx-exceptions -fexceptions -faligned-allocation -fcolor-diagnostics -vectorize-loops -vectorize-slp -faddrsig -D__GCC_HAVE_DWARF2_CFI_ASM=1 -o /tmp/pyrefcon/pytorch/csa-scan/reports -x c++ ../torch/csrc/utils/tensor_numpy.cpp
</div>
<div id='tooltiphint' hidden="true">
  <p>Keyboard shortcuts: </p>
  <ul>
    <li>Use 'j/k' keys for keyboard navigation</li>
    <li>Use 'Shift+S' to show/hide relevant lines</li>
    <li>Use '?' to toggle this window</li>
  </ul>
  <a href="#" onclick="toggleHelp(); return false;">Close</a>
</div>
<script type='text/javascript'>
var relevant_lines = {"1": {"48": 1, "49": 1, "50": 1, "51": 1, "70": 1, "71": 1, "294": 1, "295": 1, "298": 1, "299": 1, "301": 1, "302": 1}, "135401": {"9": 1, "16": 1}, "142112": {"1462": 1, "1463": 1, "1464": 1, "1465": 1, "1466": 1, "1467": 1, "1469": 1, "1472": 1, "1473": 1, "1474": 1, "1480": 1, "1485": 1, "1494": 1, "1495": 1, "1501": 1, "1507": 1, "1518": 1, "1519": 1, "1530": 1, "1537": 1}, "184642": {"5": 1}, "184644": {"5": 1, "6": 1}, "184646": {"1": 1}};

var filterCounterexample = function (hide) {
  var tables = document.getElementsByClassName("code");
  for (var t=0; t<tables.length; t++) {
    var table = tables[t];
    var file_id = table.getAttribute("data-fileid");
    var lines_in_fid = relevant_lines[file_id];
    if (!lines_in_fid) {
      lines_in_fid = {};
    }
    var lines = table.getElementsByClassName("codeline");
    for (var i=0; i<lines.length; i++) {
        var el = lines[i];
        var lineNo = el.getAttribute("data-linenumber");
        if (!lines_in_fid[lineNo]) {
          if (hide) {
            el.setAttribute("hidden", "");
          } else {
            el.removeAttribute("hidden");
          }
        }
    }
  }
}

window.addEventListener("keydown", function (event) {
  if (event.defaultPrevented) {
    return;
  }
  if (event.key == "S") {
    var checked = document.getElementsByName("showCounterexample")[0].checked;
    filterCounterexample(!checked);
    document.getElementsByName("showCounterexample")[0].checked = !checked;
  } else {
    return;
  }
  event.preventDefault();
}, true);

document.addEventListener("DOMContentLoaded", function() {
    document.querySelector('input[name="showCounterexample"]').onchange=
        function (event) {
      filterCounterexample(this.checked);
    };
});
</script>

<form>
    <input type="checkbox" name="showCounterexample" id="showCounterexample" />
    <label for="showCounterexample">
       Show only relevant lines
    </label>
</form>

<script type='text/javascript'>
var digitMatcher = new RegExp("[0-9]+");

var querySelectorAllArray = function(selector) {
  return Array.prototype.slice.call(
    document.querySelectorAll(selector));
}

document.addEventListener("DOMContentLoaded", function() {
    querySelectorAllArray(".PathNav > a").forEach(
        function(currentValue, currentIndex) {
            var hrefValue = currentValue.getAttribute("href");
            currentValue.onclick = function() {
                scrollTo(document.querySelector(hrefValue));
                return false;
            };
        });
});

var findNum = function() {
    var s = document.querySelector(".selected");
    if (!s || s.id == "EndPath") {
        return 0;
    }
    var out = parseInt(digitMatcher.exec(s.id)[0]);
    return out;
};

var scrollTo = function(el) {
    querySelectorAllArray(".selected").forEach(function(s) {
        s.classList.remove("selected");
    });
    el.classList.add("selected");
    window.scrollBy(0, el.getBoundingClientRect().top -
        (window.innerHeight / 2));
}

var move = function(num, up, numItems) {
  if (num == 1 && up || num == numItems - 1 && !up) {
    return 0;
  } else if (num == 0 && up) {
    return numItems - 1;
  } else if (num == 0 && !up) {
    return 1 % numItems;
  }
  return up ? num - 1 : num + 1;
}

var numToId = function(num) {
  if (num == 0) {
    return document.getElementById("EndPath")
  }
  return document.getElementById("Path" + num);
};

var navigateTo = function(up) {
  var numItems = document.querySelectorAll(
      ".line > .msgEvent, .line > .msgControl").length;
  var currentSelected = findNum();
  var newSelected = move(currentSelected, up, numItems);
  var newEl = numToId(newSelected, numItems);

  // Scroll element into center.
  scrollTo(newEl);
};

window.addEventListener("keydown", function (event) {
  if (event.defaultPrevented) {
    return;
  }
  if (event.key == "j") {
    navigateTo(/*up=*/false);
  } else if (event.key == "k") {
    navigateTo(/*up=*/true);
  } else {
    return;
  }
  event.preventDefault();
}, true);
</script>
  
<script type='text/javascript'>

var toggleHelp = function() {
    var hint = document.querySelector("#tooltiphint");
    var attributeName = "hidden";
    if (hint.hasAttribute(attributeName)) {
      hint.removeAttribute(attributeName);
    } else {
      hint.setAttribute("hidden", "true");
    }
};
window.addEventListener("keydown", function (event) {
  if (event.defaultPrevented) {
    return;
  }
  if (event.key == "?") {
    toggleHelp();
  } else {
    return;
  }
  event.preventDefault();
});
</script>
<div id=File1>
<h4 class=FileName>../torch/csrc/utils/tensor_numpy.cpp</h4>
<div class=FileNav><a href="#File184644">&#x2192;</a></div></div>
<table class="code" data-fileid="1">
<tr class="codeline" data-linenumber="1"><td class="num" id="LN1">1</td><td class="line"><span class='directive'>#include &lt;torch/csrc/THP.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="2"><td class="num" id="LN2">2</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/tensor_numpy.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="3"><td class="num" id="LN3">3</td><td class="line"><span class='directive'>#define WITH_NUMPY_IMPORT_ARRAY</span></td></tr>
<tr class="codeline" data-linenumber="4"><td class="num" id="LN4">4</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/numpy_stub.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="5"><td class="num" id="LN5">5</td><td class="line"><span class='directive'>#include &lt;c10/util/irange.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="6"><td class="num" id="LN6">6</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="7"><td class="num" id="LN7">7</td><td class="line"><span class='directive'>#ifndef <span class='macro'>USE_NUMPY<span class='macro_popup'>1</span></span></span></td></tr>
<tr class="codeline" data-linenumber="8"><td class="num" id="LN8">8</td><td class="line"><span class='keyword'>namespace</span> torch { <span class='keyword'>namespace</span> utils {</td></tr>
<tr class="codeline" data-linenumber="9"><td class="num" id="LN9">9</td><td class="line">PyObject* tensor_to_numpy(<span class='keyword'>const</span> at::Tensor&amp; tensor) {</td></tr>
<tr class="codeline" data-linenumber="10"><td class="num" id="LN10">10</td><td class="line">  <span class='keyword'>throw</span> std::runtime_error(<span class='string_literal'>"PyTorch was compiled without NumPy support"</span>);</td></tr>
<tr class="codeline" data-linenumber="11"><td class="num" id="LN11">11</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="12"><td class="num" id="LN12">12</td><td class="line">at::Tensor tensor_from_numpy(PyObject* obj, <span class='keyword'>bool</span> warn_if_not_writeable<span class='comment'>/*=true*/</span>) {</td></tr>
<tr class="codeline" data-linenumber="13"><td class="num" id="LN13">13</td><td class="line">  <span class='keyword'>throw</span> std::runtime_error(<span class='string_literal'>"PyTorch was compiled without NumPy support"</span>);</td></tr>
<tr class="codeline" data-linenumber="14"><td class="num" id="LN14">14</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="15"><td class="num" id="LN15">15</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="16"><td class="num" id="LN16">16</td><td class="line"><span class='keyword'>bool</span> is_numpy_available() {</td></tr>
<tr class="codeline" data-linenumber="17"><td class="num" id="LN17">17</td><td class="line">  <span class='keyword'>throw</span> std::runtime_error(<span class='string_literal'>"PyTorch was compiled without NumPy support"</span>);</td></tr>
<tr class="codeline" data-linenumber="18"><td class="num" id="LN18">18</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="19"><td class="num" id="LN19">19</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="20"><td class="num" id="LN20">20</td><td class="line"><span class='keyword'>bool</span> is_numpy_int(PyObject* obj) {</td></tr>
<tr class="codeline" data-linenumber="21"><td class="num" id="LN21">21</td><td class="line">  <span class='keyword'>throw</span> std::runtime_error(<span class='string_literal'>"PyTorch was compiled without NumPy support"</span>);</td></tr>
<tr class="codeline" data-linenumber="22"><td class="num" id="LN22">22</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="23"><td class="num" id="LN23">23</td><td class="line"><span class='keyword'>bool</span> is_numpy_scalar(PyObject* obj) {</td></tr>
<tr class="codeline" data-linenumber="24"><td class="num" id="LN24">24</td><td class="line">  <span class='keyword'>throw</span> std::runtime_error(<span class='string_literal'>"PyTorch was compiled without NumPy support"</span>);</td></tr>
<tr class="codeline" data-linenumber="25"><td class="num" id="LN25">25</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="26"><td class="num" id="LN26">26</td><td class="line">at::Tensor tensor_from_cuda_array_interface(PyObject* obj) {</td></tr>
<tr class="codeline" data-linenumber="27"><td class="num" id="LN27">27</td><td class="line">  <span class='keyword'>throw</span> std::runtime_error(<span class='string_literal'>"PyTorch was compiled without NumPy support"</span>);</td></tr>
<tr class="codeline" data-linenumber="28"><td class="num" id="LN28">28</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="29"><td class="num" id="LN29">29</td><td class="line">}}</td></tr>
<tr class="codeline" data-linenumber="30"><td class="num" id="LN30">30</td><td class="line"><span class='directive'>#else</span></td></tr>
<tr class="codeline" data-linenumber="31"><td class="num" id="LN31">31</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="32"><td class="num" id="LN32">32</td><td class="line"><span class='directive'>#include &lt;torch/csrc/DynamicTypes.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="33"><td class="num" id="LN33">33</td><td class="line"><span class='directive'>#include &lt;torch/csrc/Exceptions.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="34"><td class="num" id="LN34">34</td><td class="line"><span class='directive'>#include &lt;torch/csrc/autograd/python_variable.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="35"><td class="num" id="LN35">35</td><td class="line"><span class='directive'>#include &lt;torch/csrc/utils/object_ptr.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="36"><td class="num" id="LN36">36</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="37"><td class="num" id="LN37">37</td><td class="line"><span class='directive'>#include &lt;ATen/ATen.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="38"><td class="num" id="LN38">38</td><td class="line"><span class='directive'>#include &lt;ATen/TensorUtils.h&gt;</span></td></tr>
<tr class="codeline" data-linenumber="39"><td class="num" id="LN39">39</td><td class="line"><span class='directive'>#include &lt;memory&gt;</span></td></tr>
<tr class="codeline" data-linenumber="40"><td class="num" id="LN40">40</td><td class="line"><span class='directive'>#include &lt;sstream&gt;</span></td></tr>
<tr class="codeline" data-linenumber="41"><td class="num" id="LN41">41</td><td class="line"><span class='directive'>#include &lt;stdexcept&gt;</span></td></tr>
<tr class="codeline" data-linenumber="42"><td class="num" id="LN42">42</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="43"><td class="num" id="LN43">43</td><td class="line"><span class='keyword'>using</span> <span class='keyword'>namespace</span> at;</td></tr>
<tr class="codeline" data-linenumber="44"><td class="num" id="LN44">44</td><td class="line"><span class='keyword'>using</span> <span class='keyword'>namespace</span> torch::autograd;</td></tr>
<tr class="codeline" data-linenumber="45"><td class="num" id="LN45">45</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="46"><td class="num" id="LN46">46</td><td class="line"><span class='keyword'>namespace</span> torch { <span class='keyword'>namespace</span> utils {</td></tr>
<tr class="codeline" data-linenumber="47"><td class="num" id="LN47">47</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="48"><td class="num" id="LN48">48</td><td class="line"><span class='keyword'>bool</span> is_numpy_available() {</td></tr>
<tr class="codeline" data-linenumber="49"><td class="num" id="LN49">49</td><td class="line">  <span class='keyword'>static</span> <span class='keyword'>bool</span> available = []() {</td></tr>
<tr class="codeline" data-linenumber="50"><td class="num" id="LN50">50</td><td class="line">    <span class='keyword'>if</span> (_import_array() &gt;= 0) {</td></tr>
<tr class="codeline" data-linenumber="51"><td class="num" id="LN51">51</td><td class="line">      <span class='keyword'>return</span> <span class='keyword'>true</span>;</td></tr>
<tr class="codeline" data-linenumber="52"><td class="num" id="LN52">52</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="53"><td class="num" id="LN53">53</td><td class="line">    <span class='comment'>// Try to get exception message, print warning and return false</span></td></tr>
<tr class="codeline" data-linenumber="54"><td class="num" id="LN54">54</td><td class="line">    std::string message = <span class='string_literal'>"Failed to initialize NumPy"</span>;</td></tr>
<tr class="codeline" data-linenumber="55"><td class="num" id="LN55">55</td><td class="line">    <span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-init-variables)</span></td></tr>
<tr class="codeline" data-linenumber="56"><td class="num" id="LN56">56</td><td class="line">    PyObject *type, *value, *traceback;</td></tr>
<tr class="codeline" data-linenumber="57"><td class="num" id="LN57">57</td><td class="line">    PyErr_Fetch(&amp;type, &amp;value, &amp;traceback);</td></tr>
<tr class="codeline" data-linenumber="58"><td class="num" id="LN58">58</td><td class="line">    <span class='keyword'>if</span> (<span class='keyword'>auto</span> str = value ? PyObject_Str(value) : <span class='keyword'>nullptr</span>) {</td></tr>
<tr class="codeline" data-linenumber="59"><td class="num" id="LN59">59</td><td class="line">      <span class='keyword'>if</span> (<span class='keyword'>auto</span> enc_str = PyUnicode_AsEncodedString(str, <span class='string_literal'>"utf-8"</span>, <span class='string_literal'>"strict"</span>)) {</td></tr>
<tr class="codeline" data-linenumber="60"><td class="num" id="LN60">60</td><td class="line">        <span class='keyword'>if</span> (<span class='keyword'>auto</span> byte_str = <span class='macro'>PyBytes_AS_STRING(enc_str)<span class='macro_popup'>((((PyBytesObject *)(enc_str))-&gt;ob_sval))</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="61"><td class="num" id="LN61">61</td><td class="line">          message += <span class='string_literal'>": "</span> + std::string(byte_str);</td></tr>
<tr class="codeline" data-linenumber="62"><td class="num" id="LN62">62</td><td class="line">        }</td></tr>
<tr class="codeline" data-linenumber="63"><td class="num" id="LN63">63</td><td class="line">        <span class='macro'>Py_XDECREF(enc_str)<span class='macro_popup'>_Py_XDECREF(((PyObject*)(enc_str)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="64"><td class="num" id="LN64">64</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="65"><td class="num" id="LN65">65</td><td class="line">      <span class='macro'>Py_XDECREF(str)<span class='macro_popup'>_Py_XDECREF(((PyObject*)(str)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="66"><td class="num" id="LN66">66</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="67"><td class="num" id="LN67">67</td><td class="line">    PyErr_Clear();</td></tr>
<tr class="codeline" data-linenumber="68"><td class="num" id="LN68">68</td><td class="line">    <span class='macro'>TORCH_WARN(message)<span class='macro_popup'>::c10::Warning::warn( {__func__, "../torch/csrc/utils/tensor_numpy.cpp"<br>, static_cast&lt;uint32_t&gt;(68)}, ::c10::str(message), false<br>)</span></span>;</td></tr>
<tr class="codeline" data-linenumber="69"><td class="num" id="LN69">69</td><td class="line">    <span class='keyword'>return</span> <span class='keyword'>false</span>;</td></tr>
<tr class="codeline" data-linenumber="70"><td class="num" id="LN70">70</td><td class="line">  }();</td></tr>
<tr class="codeline" data-linenumber="71"><td class="num" id="LN71">71</td><td class="line">  <span class='keyword'>return</span> available;</td></tr>
<tr class="codeline" data-linenumber="72"><td class="num" id="LN72">72</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="73"><td class="num" id="LN73">73</td><td class="line"><span class='keyword'>static</span> std::vector&lt;npy_intp&gt; to_numpy_shape(IntArrayRef x) {</td></tr>
<tr class="codeline" data-linenumber="74"><td class="num" id="LN74">74</td><td class="line">  <span class='comment'>// shape and stride conversion from int64_t to npy_intp</span></td></tr>
<tr class="codeline" data-linenumber="75"><td class="num" id="LN75">75</td><td class="line">  <span class='keyword'>auto</span> nelem = x.size();</td></tr>
<tr class="codeline" data-linenumber="76"><td class="num" id="LN76">76</td><td class="line">  <span class='keyword'>auto</span> result = std::vector&lt;npy_intp&gt;(nelem);</td></tr>
<tr class="codeline" data-linenumber="77"><td class="num" id="LN77">77</td><td class="line">  <span class='keyword'>for</span>(<span class='keyword'>const</span> <span class='keyword'>auto</span> i : c10::irange(nelem)) {</td></tr>
<tr class="codeline" data-linenumber="78"><td class="num" id="LN78">78</td><td class="line">    result[i] = <span class='keyword'>static_cast</span>&lt;npy_intp&gt;(x[i]);</td></tr>
<tr class="codeline" data-linenumber="79"><td class="num" id="LN79">79</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="80"><td class="num" id="LN80">80</td><td class="line">  <span class='keyword'>return</span> result;</td></tr>
<tr class="codeline" data-linenumber="81"><td class="num" id="LN81">81</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="82"><td class="num" id="LN82">82</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="83"><td class="num" id="LN83">83</td><td class="line"><span class='keyword'>static</span> std::vector&lt;int64_t&gt; to_aten_shape(<span class='keyword'>int</span> ndim, npy_intp* values) {</td></tr>
<tr class="codeline" data-linenumber="84"><td class="num" id="LN84">84</td><td class="line">  <span class='comment'>// shape and stride conversion from npy_intp to int64_t</span></td></tr>
<tr class="codeline" data-linenumber="85"><td class="num" id="LN85">85</td><td class="line">  <span class='keyword'>auto</span> result = std::vector&lt;int64_t&gt;(ndim);</td></tr>
<tr class="codeline" data-linenumber="86"><td class="num" id="LN86">86</td><td class="line">  <span class='keyword'>for</span>(<span class='keyword'>const</span> <span class='keyword'>auto</span> i : c10::irange(ndim)) {</td></tr>
<tr class="codeline" data-linenumber="87"><td class="num" id="LN87">87</td><td class="line">    result[i] = <span class='keyword'>static_cast</span>&lt;int64_t&gt;(values[i]);</td></tr>
<tr class="codeline" data-linenumber="88"><td class="num" id="LN88">88</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="89"><td class="num" id="LN89">89</td><td class="line">  <span class='keyword'>return</span> result;</td></tr>
<tr class="codeline" data-linenumber="90"><td class="num" id="LN90">90</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="91"><td class="num" id="LN91">91</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="92"><td class="num" id="LN92">92</td><td class="line"><span class='keyword'>static</span> std::vector&lt;int64_t&gt; seq_to_aten_shape(PyObject *py_seq) {</td></tr>
<tr class="codeline" data-linenumber="93"><td class="num" id="LN93">93</td><td class="line">  <span class='keyword'>int</span> ndim = <span class='macro'>PySequence_Length<span class='macro_popup'>PySequence_Size</span></span>(py_seq);</td></tr>
<tr class="codeline" data-linenumber="94"><td class="num" id="LN94">94</td><td class="line">  <span class='keyword'>if</span> (ndim == -1) {</td></tr>
<tr class="codeline" data-linenumber="95"><td class="num" id="LN95">95</td><td class="line">    <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"shape and strides must be sequences"</span>);</td></tr>
<tr class="codeline" data-linenumber="96"><td class="num" id="LN96">96</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="97"><td class="num" id="LN97">97</td><td class="line">  <span class='keyword'>auto</span> result = std::vector&lt;int64_t&gt;(ndim);</td></tr>
<tr class="codeline" data-linenumber="98"><td class="num" id="LN98">98</td><td class="line">  <span class='keyword'>for</span>(<span class='keyword'>const</span> <span class='keyword'>auto</span> i : c10::irange(ndim)) {</td></tr>
<tr class="codeline" data-linenumber="99"><td class="num" id="LN99">99</td><td class="line">    <span class='keyword'>auto</span> item = THPObjectPtr(PySequence_GetItem(py_seq, i));</td></tr>
<tr class="codeline" data-linenumber="100"><td class="num" id="LN100">100</td><td class="line">    <span class='keyword'>if</span> (!item) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="101"><td class="num" id="LN101">101</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="102"><td class="num" id="LN102">102</td><td class="line">    result[i] = PyLong_AsLongLong(item);</td></tr>
<tr class="codeline" data-linenumber="103"><td class="num" id="LN103">103</td><td class="line">    <span class='keyword'>if</span> (result[i] == -1 &amp;&amp; PyErr_Occurred()) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="104"><td class="num" id="LN104">104</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="105"><td class="num" id="LN105">105</td><td class="line">  <span class='keyword'>return</span> result;</td></tr>
<tr class="codeline" data-linenumber="106"><td class="num" id="LN106">106</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="107"><td class="num" id="LN107">107</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="108"><td class="num" id="LN108">108</td><td class="line">PyObject* tensor_to_numpy(<span class='keyword'>const</span> at::Tensor&amp; tensor) {</td></tr>
<tr class="codeline" data-linenumber="109"><td class="num" id="LN109">109</td><td class="line">  <span class='keyword'>if</span> (!is_numpy_available()) {</td></tr>
<tr class="codeline" data-linenumber="110"><td class="num" id="LN110">110</td><td class="line">    <span class='keyword'>throw</span> std::runtime_error(<span class='string_literal'>"Numpy is not available"</span>);</td></tr>
<tr class="codeline" data-linenumber="111"><td class="num" id="LN111">111</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="112"><td class="num" id="LN112">112</td><td class="line">  <span class='keyword'>if</span> (tensor.device().type() != DeviceType::CPU) {</td></tr>
<tr class="codeline" data-linenumber="113"><td class="num" id="LN113">113</td><td class="line">    <span class='keyword'>throw</span> TypeError(</td></tr>
<tr class="codeline" data-linenumber="114"><td class="num" id="LN114">114</td><td class="line">      <span class='string_literal'>"can't convert %s device type tensor to numpy. Use Tensor.cpu() to "</span></td></tr>
<tr class="codeline" data-linenumber="115"><td class="num" id="LN115">115</td><td class="line">      <span class='string_literal'>"copy the tensor to host memory first."</span>, tensor.device().str().c_str());</td></tr>
<tr class="codeline" data-linenumber="116"><td class="num" id="LN116">116</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="117"><td class="num" id="LN117">117</td><td class="line">  <span class='keyword'>if</span> (tensor.layout() != Layout::Strided) {</td></tr>
<tr class="codeline" data-linenumber="118"><td class="num" id="LN118">118</td><td class="line">      <span class='keyword'>throw</span> TypeError(</td></tr>
<tr class="codeline" data-linenumber="119"><td class="num" id="LN119">119</td><td class="line">        <span class='string_literal'>"can't convert %s layout tensor to numpy."</span></td></tr>
<tr class="codeline" data-linenumber="120"><td class="num" id="LN120">120</td><td class="line">        <span class='string_literal'>"convert the tensor to a strided layout first."</span>, c10::str(tensor.layout()).c_str());</td></tr>
<tr class="codeline" data-linenumber="121"><td class="num" id="LN121">121</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="122"><td class="num" id="LN122">122</td><td class="line">  <span class='keyword'>if</span> (at::GradMode::is_enabled() &amp;&amp; tensor.requires_grad()) {</td></tr>
<tr class="codeline" data-linenumber="123"><td class="num" id="LN123">123</td><td class="line">    <span class='keyword'>throw</span> std::runtime_error(</td></tr>
<tr class="codeline" data-linenumber="124"><td class="num" id="LN124">124</td><td class="line">        <span class='string_literal'>"Can't call numpy() on Tensor that requires grad. "</span></td></tr>
<tr class="codeline" data-linenumber="125"><td class="num" id="LN125">125</td><td class="line">        <span class='string_literal'>"Use tensor.detach().numpy() instead."</span>);</td></tr>
<tr class="codeline" data-linenumber="126"><td class="num" id="LN126">126</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="127"><td class="num" id="LN127">127</td><td class="line">  <span class='keyword'>auto</span> dtype = aten_to_numpy_dtype(tensor.scalar_type());</td></tr>
<tr class="codeline" data-linenumber="128"><td class="num" id="LN128">128</td><td class="line">  <span class='keyword'>auto</span> sizes = to_numpy_shape(tensor.sizes());</td></tr>
<tr class="codeline" data-linenumber="129"><td class="num" id="LN129">129</td><td class="line">  <span class='keyword'>auto</span> strides = to_numpy_shape(tensor.strides());</td></tr>
<tr class="codeline" data-linenumber="130"><td class="num" id="LN130">130</td><td class="line">  <span class='comment'>// NumPy strides use bytes. Torch strides use element counts.</span></td></tr>
<tr class="codeline" data-linenumber="131"><td class="num" id="LN131">131</td><td class="line">  <span class='keyword'>auto</span> element_size_in_bytes = tensor.element_size();</td></tr>
<tr class="codeline" data-linenumber="132"><td class="num" id="LN132">132</td><td class="line">  <span class='keyword'>for</span> (<span class='keyword'>auto</span>&amp; stride : strides) {</td></tr>
<tr class="codeline" data-linenumber="133"><td class="num" id="LN133">133</td><td class="line">    stride *= element_size_in_bytes;</td></tr>
<tr class="codeline" data-linenumber="134"><td class="num" id="LN134">134</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="135"><td class="num" id="LN135">135</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="136"><td class="num" id="LN136">136</td><td class="line">  <span class='keyword'>auto</span> array = THPObjectPtr(<span class='macro'>PyArray_New<span class='macro_popup'>(*(PyObject * (*)(PyTypeObject *, int, npy_intp const *, int,<br> npy_intp const *, void *, int, int, PyObject *)) __numpy_array_api<br>[93])</span></span>(</td></tr>
<tr class="codeline" data-linenumber="137"><td class="num" id="LN137">137</td><td class="line">      &amp;<span class='macro'>PyArray_Type<span class='macro_popup'>(*(PyTypeObject *)__numpy_array_api[2])</span></span>,</td></tr>
<tr class="codeline" data-linenumber="138"><td class="num" id="LN138">138</td><td class="line">      tensor.dim(),</td></tr>
<tr class="codeline" data-linenumber="139"><td class="num" id="LN139">139</td><td class="line">      sizes.data(),</td></tr>
<tr class="codeline" data-linenumber="140"><td class="num" id="LN140">140</td><td class="line">      dtype,</td></tr>
<tr class="codeline" data-linenumber="141"><td class="num" id="LN141">141</td><td class="line">      strides.data(),</td></tr>
<tr class="codeline" data-linenumber="142"><td class="num" id="LN142">142</td><td class="line">      tensor.data_ptr(),</td></tr>
<tr class="codeline" data-linenumber="143"><td class="num" id="LN143">143</td><td class="line">      0,</td></tr>
<tr class="codeline" data-linenumber="144"><td class="num" id="LN144">144</td><td class="line">      <span class='macro'>NPY_ARRAY_ALIGNED<span class='macro_popup'>0x0100</span></span> | <span class='macro'>NPY_ARRAY_WRITEABLE<span class='macro_popup'>0x0400</span></span>,</td></tr>
<tr class="codeline" data-linenumber="145"><td class="num" id="LN145">145</td><td class="line">      <span class='keyword'>nullptr</span>));</td></tr>
<tr class="codeline" data-linenumber="146"><td class="num" id="LN146">146</td><td class="line">  <span class='keyword'>if</span> (!array) <span class='keyword'>return</span> <span class='keyword'>nullptr</span>;</td></tr>
<tr class="codeline" data-linenumber="147"><td class="num" id="LN147">147</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="148"><td class="num" id="LN148">148</td><td class="line">  <span class='comment'>// TODO: This attempts to keep the underlying memory alive by setting the base</span></td></tr>
<tr class="codeline" data-linenumber="149"><td class="num" id="LN149">149</td><td class="line">  <span class='comment'>// object of the ndarray to the tensor and disabling resizes on the storage.</span></td></tr>
<tr class="codeline" data-linenumber="150"><td class="num" id="LN150">150</td><td class="line">  <span class='comment'>// This is not sufficient. For example, the tensor's storage may be changed</span></td></tr>
<tr class="codeline" data-linenumber="151"><td class="num" id="LN151">151</td><td class="line">  <span class='comment'>// via Tensor.set_, which can free the underlying memory.</span></td></tr>
<tr class="codeline" data-linenumber="152"><td class="num" id="LN152">152</td><td class="line">  PyObject* py_tensor = THPVariable_Wrap(tensor);</td></tr>
<tr class="codeline" data-linenumber="153"><td class="num" id="LN153">153</td><td class="line">  <span class='keyword'>if</span> (!py_tensor) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="154"><td class="num" id="LN154">154</td><td class="line">  <span class='keyword'>if</span> (<span class='macro'>PyArray_SetBaseObject<span class='macro_popup'>(*(int (*)(PyArrayObject *, PyObject *)) __numpy_array_api[282<br>])</span></span>((PyArrayObject*)array.get(), py_tensor) == -1) {</td></tr>
<tr class="codeline" data-linenumber="155"><td class="num" id="LN155">155</td><td class="line">    <span class='keyword'>return</span> <span class='keyword'>nullptr</span>;</td></tr>
<tr class="codeline" data-linenumber="156"><td class="num" id="LN156">156</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="157"><td class="num" id="LN157">157</td><td class="line">  <span class='comment'>// Use the private storage API</span></td></tr>
<tr class="codeline" data-linenumber="158"><td class="num" id="LN158">158</td><td class="line">  tensor.storage().unsafeGetStorageImpl()-&gt;set_resizable(<span class='keyword'>false</span>);</td></tr>
<tr class="codeline" data-linenumber="159"><td class="num" id="LN159">159</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="160"><td class="num" id="LN160">160</td><td class="line">  <span class='keyword'>return</span> array.release();</td></tr>
<tr class="codeline" data-linenumber="161"><td class="num" id="LN161">161</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="162"><td class="num" id="LN162">162</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="163"><td class="num" id="LN163">163</td><td class="line">at::Tensor tensor_from_numpy(PyObject* obj, <span class='keyword'>bool</span> warn_if_not_writeable<span class='comment'>/*=true*/</span>) {</td></tr>
<tr class="codeline" data-linenumber="164"><td class="num" id="LN164">164</td><td class="line">  <span class='keyword'>if</span> (!is_numpy_available()) {</td></tr>
<tr class="codeline" data-linenumber="165"><td class="num" id="LN165">165</td><td class="line">    <span class='keyword'>throw</span> std::runtime_error(<span class='string_literal'>"Numpy is not available"</span>);</td></tr>
<tr class="codeline" data-linenumber="166"><td class="num" id="LN166">166</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="167"><td class="num" id="LN167">167</td><td class="line">  <span class='keyword'>if</span> (!<span class='macro'>PyArray_Check(obj)<span class='macro_popup'>((((PyObject*)(obj))-&gt;ob_type) == (&amp;(*(PyTypeObject *)<br>__numpy_array_api[2])) || PyType_IsSubtype((((PyObject*)(obj)<br>)-&gt;ob_type), (&amp;(*(PyTypeObject *)__numpy_array_api[2])<br>)))</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="168"><td class="num" id="LN168">168</td><td class="line">    <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"expected np.ndarray (got %s)"</span>, <span class='macro'>Py_TYPE(obj)<span class='macro_popup'>(((PyObject*)(obj))-&gt;ob_type)</span></span>-&gt;tp_name);</td></tr>
<tr class="codeline" data-linenumber="169"><td class="num" id="LN169">169</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="170"><td class="num" id="LN170">170</td><td class="line">  <span class='keyword'>auto</span> array = (PyArrayObject*)obj;</td></tr>
<tr class="codeline" data-linenumber="171"><td class="num" id="LN171">171</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="172"><td class="num" id="LN172">172</td><td class="line">  <span class='comment'>// warn_if_not_writable is true when a copy of numpy variable is created.</span></td></tr>
<tr class="codeline" data-linenumber="173"><td class="num" id="LN173">173</td><td class="line">  <span class='comment'>// the warning is suppressed when a copy is being created.</span></td></tr>
<tr class="codeline" data-linenumber="174"><td class="num" id="LN174">174</td><td class="line">  <span class='keyword'>if</span> (!<span class='macro'>PyArray_ISWRITEABLE(array)<span class='macro_popup'>PyArray_CHKFLAGS((array), 0x0400)</span></span> &amp;&amp; warn_if_not_writeable) {</td></tr>
<tr class="codeline" data-linenumber="175"><td class="num" id="LN175">175</td><td class="line">    <span class='macro'>TORCH_WARN_ONCE(<span class='macro_popup'>if (::c10::Warning::get_warnAlways()) { ::c10::Warning::warn(<br> {__func__, "../torch/csrc/utils/tensor_numpy.cpp", static_cast<br>&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); } else {<br> __attribute__((__unused__)) static const auto torch_warn_once_4<br> = [&amp;] { ::c10::Warning::warn( {__func__, "../torch/csrc/utils/tensor_numpy.cpp"<br>, static_cast&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); return true<br>; }(); }</span></span></td></tr>
<tr class="codeline" data-linenumber="176"><td class="num" id="LN176">176</td><td class="line">      <span class='string_literal'><span class='macro'>"The given NumPy array is not writeable, and PyTorch does "<span class='macro_popup'>if (::c10::Warning::get_warnAlways()) { ::c10::Warning::warn(<br> {__func__, "../torch/csrc/utils/tensor_numpy.cpp", static_cast<br>&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); } else {<br> __attribute__((__unused__)) static const auto torch_warn_once_4<br> = [&amp;] { ::c10::Warning::warn( {__func__, "../torch/csrc/utils/tensor_numpy.cpp"<br>, static_cast&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); return true<br>; }(); }</span></span></span></td></tr>
<tr class="codeline" data-linenumber="177"><td class="num" id="LN177">177</td><td class="line">      <span class='string_literal'><span class='macro'>"not support non-writeable tensors. This means you can write to the "<span class='macro_popup'>if (::c10::Warning::get_warnAlways()) { ::c10::Warning::warn(<br> {__func__, "../torch/csrc/utils/tensor_numpy.cpp", static_cast<br>&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); } else {<br> __attribute__((__unused__)) static const auto torch_warn_once_4<br> = [&amp;] { ::c10::Warning::warn( {__func__, "../torch/csrc/utils/tensor_numpy.cpp"<br>, static_cast&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); return true<br>; }(); }</span></span></span></td></tr>
<tr class="codeline" data-linenumber="178"><td class="num" id="LN178">178</td><td class="line">      <span class='string_literal'><span class='macro'>"underlying (supposedly non-writeable) NumPy array using the tensor. "<span class='macro_popup'>if (::c10::Warning::get_warnAlways()) { ::c10::Warning::warn(<br> {__func__, "../torch/csrc/utils/tensor_numpy.cpp", static_cast<br>&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); } else {<br> __attribute__((__unused__)) static const auto torch_warn_once_4<br> = [&amp;] { ::c10::Warning::warn( {__func__, "../torch/csrc/utils/tensor_numpy.cpp"<br>, static_cast&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); return true<br>; }(); }</span></span></span></td></tr>
<tr class="codeline" data-linenumber="179"><td class="num" id="LN179">179</td><td class="line">      <span class='string_literal'><span class='macro'>"You may want to copy the array to protect its data or make it writeable "<span class='macro_popup'>if (::c10::Warning::get_warnAlways()) { ::c10::Warning::warn(<br> {__func__, "../torch/csrc/utils/tensor_numpy.cpp", static_cast<br>&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); } else {<br> __attribute__((__unused__)) static const auto torch_warn_once_4<br> = [&amp;] { ::c10::Warning::warn( {__func__, "../torch/csrc/utils/tensor_numpy.cpp"<br>, static_cast&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); return true<br>; }(); }</span></span></span></td></tr>
<tr class="codeline" data-linenumber="180"><td class="num" id="LN180">180</td><td class="line">      <span class='string_literal'><span class='macro'>"before converting it to a tensor. This type of warning will be "<span class='macro_popup'>if (::c10::Warning::get_warnAlways()) { ::c10::Warning::warn(<br> {__func__, "../torch/csrc/utils/tensor_numpy.cpp", static_cast<br>&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); } else {<br> __attribute__((__unused__)) static const auto torch_warn_once_4<br> = [&amp;] { ::c10::Warning::warn( {__func__, "../torch/csrc/utils/tensor_numpy.cpp"<br>, static_cast&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); return true<br>; }(); }</span></span></span></td></tr>
<tr class="codeline" data-linenumber="181"><td class="num" id="LN181">181</td><td class="line">      <span class='string_literal'><span class='macro'>"suppressed for the rest of this program."</span>)<span class='macro_popup'>if (::c10::Warning::get_warnAlways()) { ::c10::Warning::warn(<br> {__func__, "../torch/csrc/utils/tensor_numpy.cpp", static_cast<br>&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); } else {<br> __attribute__((__unused__)) static const auto torch_warn_once_4<br> = [&amp;] { ::c10::Warning::warn( {__func__, "../torch/csrc/utils/tensor_numpy.cpp"<br>, static_cast&lt;uint32_t&gt;(181)}, ::c10::str("The given NumPy array is not writeable, and PyTorch does "<br> "not support non-writeable tensors. This means you can write to the "<br> "underlying (supposedly non-writeable) NumPy array using the tensor. "<br> "You may want to copy the array to protect its data or make it writeable "<br> "before converting it to a tensor. This type of warning will be "<br> "suppressed for the rest of this program."), false); return true<br>; }(); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="182"><td class="num" id="LN182">182</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="183"><td class="num" id="LN183">183</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="184"><td class="num" id="LN184">184</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="185"><td class="num" id="LN185">185</td><td class="line">  <span class='keyword'>int</span> ndim = PyArray_NDIM(array);</td></tr>
<tr class="codeline" data-linenumber="186"><td class="num" id="LN186">186</td><td class="line">  <span class='keyword'>auto</span> sizes = to_aten_shape(ndim, PyArray_DIMS(array));</td></tr>
<tr class="codeline" data-linenumber="187"><td class="num" id="LN187">187</td><td class="line">  <span class='keyword'>auto</span> strides = to_aten_shape(ndim, PyArray_STRIDES(array));</td></tr>
<tr class="codeline" data-linenumber="188"><td class="num" id="LN188">188</td><td class="line">  <span class='comment'>// NumPy strides use bytes. Torch strides use element counts.</span></td></tr>
<tr class="codeline" data-linenumber="189"><td class="num" id="LN189">189</td><td class="line">  <span class='keyword'>auto</span> element_size_in_bytes = PyArray_ITEMSIZE(array);</td></tr>
<tr class="codeline" data-linenumber="190"><td class="num" id="LN190">190</td><td class="line">  <span class='keyword'>for</span> (<span class='keyword'>auto</span>&amp; stride : strides) {</td></tr>
<tr class="codeline" data-linenumber="191"><td class="num" id="LN191">191</td><td class="line">    <span class='keyword'>if</span> (stride%element_size_in_bytes != 0) {</td></tr>
<tr class="codeline" data-linenumber="192"><td class="num" id="LN192">192</td><td class="line">      <span class='keyword'>throw</span> ValueError(</td></tr>
<tr class="codeline" data-linenumber="193"><td class="num" id="LN193">193</td><td class="line">        <span class='string_literal'>"given numpy array strides not a multiple of the element byte size. "</span></td></tr>
<tr class="codeline" data-linenumber="194"><td class="num" id="LN194">194</td><td class="line">        <span class='string_literal'>"Copy the numpy array to reallocate the memory."</span>);</td></tr>
<tr class="codeline" data-linenumber="195"><td class="num" id="LN195">195</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="196"><td class="num" id="LN196">196</td><td class="line">    stride /= element_size_in_bytes;</td></tr>
<tr class="codeline" data-linenumber="197"><td class="num" id="LN197">197</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="198"><td class="num" id="LN198">198</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="199"><td class="num" id="LN199">199</td><td class="line">  size_t storage_size = 1;</td></tr>
<tr class="codeline" data-linenumber="200"><td class="num" id="LN200">200</td><td class="line">  <span class='keyword'>for</span>(<span class='keyword'>const</span> <span class='keyword'>auto</span> i : c10::irange(ndim)) {</td></tr>
<tr class="codeline" data-linenumber="201"><td class="num" id="LN201">201</td><td class="line">    <span class='keyword'>if</span> (strides[i] &lt; 0) {</td></tr>
<tr class="codeline" data-linenumber="202"><td class="num" id="LN202">202</td><td class="line">      <span class='keyword'>throw</span> ValueError(</td></tr>
<tr class="codeline" data-linenumber="203"><td class="num" id="LN203">203</td><td class="line">          <span class='string_literal'>"At least one stride in the given numpy array is negative, "</span></td></tr>
<tr class="codeline" data-linenumber="204"><td class="num" id="LN204">204</td><td class="line">          <span class='string_literal'>"and tensors with negative strides are not currently supported. "</span></td></tr>
<tr class="codeline" data-linenumber="205"><td class="num" id="LN205">205</td><td class="line">          <span class='string_literal'>"(You can probably work around this by making a copy of your array "</span></td></tr>
<tr class="codeline" data-linenumber="206"><td class="num" id="LN206">206</td><td class="line">          <span class='string_literal'>" with array.copy().) "</span>);</td></tr>
<tr class="codeline" data-linenumber="207"><td class="num" id="LN207">207</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="208"><td class="num" id="LN208">208</td><td class="line">    <span class='comment'>// XXX: this won't work for negative strides</span></td></tr>
<tr class="codeline" data-linenumber="209"><td class="num" id="LN209">209</td><td class="line">    storage_size += (sizes[i] - 1) * strides[i];</td></tr>
<tr class="codeline" data-linenumber="210"><td class="num" id="LN210">210</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="211"><td class="num" id="LN211">211</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="212"><td class="num" id="LN212">212</td><td class="line">  <span class='keyword'>void</span>* data_ptr = PyArray_DATA(array);</td></tr>
<tr class="codeline" data-linenumber="213"><td class="num" id="LN213">213</td><td class="line">  <span class='keyword'>if</span> (!<span class='macro'>PyArray_EquivByteorders(PyArray_DESCR(array)-&gt;byteorder, NPY_NATIVE)<span class='macro_popup'>(((PyArray_DESCR(array)-&gt;byteorder) == ('=')) || (((PyArray_DESCR<br>(array)-&gt;byteorder) != '&gt;') == (('=') != '&gt;')))</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="214"><td class="num" id="LN214">214</td><td class="line">    <span class='keyword'>throw</span> ValueError(</td></tr>
<tr class="codeline" data-linenumber="215"><td class="num" id="LN215">215</td><td class="line">        <span class='string_literal'>"given numpy array has byte order different from the native byte order. "</span></td></tr>
<tr class="codeline" data-linenumber="216"><td class="num" id="LN216">216</td><td class="line">        <span class='string_literal'>"Conversion between byte orders is currently not supported."</span>);</td></tr>
<tr class="codeline" data-linenumber="217"><td class="num" id="LN217">217</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="218"><td class="num" id="LN218">218</td><td class="line">  <span class='macro'>Py_INCREF(obj)<span class='macro_popup'>_Py_INCREF(((PyObject*)(obj)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="219"><td class="num" id="LN219">219</td><td class="line">  <span class='keyword'>return</span> at::from_blob(</td></tr>
<tr class="codeline" data-linenumber="220"><td class="num" id="LN220">220</td><td class="line">      data_ptr,</td></tr>
<tr class="codeline" data-linenumber="221"><td class="num" id="LN221">221</td><td class="line">      sizes,</td></tr>
<tr class="codeline" data-linenumber="222"><td class="num" id="LN222">222</td><td class="line">      strides,</td></tr>
<tr class="codeline" data-linenumber="223"><td class="num" id="LN223">223</td><td class="line">      [obj](<span class='keyword'>void</span>* data) {</td></tr>
<tr class="codeline" data-linenumber="224"><td class="num" id="LN224">224</td><td class="line">        pybind11::gil_scoped_acquire gil;</td></tr>
<tr class="codeline" data-linenumber="225"><td class="num" id="LN225">225</td><td class="line">        <span class='macro'>Py_DECREF(obj)<span class='macro_popup'>_Py_DECREF(((PyObject*)(obj)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="226"><td class="num" id="LN226">226</td><td class="line">      },</td></tr>
<tr class="codeline" data-linenumber="227"><td class="num" id="LN227">227</td><td class="line">      at::device(kCPU).dtype(numpy_dtype_to_aten(PyArray_TYPE(array)))</td></tr>
<tr class="codeline" data-linenumber="228"><td class="num" id="LN228">228</td><td class="line">  );</td></tr>
<tr class="codeline" data-linenumber="229"><td class="num" id="LN229">229</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="230"><td class="num" id="LN230">230</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="231"><td class="num" id="LN231">231</td><td class="line"><span class='keyword'>int</span> aten_to_numpy_dtype(<span class='keyword'>const</span> ScalarType scalar_type) {</td></tr>
<tr class="codeline" data-linenumber="232"><td class="num" id="LN232">232</td><td class="line">  <span class='keyword'>switch</span> (scalar_type) {</td></tr>
<tr class="codeline" data-linenumber="233"><td class="num" id="LN233">233</td><td class="line">    <span class='keyword'>case</span> kDouble: <span class='keyword'>return</span> NPY_DOUBLE;</td></tr>
<tr class="codeline" data-linenumber="234"><td class="num" id="LN234">234</td><td class="line">    <span class='keyword'>case</span> kFloat: <span class='keyword'>return</span> NPY_FLOAT;</td></tr>
<tr class="codeline" data-linenumber="235"><td class="num" id="LN235">235</td><td class="line">    <span class='keyword'>case</span> kHalf: <span class='keyword'>return</span> NPY_HALF;</td></tr>
<tr class="codeline" data-linenumber="236"><td class="num" id="LN236">236</td><td class="line">    <span class='keyword'>case</span> kComplexDouble: <span class='keyword'>return</span> <span class='macro'>NPY_COMPLEX128<span class='macro_popup'>NPY_CDOUBLE</span></span>;</td></tr>
<tr class="codeline" data-linenumber="237"><td class="num" id="LN237">237</td><td class="line">    <span class='keyword'>case</span> kComplexFloat: <span class='keyword'>return</span> <span class='macro'>NPY_COMPLEX64<span class='macro_popup'>NPY_CFLOAT</span></span>;</td></tr>
<tr class="codeline" data-linenumber="238"><td class="num" id="LN238">238</td><td class="line">    <span class='keyword'>case</span> kLong: <span class='keyword'>return</span> <span class='macro'>NPY_INT64<span class='macro_popup'>NPY_LONG</span></span>;</td></tr>
<tr class="codeline" data-linenumber="239"><td class="num" id="LN239">239</td><td class="line">    <span class='keyword'>case</span> kInt: <span class='keyword'>return</span> <span class='macro'>NPY_INT32<span class='macro_popup'>NPY_INT</span></span>;</td></tr>
<tr class="codeline" data-linenumber="240"><td class="num" id="LN240">240</td><td class="line">    <span class='keyword'>case</span> kShort: <span class='keyword'>return</span> <span class='macro'>NPY_INT16<span class='macro_popup'>NPY_SHORT</span></span>;</td></tr>
<tr class="codeline" data-linenumber="241"><td class="num" id="LN241">241</td><td class="line">    <span class='keyword'>case</span> kChar: <span class='keyword'>return</span> <span class='macro'>NPY_INT8<span class='macro_popup'>NPY_BYTE</span></span>;</td></tr>
<tr class="codeline" data-linenumber="242"><td class="num" id="LN242">242</td><td class="line">    <span class='keyword'>case</span> kByte: <span class='keyword'>return</span> <span class='macro'>NPY_UINT8<span class='macro_popup'>NPY_UBYTE</span></span>;</td></tr>
<tr class="codeline" data-linenumber="243"><td class="num" id="LN243">243</td><td class="line">    <span class='keyword'>case</span> kBool: <span class='keyword'>return</span> NPY_BOOL;</td></tr>
<tr class="codeline" data-linenumber="244"><td class="num" id="LN244">244</td><td class="line">    <span class='keyword'>default</span>:</td></tr>
<tr class="codeline" data-linenumber="245"><td class="num" id="LN245">245</td><td class="line">      <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"Got unsupported ScalarType %s"</span>, toString(scalar_type));</td></tr>
<tr class="codeline" data-linenumber="246"><td class="num" id="LN246">246</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="247"><td class="num" id="LN247">247</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="248"><td class="num" id="LN248">248</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="249"><td class="num" id="LN249">249</td><td class="line">ScalarType numpy_dtype_to_aten(<span class='keyword'>int</span> dtype) {</td></tr>
<tr class="codeline" data-linenumber="250"><td class="num" id="LN250">250</td><td class="line">  <span class='keyword'>switch</span> (dtype) {</td></tr>
<tr class="codeline" data-linenumber="251"><td class="num" id="LN251">251</td><td class="line">    <span class='keyword'>case</span> NPY_DOUBLE: <span class='keyword'>return</span> kDouble;</td></tr>
<tr class="codeline" data-linenumber="252"><td class="num" id="LN252">252</td><td class="line">    <span class='keyword'>case</span> NPY_FLOAT: <span class='keyword'>return</span> kFloat;</td></tr>
<tr class="codeline" data-linenumber="253"><td class="num" id="LN253">253</td><td class="line">    <span class='keyword'>case</span> NPY_HALF: <span class='keyword'>return</span> kHalf;</td></tr>
<tr class="codeline" data-linenumber="254"><td class="num" id="LN254">254</td><td class="line">    <span class='keyword'>case</span> <span class='macro'>NPY_COMPLEX64<span class='macro_popup'>NPY_CFLOAT</span></span>: <span class='keyword'>return</span> kComplexFloat;</td></tr>
<tr class="codeline" data-linenumber="255"><td class="num" id="LN255">255</td><td class="line">    <span class='keyword'>case</span> <span class='macro'>NPY_COMPLEX128<span class='macro_popup'>NPY_CDOUBLE</span></span>: <span class='keyword'>return</span> kComplexDouble;</td></tr>
<tr class="codeline" data-linenumber="256"><td class="num" id="LN256">256</td><td class="line">    <span class='keyword'>case</span> <span class='macro'>NPY_INT16<span class='macro_popup'>NPY_SHORT</span></span>: <span class='keyword'>return</span> kShort;</td></tr>
<tr class="codeline" data-linenumber="257"><td class="num" id="LN257">257</td><td class="line">    <span class='keyword'>case</span> <span class='macro'>NPY_INT8<span class='macro_popup'>NPY_BYTE</span></span>: <span class='keyword'>return</span> kChar;</td></tr>
<tr class="codeline" data-linenumber="258"><td class="num" id="LN258">258</td><td class="line">    <span class='keyword'>case</span> <span class='macro'>NPY_UINT8<span class='macro_popup'>NPY_UBYTE</span></span>: <span class='keyword'>return</span> kByte;</td></tr>
<tr class="codeline" data-linenumber="259"><td class="num" id="LN259">259</td><td class="line">    <span class='keyword'>case</span> NPY_BOOL: <span class='keyword'>return</span> kBool;</td></tr>
<tr class="codeline" data-linenumber="260"><td class="num" id="LN260">260</td><td class="line">    <span class='keyword'>default</span>:</td></tr>
<tr class="codeline" data-linenumber="261"><td class="num" id="LN261">261</td><td class="line">      <span class='comment'>// Workaround: MSVC does not support two switch cases that have the same value</span></td></tr>
<tr class="codeline" data-linenumber="262"><td class="num" id="LN262">262</td><td class="line">      <span class='keyword'>if</span> (dtype == NPY_INT || dtype == <span class='macro'>NPY_INT32<span class='macro_popup'>NPY_INT</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="263"><td class="num" id="LN263">263</td><td class="line">        <span class='comment'>// To cover all cases we must use NPY_INT because</span></td></tr>
<tr class="codeline" data-linenumber="264"><td class="num" id="LN264">264</td><td class="line">        <span class='comment'>// NPY_INT32 is an alias which maybe equal to:</span></td></tr>
<tr class="codeline" data-linenumber="265"><td class="num" id="LN265">265</td><td class="line">        <span class='comment'>// - NPY_INT, when sizeof(int) = 4 and sizeof(long) = 8</span></td></tr>
<tr class="codeline" data-linenumber="266"><td class="num" id="LN266">266</td><td class="line">        <span class='comment'>// - NPY_LONG, when sizeof(int) = 4 and sizeof(long) = 4</span></td></tr>
<tr class="codeline" data-linenumber="267"><td class="num" id="LN267">267</td><td class="line">        <span class='keyword'>return</span> kInt;</td></tr>
<tr class="codeline" data-linenumber="268"><td class="num" id="LN268">268</td><td class="line">      } <span class='keyword'>else</span> <span class='keyword'>if</span> (dtype == NPY_LONGLONG || dtype == <span class='macro'>NPY_INT64<span class='macro_popup'>NPY_LONG</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="269"><td class="num" id="LN269">269</td><td class="line">        <span class='comment'>// NPY_INT64 is an alias which maybe equal to:</span></td></tr>
<tr class="codeline" data-linenumber="270"><td class="num" id="LN270">270</td><td class="line">        <span class='comment'>// - NPY_LONG, when sizeof(long) = 8 and sizeof(long long) = 8</span></td></tr>
<tr class="codeline" data-linenumber="271"><td class="num" id="LN271">271</td><td class="line">        <span class='comment'>// - NPY_LONGLONG, when sizeof(long) = 4 and sizeof(long long) = 8</span></td></tr>
<tr class="codeline" data-linenumber="272"><td class="num" id="LN272">272</td><td class="line">        <span class='keyword'>return</span> kLong;</td></tr>
<tr class="codeline" data-linenumber="273"><td class="num" id="LN273">273</td><td class="line">      } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="274"><td class="num" id="LN274">274</td><td class="line">        <span class='keyword'>break</span>;  <span class='comment'>// break as if this is one of the cases above because this is only a workaround</span></td></tr>
<tr class="codeline" data-linenumber="275"><td class="num" id="LN275">275</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="276"><td class="num" id="LN276">276</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="277"><td class="num" id="LN277">277</td><td class="line">  <span class='keyword'>auto</span> pytype = THPObjectPtr(<span class='macro'>PyArray_TypeObjectFromType<span class='macro_popup'>(*(PyObject * (*)(int)) __numpy_array_api[46])</span></span>(dtype));</td></tr>
<tr class="codeline" data-linenumber="278"><td class="num" id="LN278">278</td><td class="line">  <span class='keyword'>if</span> (!pytype) <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="279"><td class="num" id="LN279">279</td><td class="line">  <span class='keyword'>throw</span> TypeError(</td></tr>
<tr class="codeline" data-linenumber="280"><td class="num" id="LN280">280</td><td class="line">      <span class='string_literal'>"can't convert np.ndarray of type %s. The only supported types are: "</span></td></tr>
<tr class="codeline" data-linenumber="281"><td class="num" id="LN281">281</td><td class="line">      <span class='string_literal'>"float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."</span>,</td></tr>
<tr class="codeline" data-linenumber="282"><td class="num" id="LN282">282</td><td class="line">      ((PyTypeObject*)pytype.get())-&gt;tp_name);</td></tr>
<tr class="codeline" data-linenumber="283"><td class="num" id="LN283">283</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="284"><td class="num" id="LN284">284</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="285"><td class="num" id="LN285">285</td><td class="line"><span class='keyword'>bool</span> is_numpy_int(PyObject* obj) {</td></tr>
<tr class="codeline" data-linenumber="286"><td class="num" id="LN286">286</td><td class="line">  <span class='keyword'>return</span> is_numpy_available() &amp;&amp; <span class='macro'>PyArray_IsScalar((obj), Integer)<span class='macro_popup'>(((((PyObject*)((obj)))-&gt;ob_type) == (&amp;(*(PyTypeObject<br> *)__numpy_array_api[12])) || PyType_IsSubtype((((PyObject*)(<br>(obj)))-&gt;ob_type), (&amp;(*(PyTypeObject *)__numpy_array_api<br>[12])))))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="287"><td class="num" id="LN287">287</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="288"><td class="num" id="LN288">288</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="289"><td class="num" id="LN289">289</td><td class="line"><span class='keyword'>bool</span> is_numpy_scalar(PyObject* obj) {</td></tr>
<tr class="codeline" data-linenumber="290"><td class="num" id="LN290">290</td><td class="line">  <span class='keyword'>return</span> is_numpy_available() &amp;&amp; (is_numpy_int(obj) || <span class='macro'>PyArray_IsScalar(obj, Bool)<span class='macro_popup'>(((((PyObject*)(obj))-&gt;ob_type) == (&amp;(*(PyTypeObject *<br>)__numpy_array_api[8])) || PyType_IsSubtype((((PyObject*)(obj<br>))-&gt;ob_type), (&amp;(*(PyTypeObject *)__numpy_array_api[8]<br>)))))</span></span> ||</td></tr>
<tr class="codeline" data-linenumber="291"><td class="num" id="LN291">291</td><td class="line">         <span class='macro'>PyArray_IsScalar(obj, Floating)<span class='macro_popup'>(((((PyObject*)(obj))-&gt;ob_type) == (&amp;(*(PyTypeObject *<br>)__numpy_array_api[16])) || PyType_IsSubtype((((PyObject*)(obj<br>))-&gt;ob_type), (&amp;(*(PyTypeObject *)__numpy_array_api[16<br>])))))</span></span> || <span class='macro'>PyArray_IsScalar(obj, ComplexFloating)<span class='macro_popup'>(((((PyObject*)(obj))-&gt;ob_type) == (&amp;(*(PyTypeObject *<br>)__numpy_array_api[17])) || PyType_IsSubtype((((PyObject*)(obj<br>))-&gt;ob_type), (&amp;(*(PyTypeObject *)__numpy_array_api[17<br>])))))</span></span>);</td></tr>
<tr class="codeline" data-linenumber="292"><td class="num" id="LN292">292</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="293"><td class="num" id="LN293">293</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="294"><td class="num" id="LN294">294</td><td class="line">at::Tensor tensor_from_cuda_array_interface(PyObject* obj) {</td></tr>
<tr class="codeline" data-linenumber="295"><td class="num" id="LN295">295</td><td class="line">  <span class='keyword'>if</span> (!is_numpy_available()) {</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path1" class="msg msgControl" style="margin-left:3ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexControl">1</div></td><td>Taking false branch</td><td><div class="PathNav"><a href="#Path2" title="Next event (2)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="296"><td class="num" id="LN296">296</td><td class="line">    <span class='keyword'>throw</span> std::runtime_error(<span class='string_literal'>"Numpy is not available"</span>);</td></tr>
<tr class="codeline" data-linenumber="297"><td class="num" id="LN297">297</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="298"><td class="num" id="LN298">298</td><td class="line">  <span class='keyword'>auto</span> cuda_dict = THPObjectPtr(<span class="mrange"><span class="mrange">PyObject_GetAttrString(obj, <span class='string_literal'>"__cuda_array_interface__"</span>)</span></span>);</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path2" class="msg msgEvent" style="margin-left:33ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">2</div></td><td><div class="PathNav"><a href="#Path1" title="Previous event (1)">&#x2190;</a></div></td><td>Calling 'PyObject_GetAttrString'</td><td><div class="PathNav"><a href="#Path3" title="Next event (3)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path4" class="msg msgEvent" style="margin-left:33ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">4</div></td><td><div class="PathNav"><a href="#Path3" title="Previous event (3)">&#x2190;</a></div></td><td>Returning from 'PyObject_GetAttrString'</td><td><div class="PathNav"><a href="#Path5" title="Next event (5)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="EndPath" class="msg msgEvent" style="margin-left:33ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">9</div></td><td><div class="PathNav"><a href="#Path8" title="Previous event (8)">&#x2190;</a></div></td><td>PyObject ownership leak with reference count of 1</td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="299"><td class="num" id="LN299">299</td><td class="line">  <span class="mrange"><span class='macro'>TORCH_INTERNAL_ASSERT</span>(cuda_dict)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(cuda_dict)), 0<br>))) { ::c10::detail::torchInternalAssertFail( __func__, "../torch/csrc/utils/tensor_numpy.cpp"<br>, static_cast&lt;uint32_t&gt;(299), "cuda_dict" "INTERNAL ASSERT FAILED at "<br> "\"../torch/csrc/utils/tensor_numpy.cpp\"" ":" "299" ", please report a bug to PyTorch. "<br>, c10::str()); }</span></span>;</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path5" class="msg msgEvent" style="margin-left:3ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">5</div></td><td><div class="PathNav"><a href="#Path4" title="Previous event (4)">&#x2190;</a></div></td><td>Assuming the condition is false</td><td><div class="PathNav"><a href="#Path6" title="Next event (6)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path6" class="msg msgControl" style="margin-left:3ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexControl">6</div></td><td><div class="PathNav"><a href="#Path5" title="Previous event (5)">&#x2190;</a></div></td><td>Taking false branch</td><td><div class="PathNav"><a href="#Path7" title="Next event (7)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="300"><td class="num" id="LN300">300</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="301"><td class="num" id="LN301">301</td><td class="line">  <span class='keyword'>if</span> (!<span class="mrange"><span class='macro'>PyDict_Check</span>(cuda_dict)<span class='macro_popup'>((((((PyObject*)(cuda_dict))-&gt;ob_type))-&gt;tp_flags &amp;<br> ((1UL &lt;&lt; 29))) != 0)</span></span>) {</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path7" class="msg msgEvent" style="margin-left:8ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">7</div></td><td><div class="PathNav"><a href="#Path6" title="Previous event (6)">&#x2190;</a></div></td><td>Assuming the condition is false</td><td><div class="PathNav"><a href="#Path8" title="Next event (8)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr><td class="num"></td><td class="line"><div id="Path8" class="msg msgControl" style="margin-left:3ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexControl">8</div></td><td><div class="PathNav"><a href="#Path7" title="Previous event (7)">&#x2190;</a></div></td><td>Taking true branch</td><td><div class="PathNav"><a href="#EndPath" title="Next event (9)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="302"><td class="num" id="LN302">302</td><td class="line">    <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"`__cuda_array_interface__` must be a dict"</span>);</td></tr>
<tr class="codeline" data-linenumber="303"><td class="num" id="LN303">303</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="304"><td class="num" id="LN304">304</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="305"><td class="num" id="LN305">305</td><td class="line">  <span class='comment'>// Extract the `obj.__cuda_array_interface__['shape']` attribute</span></td></tr>
<tr class="codeline" data-linenumber="306"><td class="num" id="LN306">306</td><td class="line">  std::vector&lt;int64_t&gt; sizes;</td></tr>
<tr class="codeline" data-linenumber="307"><td class="num" id="LN307">307</td><td class="line">  {</td></tr>
<tr class="codeline" data-linenumber="308"><td class="num" id="LN308">308</td><td class="line">    PyObject *py_shape = PyDict_GetItemString(cuda_dict, <span class='string_literal'>"shape"</span>);</td></tr>
<tr class="codeline" data-linenumber="309"><td class="num" id="LN309">309</td><td class="line">    <span class='keyword'>if</span> (py_shape == <span class='keyword'>nullptr</span>) {</td></tr>
<tr class="codeline" data-linenumber="310"><td class="num" id="LN310">310</td><td class="line">      <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"attribute `shape` must exist"</span>);</td></tr>
<tr class="codeline" data-linenumber="311"><td class="num" id="LN311">311</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="312"><td class="num" id="LN312">312</td><td class="line">    sizes = seq_to_aten_shape(py_shape);</td></tr>
<tr class="codeline" data-linenumber="313"><td class="num" id="LN313">313</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="314"><td class="num" id="LN314">314</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="315"><td class="num" id="LN315">315</td><td class="line">  <span class='comment'>// Extract the `obj.__cuda_array_interface__['typestr']` attribute</span></td></tr>
<tr class="codeline" data-linenumber="316"><td class="num" id="LN316">316</td><td class="line">  ScalarType dtype;</td></tr>
<tr class="codeline" data-linenumber="317"><td class="num" id="LN317">317</td><td class="line">  <span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-init-variables)</span></td></tr>
<tr class="codeline" data-linenumber="318"><td class="num" id="LN318">318</td><td class="line">  <span class='keyword'>int</span> dtype_size_in_bytes;</td></tr>
<tr class="codeline" data-linenumber="319"><td class="num" id="LN319">319</td><td class="line">  {</td></tr>
<tr class="codeline" data-linenumber="320"><td class="num" id="LN320">320</td><td class="line">    PyObject *py_typestr = PyDict_GetItemString(cuda_dict, <span class='string_literal'>"typestr"</span>);</td></tr>
<tr class="codeline" data-linenumber="321"><td class="num" id="LN321">321</td><td class="line">    <span class='keyword'>if</span> (py_typestr == <span class='keyword'>nullptr</span>) {</td></tr>
<tr class="codeline" data-linenumber="322"><td class="num" id="LN322">322</td><td class="line">      <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"attribute `typestr` must exist"</span>);</td></tr>
<tr class="codeline" data-linenumber="323"><td class="num" id="LN323">323</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="324"><td class="num" id="LN324">324</td><td class="line">    <span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-init-variables)</span></td></tr>
<tr class="codeline" data-linenumber="325"><td class="num" id="LN325">325</td><td class="line">    PyArray_Descr *descr;</td></tr>
<tr class="codeline" data-linenumber="326"><td class="num" id="LN326">326</td><td class="line">    <span class='keyword'>if</span>(!<span class='macro'>PyArray_DescrConverter<span class='macro_popup'>(*(int (*)(PyObject *, PyArray_Descr **)) __numpy_array_api[174<br>])</span></span>(py_typestr, &amp;descr)) {</td></tr>
<tr class="codeline" data-linenumber="327"><td class="num" id="LN327">327</td><td class="line">      <span class='keyword'>throw</span> ValueError(<span class='string_literal'>"cannot parse `typestr`"</span>);</td></tr>
<tr class="codeline" data-linenumber="328"><td class="num" id="LN328">328</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="329"><td class="num" id="LN329">329</td><td class="line">    dtype = numpy_dtype_to_aten(descr-&gt;type_num);</td></tr>
<tr class="codeline" data-linenumber="330"><td class="num" id="LN330">330</td><td class="line">    dtype_size_in_bytes = descr-&gt;elsize;</td></tr>
<tr class="codeline" data-linenumber="331"><td class="num" id="LN331">331</td><td class="line">    <span class='macro'>TORCH_INTERNAL_ASSERT(dtype_size_in_bytes &gt; 0)<span class='macro_popup'>if ((__builtin_expect(static_cast&lt;bool&gt;(!(dtype_size_in_bytes<br> &gt; 0)), 0))) { ::c10::detail::torchInternalAssertFail( __func__<br>, "../torch/csrc/utils/tensor_numpy.cpp", static_cast&lt;uint32_t<br>&gt;(331), "dtype_size_in_bytes &gt; 0" "INTERNAL ASSERT FAILED at "<br> "\"../torch/csrc/utils/tensor_numpy.cpp\"" ":" "331" ", please report a bug to PyTorch. "<br>, c10::str()); }</span></span>;</td></tr>
<tr class="codeline" data-linenumber="332"><td class="num" id="LN332">332</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="333"><td class="num" id="LN333">333</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="334"><td class="num" id="LN334">334</td><td class="line">  <span class='comment'>// Extract the `obj.__cuda_array_interface__['data']` attribute</span></td></tr>
<tr class="codeline" data-linenumber="335"><td class="num" id="LN335">335</td><td class="line">  <span class='comment'>// NOLINTNEXTLINE(cppcoreguidelines-init-variables)</span></td></tr>
<tr class="codeline" data-linenumber="336"><td class="num" id="LN336">336</td><td class="line">  <span class='keyword'>void</span> *data_ptr;</td></tr>
<tr class="codeline" data-linenumber="337"><td class="num" id="LN337">337</td><td class="line">  {</td></tr>
<tr class="codeline" data-linenumber="338"><td class="num" id="LN338">338</td><td class="line">    PyObject *py_data = PyDict_GetItemString(cuda_dict, <span class='string_literal'>"data"</span>);</td></tr>
<tr class="codeline" data-linenumber="339"><td class="num" id="LN339">339</td><td class="line">    <span class='keyword'>if</span> (py_data == <span class='keyword'>nullptr</span>) {</td></tr>
<tr class="codeline" data-linenumber="340"><td class="num" id="LN340">340</td><td class="line">      <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"attribute `shape` data exist"</span>);</td></tr>
<tr class="codeline" data-linenumber="341"><td class="num" id="LN341">341</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="342"><td class="num" id="LN342">342</td><td class="line">    <span class='keyword'>if</span>(!<span class='macro'>PyTuple_Check(py_data)<span class='macro_popup'>((((((PyObject*)(py_data))-&gt;ob_type))-&gt;tp_flags &amp; (<br>(1UL &lt;&lt; 26))) != 0)</span></span> || <span class='macro'>PyTuple_GET_SIZE(py_data)<span class='macro_popup'>(((PyVarObject*)(((PyTupleObject *)(py_data))))-&gt;ob_size)</span></span> != 2) {</td></tr>
<tr class="codeline" data-linenumber="343"><td class="num" id="LN343">343</td><td class="line">      <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"`data` must be a 2-tuple of (int, bool)"</span>);</td></tr>
<tr class="codeline" data-linenumber="344"><td class="num" id="LN344">344</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="345"><td class="num" id="LN345">345</td><td class="line">    data_ptr = PyLong_AsVoidPtr(<span class='macro'>PyTuple_GET_ITEM(py_data, 0)<span class='macro_popup'>(((PyTupleObject *)(py_data))-&gt;ob_item[0])</span></span>);</td></tr>
<tr class="codeline" data-linenumber="346"><td class="num" id="LN346">346</td><td class="line">    <span class='keyword'>if</span> (data_ptr == <span class='keyword'>nullptr</span> &amp;&amp; PyErr_Occurred()) {</td></tr>
<tr class="codeline" data-linenumber="347"><td class="num" id="LN347">347</td><td class="line">      <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="348"><td class="num" id="LN348">348</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="349"><td class="num" id="LN349">349</td><td class="line">    <span class='keyword'>int</span> read_only = PyObject_IsTrue(<span class='macro'>PyTuple_GET_ITEM(py_data, 1)<span class='macro_popup'>(((PyTupleObject *)(py_data))-&gt;ob_item[1])</span></span>);</td></tr>
<tr class="codeline" data-linenumber="350"><td class="num" id="LN350">350</td><td class="line">    <span class='keyword'>if</span> (read_only == -1) {</td></tr>
<tr class="codeline" data-linenumber="351"><td class="num" id="LN351">351</td><td class="line">      <span class='keyword'>throw</span> python_error();</td></tr>
<tr class="codeline" data-linenumber="352"><td class="num" id="LN352">352</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="353"><td class="num" id="LN353">353</td><td class="line">    <span class='keyword'>if</span> (read_only) {</td></tr>
<tr class="codeline" data-linenumber="354"><td class="num" id="LN354">354</td><td class="line">      <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"the read only flag is not supported, should always be False"</span>);</td></tr>
<tr class="codeline" data-linenumber="355"><td class="num" id="LN355">355</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="356"><td class="num" id="LN356">356</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="357"><td class="num" id="LN357">357</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="358"><td class="num" id="LN358">358</td><td class="line">  <span class='comment'>// Extract the `obj.__cuda_array_interface__['strides']` attribute</span></td></tr>
<tr class="codeline" data-linenumber="359"><td class="num" id="LN359">359</td><td class="line">  std::vector&lt;int64_t&gt; strides;</td></tr>
<tr class="codeline" data-linenumber="360"><td class="num" id="LN360">360</td><td class="line">  {</td></tr>
<tr class="codeline" data-linenumber="361"><td class="num" id="LN361">361</td><td class="line">    PyObject *py_strides = PyDict_GetItemString(cuda_dict, <span class='string_literal'>"strides"</span>);</td></tr>
<tr class="codeline" data-linenumber="362"><td class="num" id="LN362">362</td><td class="line">    <span class='keyword'>if</span> (py_strides != <span class='keyword'>nullptr</span> &amp;&amp; py_strides != <span class='macro'>Py_None<span class='macro_popup'>(&amp;_Py_NoneStruct)</span></span>) {</td></tr>
<tr class="codeline" data-linenumber="363"><td class="num" id="LN363">363</td><td class="line">      <span class='keyword'>if</span> (<span class='macro'>PySequence_Length<span class='macro_popup'>PySequence_Size</span></span>(py_strides) == -1 || <span class='macro'>PySequence_Length<span class='macro_popup'>PySequence_Size</span></span>(py_strides) != sizes.size()) {</td></tr>
<tr class="codeline" data-linenumber="364"><td class="num" id="LN364">364</td><td class="line">        <span class='keyword'>throw</span> TypeError(<span class='string_literal'>"strides must be a sequence of the same length as shape"</span>);</td></tr>
<tr class="codeline" data-linenumber="365"><td class="num" id="LN365">365</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="366"><td class="num" id="LN366">366</td><td class="line">      strides = seq_to_aten_shape(py_strides);</td></tr>
<tr class="codeline" data-linenumber="367"><td class="num" id="LN367">367</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="368"><td class="num" id="LN368">368</td><td class="line">      <span class='comment'>// __cuda_array_interface__ strides use bytes. Torch strides use element counts.</span></td></tr>
<tr class="codeline" data-linenumber="369"><td class="num" id="LN369">369</td><td class="line">      <span class='keyword'>for</span> (<span class='keyword'>auto</span>&amp; stride : strides) {</td></tr>
<tr class="codeline" data-linenumber="370"><td class="num" id="LN370">370</td><td class="line">        <span class='keyword'>if</span> (stride%dtype_size_in_bytes != 0) {</td></tr>
<tr class="codeline" data-linenumber="371"><td class="num" id="LN371">371</td><td class="line">          <span class='keyword'>throw</span> ValueError(</td></tr>
<tr class="codeline" data-linenumber="372"><td class="num" id="LN372">372</td><td class="line">              <span class='string_literal'>"given array strides not a multiple of the element byte size. "</span></td></tr>
<tr class="codeline" data-linenumber="373"><td class="num" id="LN373">373</td><td class="line">              <span class='string_literal'>"Make a copy of the array to reallocate the memory."</span>);</td></tr>
<tr class="codeline" data-linenumber="374"><td class="num" id="LN374">374</td><td class="line">          }</td></tr>
<tr class="codeline" data-linenumber="375"><td class="num" id="LN375">375</td><td class="line">        stride /= dtype_size_in_bytes;</td></tr>
<tr class="codeline" data-linenumber="376"><td class="num" id="LN376">376</td><td class="line">      }</td></tr>
<tr class="codeline" data-linenumber="377"><td class="num" id="LN377">377</td><td class="line">    } <span class='keyword'>else</span> {</td></tr>
<tr class="codeline" data-linenumber="378"><td class="num" id="LN378">378</td><td class="line">      strides = at::detail::defaultStrides(sizes);</td></tr>
<tr class="codeline" data-linenumber="379"><td class="num" id="LN379">379</td><td class="line">    }</td></tr>
<tr class="codeline" data-linenumber="380"><td class="num" id="LN380">380</td><td class="line">  }</td></tr>
<tr class="codeline" data-linenumber="381"><td class="num" id="LN381">381</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="382"><td class="num" id="LN382">382</td><td class="line">  <span class='macro'>Py_INCREF(obj)<span class='macro_popup'>_Py_INCREF(((PyObject*)(obj)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="383"><td class="num" id="LN383">383</td><td class="line">  <span class='keyword'>return</span> at::from_blob(</td></tr>
<tr class="codeline" data-linenumber="384"><td class="num" id="LN384">384</td><td class="line">      data_ptr,</td></tr>
<tr class="codeline" data-linenumber="385"><td class="num" id="LN385">385</td><td class="line">      sizes,</td></tr>
<tr class="codeline" data-linenumber="386"><td class="num" id="LN386">386</td><td class="line">      strides,</td></tr>
<tr class="codeline" data-linenumber="387"><td class="num" id="LN387">387</td><td class="line">      [obj](<span class='keyword'>void</span>* data) {</td></tr>
<tr class="codeline" data-linenumber="388"><td class="num" id="LN388">388</td><td class="line">        pybind11::gil_scoped_acquire gil;</td></tr>
<tr class="codeline" data-linenumber="389"><td class="num" id="LN389">389</td><td class="line">        <span class='macro'>Py_DECREF(obj)<span class='macro_popup'>_Py_DECREF(((PyObject*)(obj)))</span></span>;</td></tr>
<tr class="codeline" data-linenumber="390"><td class="num" id="LN390">390</td><td class="line">      },</td></tr>
<tr class="codeline" data-linenumber="391"><td class="num" id="LN391">391</td><td class="line">      at::device(kCUDA).dtype(dtype)</td></tr>
<tr class="codeline" data-linenumber="392"><td class="num" id="LN392">392</td><td class="line">  );</td></tr>
<tr class="codeline" data-linenumber="393"><td class="num" id="LN393">393</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="394"><td class="num" id="LN394">394</td><td class="line">}} <span class='comment'>// namespace torch::utils</span></td></tr>
<tr class="codeline" data-linenumber="395"><td class="num" id="LN395">395</td><td class="line"> </td></tr>
<tr class="codeline" data-linenumber="396"><td class="num" id="LN396">396</td><td class="line"><span class='directive'>#endif  // USE_NUMPY</span></td></tr>
</table><hr class=divider>
<div id=File184644>
<div class=FileNav><a href="#File1">&#x2190;</a></div><h4 class=FileName>/opt/pyrefcon/lib/pyrefcon/models/models/PyObject_GetAttrString.model</h4>
</div>
<table class="code" data-fileid="184644">
<tr class="codeline" data-linenumber="1"><td class="num" id="LN1">1</td><td class="line"><span class='directive'>#ifndef PyObject_GetAttrString</span></td></tr>
<tr class="codeline" data-linenumber="2"><td class="num" id="LN2">2</td><td class="line"><span class='keyword'>struct</span> _object;</td></tr>
<tr class="codeline" data-linenumber="3"><td class="num" id="LN3">3</td><td class="line"><span class='keyword'>typedef</span> <span class='keyword'>struct</span> _object PyObject;</td></tr>
<tr class="codeline" data-linenumber="4"><td class="num" id="LN4">4</td><td class="line">PyObject* clang_analyzer_PyObject_New_Reference();</td></tr>
<tr class="codeline" data-linenumber="5"><td class="num" id="LN5">5</td><td class="line">PyObject* PyObject_GetAttrString(PyObject *o, <span class='keyword'>const</span> <span class='keyword'>char</span> *attr_name) {</td></tr>
<tr class="codeline" data-linenumber="6"><td class="num" id="LN6">6</td><td class="line">  <span class='keyword'>return</span> <span class="mrange">clang_analyzer_PyObject_New_Reference()</span>;</td></tr>
<tr><td class="num"></td><td class="line"><div id="Path3" class="msg msgEvent" style="margin-left:10ex"><table class="msgT"><tr><td valign="top"><div class="PathIndex PathIndexEvent">3</div></td><td><div class="PathNav"><a href="#Path2" title="Previous event (2)">&#x2190;</a></div></td><td>Setting reference count to 1</td><td><div class="PathNav"><a href="#Path4" title="Next event (4)">&#x2192;</a></div></td></tr></table></div></td></tr>
<tr class="codeline" data-linenumber="7"><td class="num" id="LN7">7</td><td class="line">}</td></tr>
<tr class="codeline" data-linenumber="8"><td class="num" id="LN8">8</td><td class="line"><span class='directive'>#else</span></td></tr>
<tr class="codeline" data-linenumber="9"><td class="num" id="LN9">9</td><td class="line"><span class='directive'>#warning "API PyObject_GetAttrString is defined as a macro."</span></td></tr>
<tr class="codeline" data-linenumber="10"><td class="num" id="LN10">10</td><td class="line"><span class='directive'>#endif</span></td></tr></table></body></html>
